{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dNation Kubernetes Monitoring See status of your Kubernetes infrastructure and applications at a glance using semaphore (green/orange/red) principle: It is a set of Grafana dashboards and Prometheus alerts written in Jsonnet . This Monitoring also supports multi-cluster monitoring by Thanos and following 3 basic design principles: Intuitive - Green, orange and red colors signaling whether or not your action is needed Drill-down - if you want details why something is green, orange or red, just click it Relevant information only - provide only metrics relevant for this particular area of interest and drill-down level, side-by-side with logs (experimental feature) Monitoring targets are: Kubernetes Hosts Applications Full Installation In case your current Kubernetes installation doesn't contain Prometheus Operator, Grafana or Loki, please install dNation Kubernetes Monitoring Stack helm chart (recommended). Dashboards and Alerts only Installation In case your current Kubernetes installation already contains Prometheus Operator, Grafana and Loki, please follow here . Documentation See the documentation and FAQ for further information. Contribution guidelines If you want to contribute, please read following: Contribution Guidelines Code of Conduct How To simplify your local development We use GitHub issues to manage requests and bugs. Commercial support This project has been developed, maintained and used in production by professionals to simplify their day-to-day monitoring tasks and reduce incident reaction time. Commercial support is available, including 24/7, please contact us .","title":"Home"},{"location":"#dnation-kubernetes-monitoring","text":"See status of your Kubernetes infrastructure and applications at a glance using semaphore (green/orange/red) principle: It is a set of Grafana dashboards and Prometheus alerts written in Jsonnet . This Monitoring also supports multi-cluster monitoring by Thanos and following 3 basic design principles: Intuitive - Green, orange and red colors signaling whether or not your action is needed Drill-down - if you want details why something is green, orange or red, just click it Relevant information only - provide only metrics relevant for this particular area of interest and drill-down level, side-by-side with logs (experimental feature) Monitoring targets are: Kubernetes Hosts Applications","title":"dNation Kubernetes Monitoring"},{"location":"#full-installation","text":"In case your current Kubernetes installation doesn't contain Prometheus Operator, Grafana or Loki, please install dNation Kubernetes Monitoring Stack helm chart (recommended).","title":"Full Installation"},{"location":"#dashboards-and-alerts-only-installation","text":"In case your current Kubernetes installation already contains Prometheus Operator, Grafana and Loki, please follow here .","title":"Dashboards and Alerts only Installation"},{"location":"#documentation","text":"See the documentation and FAQ for further information.","title":"Documentation"},{"location":"#contribution-guidelines","text":"If you want to contribute, please read following: Contribution Guidelines Code of Conduct How To simplify your local development We use GitHub issues to manage requests and bugs.","title":"Contribution guidelines"},{"location":"#commercial-support","text":"This project has been developed, maintained and used in production by professionals to simplify their day-to-day monitoring tasks and reduce incident reaction time. Commercial support is available, including 24/7, please contact us .","title":"Commercial support"},{"location":"CHANGELOG/","text":"Changelog v2.5.4 (2023-11-14) Full Changelog Merged pull requests: Fix queries #178 v2.5.3 (2023-07-18) Full Changelog Merged pull requests: Fix x and y coordinates of app box overview #170 v2.5.2 (2023-06-27) Full Changelog Merged pull requests: Add ssl-exporter app to main dashboard #169 v2.5.1 (2023-06-19) Full Changelog Enhancements Add Harbor dashboard #164 Merged pull requests: Update SSL Exporter dashboard #168 v2.5.0 (2023-05-26) Full Changelog Merged pull requests: Add Harbor dashboard #167 v2.4.0 (2023-05-24) Full Changelog Merged pull requests: Small comment in values.yaml #166 Add support for k8s version 1.24+ #165 v2.3.1 (2023-05-09) Full Changelog Merged pull requests: Fix, ssl exporter dashboard #163 v2.3.0 (2023-05-04) Full Changelog Merged pull requests: Add dashboard for ssl-exporter #162 v2.2.2 (2023-04-18) Full Changelog Merged pull requests: Fix duplicate series for host monitoring in multicluster setup #161 v2.2.1 (2023-04-12) Full Changelog Merged pull requests: Bump up monitoring version #160 Fix Docs badge #159 v2.2.0 (2023-03-24) Full Changelog Closed issues: Create and fill CHANGELOG.md #83 Add Prometheus dashboard #35 Merged pull requests: Bump up translator version in dependencies #158 v2.1.1 (2023-01-26) Full Changelog Merged pull requests: Fix generation of docs and pages #157 v2.1.0 (2023-01-23) Full Changelog Merged pull requests: V2.1.x #156 v2.0.2 (2023-01-19) Full Changelog Closed issues: Mysql exporter alerts missing job label #148 Merged pull requests: Added alertgroup label for apps.rules #154 v2.0.0 (2022-11-08) Full Changelog Merged pull requests: Update dashboards for multi-cluster monitoring #153 v1.4.2 (2022-10-11) Full Changelog Closed issues: Fix Tag branch github action #150 Merged pull requests: Change ControlPlane template #152 Update github-tag-action to 1.39.0 #151 v1.4.1 (2022-08-15) Full Changelog Fixes Wrong job variable for host monitoring when going directly to dashboard #133 Closed issues: Add dashboard for websocket server #97 Add dashboard for JVM exporter #96 Add Loki dashboard #34 Merged pull requests: Update template values to work with grafana 9 #149 Fixed config for kind #142 v1.4.0 (2022-02-11) Full Changelog Fixes Color problem: white value on white background #129 Duplicate series error after alertGroup label deletition #117 Improve PromQL expressions to resist the label change #115 Closed issues: Add ci actions to test monitoring installation in latest k8s [releases](https://kubernetes.io/releases/) #130 Merged pull requests: V1.4.x #139 v1.3.4 (2022-01-11) Full Changelog Closed issues: Update docs - define minimal cpu/mem requirements needed for successful installation #131 Merged pull requests: Fix color problem on Light theme for Used/Total Cores/Ram/Disk #132 v1.3.3 (2021-06-22) Full Changelog Merged pull requests: Use 'Last (not null)' reducer function in L3 pvc gauge panels #126 v1.3.2 (2021-06-21) Full Changelog Fixes Column sorting in L2 tables doesn't work #123 Links in L2 tables have bad variable mapping #121 Merged pull requests: L2fixes #124 v1.3.1 (2021-06-17) Full Changelog Fixes Missing hyperlink in MySQL Application Graph - Host Monitoring #120 Fix legend in Request duration 99th quantile graph - Api server dashboard #119 Fix Job graph expression #116 Closed issues: MySQL dashboard should have hostname #118 L0 expression - If host/cluster is DOWN L0 state panel still shows OK state #112 Replace legacy nginx vts metrics #110 Split Node Metrics to Master/Nodes #107 Merged pull requests: Minor graph fixies #122 v1.3.0 (2021-06-07) Full Changelog Closed issues: Investigate and, if necessary, change usedDisk expression #105 Host monitoring - improve host monitoring expressions to be resistant in case of IP address miss configuration #100 Improve Nginx VTS dashboard #95 Hard coded label alertGroup \"Host\" in PromQL expression #88 Add support for KubeVirt VMs #79 Visualize Virtual Filesystems Utilization in separate graph #76 Re-organize cluster main dashboard #75 Merged pull requests: V1.3.x #114 Update AWS_doc.md #106 Update aws_doc.md #102 Rename namespace in AWS-doc.md #101 Aws doc #92 v1.2.1 (2021-05-13) Full Changelog Closed issues: 2x increase in CPU usage after redeployment of monitoring #90 Merged pull requests: Consider null values in stacked graphs as zero #91 cicd-jsonnet-v1.0.3 (2021-05-04) Full Changelog v1.2.0 (2021-05-04) Full Changelog Closed issues: In the nginx ingress alert Detailed link is 'var-job=' empty #80 Make copyright headers consistent across #71 Add dashboards for MySQL/MariaDB #70 Merged pull requests: V1.2.x #89 v1.1.2 (2021-04-21) Full Changelog Fixes L0 Warning state panel with red color backgroud #66 Closed issues: Add template logic to the L3 layers #65 Re-Order Columns in L2 dashboards #63 Add template logic to the L2 layers #17 Merged pull requests: Change nginxIngressCertificateExpiry expr to preserve labels #81 v1.1.1 (2021-02-25) Full Changelog Closed issues: Fix Job select in Job Overview L2 dashboard #62 Release v1.1.x #57 Merged pull requests: Fix Job select in Job Overview #64 v1.1.0 (2021-02-23) Full Changelog Closed issues: Dependencies update - dnation jsonnet translator version #56 Update CONTRIBUTING.md file according to the latest versioning rules #50 Annoying Do you want to save your changes? alert even when dashboard has not been changed #47 CI pipeline improvement - follow up issue #46 Update FAQ - Why L2 table item has different background color as corresponding L1 stat panel? #45 Add basic drop-down menu on k8s L2 dashboards #43 Add release version badge to the README #41 Update L1 dashboards - rename RPC to GRPC #39 Update L1 links in k8s monitoring dashboard #37 [Docs] Escape double quotes in expressions #36 Add possibility to exclude dashboard from helm upgrade process #33 Add template logic to the L0 layer #25 Ingress controller dashboard L1 - improve expression #19 Merged pull requests: V1.1.x #61 Escape quotes in docs #53 Update contributing #52 Lint appVersion always with main branch #49 v1.0.25 (2021-02-05) Full Changelog Closed issues: Create CI/CD job to check version of monitoring application #30 Merged pull requests: Fix linting appVersion in ci #44 Fix missing escape of '%' in jsonnet #42 Add ci check of appVersion #38 v1.0.24 (2021-02-03) Full Changelog Closed issues: Add hyperlinks to the Alerts table panel and/or messages #21 Merged pull requests: Round value of NginxIngressSuccessRateLow alert #40 v1.0.23 (2021-02-03) Full Changelog Closed issues: Add FAQ - How to interpret displayed monitoring values across layers? #22 Add HOST IP address info to the L3 layer of monitoring #20 Merged pull requests: Add grafana dashboard link to alerts #32 Simplify local development #31 v1.0.22 (2021-01-25) Full Changelog Closed issues: Add possibility to adjust the monitoring dashboards default directory #18 Merged pull requests: Add HOST IP address info table to the NodeExporter #29 v1.0.21 (2021-01-25) Full Changelog Closed issues: Docs json->yaml #24 Record short video to show the latest monitoring state. #16 Merged pull requests: Add option to change grafana directory #28 Change docs from json to yaml #26 Update README.md #23 v1.0.20 (2021-01-14) Full Changelog Merged pull requests: Docs #15 cicd-jsonnet-v1.0.2 (2021-01-14) Full Changelog Merged pull requests: Update ci/cd image version to 1.0.2 #14 Sync gitlab main #13 Fix release.yaml bug #12 Update CODE_OF_CONDUCT.md #11 Update CODE_OF_CONDUCT.md #10 v1.0.19 (2020-11-13) Full Changelog Merged pull requests: Update README.md #9 Node filter #8 cicd-jsonnet-v1.0.1 (2020-11-11) Full Changelog Merged pull requests: Increase version of packages in docker image #7 cicd-jsonnet-v1.0.0 (2020-11-10) Full Changelog Merged pull requests: Create tag when CI/CD jsonnet docker code changes #6 v1.0.18 (2020-10-28) Full Changelog Merged pull requests: Notes #4 v1.0.17 (2020-10-27) Full Changelog Merged pull requests: Update CDN image urls #3 v1.0.16 (2020-10-26) Full Changelog Merged pull requests: Add jsonnet build and lint to the GitHub CI/CD pipeline and update chart version #2 Add GitHub CI/CD #1","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#v254-2023-11-14","text":"Full Changelog Merged pull requests: Fix queries #178","title":"v2.5.4 (2023-11-14)"},{"location":"CHANGELOG/#v253-2023-07-18","text":"Full Changelog Merged pull requests: Fix x and y coordinates of app box overview #170","title":"v2.5.3 (2023-07-18)"},{"location":"CHANGELOG/#v252-2023-06-27","text":"Full Changelog Merged pull requests: Add ssl-exporter app to main dashboard #169","title":"v2.5.2 (2023-06-27)"},{"location":"CHANGELOG/#v251-2023-06-19","text":"Full Changelog","title":"v2.5.1 (2023-06-19)"},{"location":"CHANGELOG/#enhancements","text":"Add Harbor dashboard #164 Merged pull requests: Update SSL Exporter dashboard #168","title":"Enhancements"},{"location":"CHANGELOG/#v250-2023-05-26","text":"Full Changelog Merged pull requests: Add Harbor dashboard #167","title":"v2.5.0 (2023-05-26)"},{"location":"CHANGELOG/#v240-2023-05-24","text":"Full Changelog Merged pull requests: Small comment in values.yaml #166 Add support for k8s version 1.24+ #165","title":"v2.4.0 (2023-05-24)"},{"location":"CHANGELOG/#v231-2023-05-09","text":"Full Changelog Merged pull requests: Fix, ssl exporter dashboard #163","title":"v2.3.1 (2023-05-09)"},{"location":"CHANGELOG/#v230-2023-05-04","text":"Full Changelog Merged pull requests: Add dashboard for ssl-exporter #162","title":"v2.3.0 (2023-05-04)"},{"location":"CHANGELOG/#v222-2023-04-18","text":"Full Changelog Merged pull requests: Fix duplicate series for host monitoring in multicluster setup #161","title":"v2.2.2 (2023-04-18)"},{"location":"CHANGELOG/#v221-2023-04-12","text":"Full Changelog Merged pull requests: Bump up monitoring version #160 Fix Docs badge #159","title":"v2.2.1 (2023-04-12)"},{"location":"CHANGELOG/#v220-2023-03-24","text":"Full Changelog Closed issues: Create and fill CHANGELOG.md #83 Add Prometheus dashboard #35 Merged pull requests: Bump up translator version in dependencies #158","title":"v2.2.0 (2023-03-24)"},{"location":"CHANGELOG/#v211-2023-01-26","text":"Full Changelog Merged pull requests: Fix generation of docs and pages #157","title":"v2.1.1 (2023-01-26)"},{"location":"CHANGELOG/#v210-2023-01-23","text":"Full Changelog Merged pull requests: V2.1.x #156","title":"v2.1.0 (2023-01-23)"},{"location":"CHANGELOG/#v202-2023-01-19","text":"Full Changelog Closed issues: Mysql exporter alerts missing job label #148 Merged pull requests: Added alertgroup label for apps.rules #154","title":"v2.0.2 (2023-01-19)"},{"location":"CHANGELOG/#v200-2022-11-08","text":"Full Changelog Merged pull requests: Update dashboards for multi-cluster monitoring #153","title":"v2.0.0 (2022-11-08)"},{"location":"CHANGELOG/#v142-2022-10-11","text":"Full Changelog Closed issues: Fix Tag branch github action #150 Merged pull requests: Change ControlPlane template #152 Update github-tag-action to 1.39.0 #151","title":"v1.4.2 (2022-10-11)"},{"location":"CHANGELOG/#v141-2022-08-15","text":"Full Changelog","title":"v1.4.1 (2022-08-15)"},{"location":"CHANGELOG/#fixes","text":"Wrong job variable for host monitoring when going directly to dashboard #133 Closed issues: Add dashboard for websocket server #97 Add dashboard for JVM exporter #96 Add Loki dashboard #34 Merged pull requests: Update template values to work with grafana 9 #149 Fixed config for kind #142","title":"Fixes"},{"location":"CHANGELOG/#v140-2022-02-11","text":"Full Changelog","title":"v1.4.0 (2022-02-11)"},{"location":"CHANGELOG/#fixes_1","text":"Color problem: white value on white background #129 Duplicate series error after alertGroup label deletition #117 Improve PromQL expressions to resist the label change #115 Closed issues: Add ci actions to test monitoring installation in latest k8s [releases](https://kubernetes.io/releases/) #130 Merged pull requests: V1.4.x #139","title":"Fixes"},{"location":"CHANGELOG/#v134-2022-01-11","text":"Full Changelog Closed issues: Update docs - define minimal cpu/mem requirements needed for successful installation #131 Merged pull requests: Fix color problem on Light theme for Used/Total Cores/Ram/Disk #132","title":"v1.3.4 (2022-01-11)"},{"location":"CHANGELOG/#v133-2021-06-22","text":"Full Changelog Merged pull requests: Use 'Last (not null)' reducer function in L3 pvc gauge panels #126","title":"v1.3.3 (2021-06-22)"},{"location":"CHANGELOG/#v132-2021-06-21","text":"Full Changelog","title":"v1.3.2 (2021-06-21)"},{"location":"CHANGELOG/#fixes_2","text":"Column sorting in L2 tables doesn't work #123 Links in L2 tables have bad variable mapping #121 Merged pull requests: L2fixes #124","title":"Fixes"},{"location":"CHANGELOG/#v131-2021-06-17","text":"Full Changelog","title":"v1.3.1 (2021-06-17)"},{"location":"CHANGELOG/#fixes_3","text":"Missing hyperlink in MySQL Application Graph - Host Monitoring #120 Fix legend in Request duration 99th quantile graph - Api server dashboard #119 Fix Job graph expression #116 Closed issues: MySQL dashboard should have hostname #118 L0 expression - If host/cluster is DOWN L0 state panel still shows OK state #112 Replace legacy nginx vts metrics #110 Split Node Metrics to Master/Nodes #107 Merged pull requests: Minor graph fixies #122","title":"Fixes"},{"location":"CHANGELOG/#v130-2021-06-07","text":"Full Changelog Closed issues: Investigate and, if necessary, change usedDisk expression #105 Host monitoring - improve host monitoring expressions to be resistant in case of IP address miss configuration #100 Improve Nginx VTS dashboard #95 Hard coded label alertGroup \"Host\" in PromQL expression #88 Add support for KubeVirt VMs #79 Visualize Virtual Filesystems Utilization in separate graph #76 Re-organize cluster main dashboard #75 Merged pull requests: V1.3.x #114 Update AWS_doc.md #106 Update aws_doc.md #102 Rename namespace in AWS-doc.md #101 Aws doc #92","title":"v1.3.0 (2021-06-07)"},{"location":"CHANGELOG/#v121-2021-05-13","text":"Full Changelog Closed issues: 2x increase in CPU usage after redeployment of monitoring #90 Merged pull requests: Consider null values in stacked graphs as zero #91","title":"v1.2.1 (2021-05-13)"},{"location":"CHANGELOG/#cicd-jsonnet-v103-2021-05-04","text":"Full Changelog","title":"cicd-jsonnet-v1.0.3 (2021-05-04)"},{"location":"CHANGELOG/#v120-2021-05-04","text":"Full Changelog Closed issues: In the nginx ingress alert Detailed link is 'var-job=' empty #80 Make copyright headers consistent across #71 Add dashboards for MySQL/MariaDB #70 Merged pull requests: V1.2.x #89","title":"v1.2.0 (2021-05-04)"},{"location":"CHANGELOG/#v112-2021-04-21","text":"Full Changelog","title":"v1.1.2 (2021-04-21)"},{"location":"CHANGELOG/#fixes_4","text":"L0 Warning state panel with red color backgroud #66 Closed issues: Add template logic to the L3 layers #65 Re-Order Columns in L2 dashboards #63 Add template logic to the L2 layers #17 Merged pull requests: Change nginxIngressCertificateExpiry expr to preserve labels #81","title":"Fixes"},{"location":"CHANGELOG/#v111-2021-02-25","text":"Full Changelog Closed issues: Fix Job select in Job Overview L2 dashboard #62 Release v1.1.x #57 Merged pull requests: Fix Job select in Job Overview #64","title":"v1.1.1 (2021-02-25)"},{"location":"CHANGELOG/#v110-2021-02-23","text":"Full Changelog Closed issues: Dependencies update - dnation jsonnet translator version #56 Update CONTRIBUTING.md file according to the latest versioning rules #50 Annoying Do you want to save your changes? alert even when dashboard has not been changed #47 CI pipeline improvement - follow up issue #46 Update FAQ - Why L2 table item has different background color as corresponding L1 stat panel? #45 Add basic drop-down menu on k8s L2 dashboards #43 Add release version badge to the README #41 Update L1 dashboards - rename RPC to GRPC #39 Update L1 links in k8s monitoring dashboard #37 [Docs] Escape double quotes in expressions #36 Add possibility to exclude dashboard from helm upgrade process #33 Add template logic to the L0 layer #25 Ingress controller dashboard L1 - improve expression #19 Merged pull requests: V1.1.x #61 Escape quotes in docs #53 Update contributing #52 Lint appVersion always with main branch #49","title":"v1.1.0 (2021-02-23)"},{"location":"CHANGELOG/#v1025-2021-02-05","text":"Full Changelog Closed issues: Create CI/CD job to check version of monitoring application #30 Merged pull requests: Fix linting appVersion in ci #44 Fix missing escape of '%' in jsonnet #42 Add ci check of appVersion #38","title":"v1.0.25 (2021-02-05)"},{"location":"CHANGELOG/#v1024-2021-02-03","text":"Full Changelog Closed issues: Add hyperlinks to the Alerts table panel and/or messages #21 Merged pull requests: Round value of NginxIngressSuccessRateLow alert #40","title":"v1.0.24 (2021-02-03)"},{"location":"CHANGELOG/#v1023-2021-02-03","text":"Full Changelog Closed issues: Add FAQ - How to interpret displayed monitoring values across layers? #22 Add HOST IP address info to the L3 layer of monitoring #20 Merged pull requests: Add grafana dashboard link to alerts #32 Simplify local development #31","title":"v1.0.23 (2021-02-03)"},{"location":"CHANGELOG/#v1022-2021-01-25","text":"Full Changelog Closed issues: Add possibility to adjust the monitoring dashboards default directory #18 Merged pull requests: Add HOST IP address info table to the NodeExporter #29","title":"v1.0.22 (2021-01-25)"},{"location":"CHANGELOG/#v1021-2021-01-25","text":"Full Changelog Closed issues: Docs json->yaml #24 Record short video to show the latest monitoring state. #16 Merged pull requests: Add option to change grafana directory #28 Change docs from json to yaml #26 Update README.md #23","title":"v1.0.21 (2021-01-25)"},{"location":"CHANGELOG/#v1020-2021-01-14","text":"Full Changelog Merged pull requests: Docs #15","title":"v1.0.20 (2021-01-14)"},{"location":"CHANGELOG/#cicd-jsonnet-v102-2021-01-14","text":"Full Changelog Merged pull requests: Update ci/cd image version to 1.0.2 #14 Sync gitlab main #13 Fix release.yaml bug #12 Update CODE_OF_CONDUCT.md #11 Update CODE_OF_CONDUCT.md #10","title":"cicd-jsonnet-v1.0.2 (2021-01-14)"},{"location":"CHANGELOG/#v1019-2020-11-13","text":"Full Changelog Merged pull requests: Update README.md #9 Node filter #8","title":"v1.0.19 (2020-11-13)"},{"location":"CHANGELOG/#cicd-jsonnet-v101-2020-11-11","text":"Full Changelog Merged pull requests: Increase version of packages in docker image #7","title":"cicd-jsonnet-v1.0.1 (2020-11-11)"},{"location":"CHANGELOG/#cicd-jsonnet-v100-2020-11-10","text":"Full Changelog Merged pull requests: Create tag when CI/CD jsonnet docker code changes #6","title":"cicd-jsonnet-v1.0.0 (2020-11-10)"},{"location":"CHANGELOG/#v1018-2020-10-28","text":"Full Changelog Merged pull requests: Notes #4","title":"v1.0.18 (2020-10-28)"},{"location":"CHANGELOG/#v1017-2020-10-27","text":"Full Changelog Merged pull requests: Update CDN image urls #3","title":"v1.0.17 (2020-10-27)"},{"location":"CHANGELOG/#v1016-2020-10-26","text":"Full Changelog Merged pull requests: Add jsonnet build and lint to the GitHub CI/CD pipeline and update chart version #2 Add GitHub CI/CD #1","title":"v1.0.16 (2020-10-26)"},{"location":"CODE_OF_CONDUCT/","text":"dNation Kubernetes Monitoring Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at [INSERT CONTACT METHOD]. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"dNation Kubernetes Monitoring Code of Conduct"},{"location":"CODE_OF_CONDUCT/#dnation-kubernetes-monitoring-code-of-conduct","text":"","title":"dNation Kubernetes Monitoring Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at [INSERT CONTACT METHOD]. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"CONTRIBUTING/","text":"dNation Kubernetes Monitoring Contributing Guidelines Thank you for considering to contribute to dNation Kubernetes Monitoring project. When contributing to this repository, please first discuss the change you wish to make via issue, email, or by any other method with the owners of this repository before making a change. Pull Request Checklist Before sending your pull requests, make sure you followed this list. Read Contributing Guidelines Read Code of Conduct Read Commit Message Convention Read How To simplify your local development Set up the Developer Certificate of Origin (DCO) Include a License at the top of new files Update the Readme with details of changes to the interface In case the pull request would update the version number, please edit the version number in all appropriate files e.g. Chart.yaml . Read more about our chart versioning policy . Choose appropriate base branch for pull request. Read more about our release policy . You may merge the Pull Request once you have the sign-off of other developer, or if you don't have the permission to do that, you may request the reviewer to merge it for you Developer Certificate of Origin (DCO) The Developer Certificate of Origin (DCO) is a legally binding statement that asserts that you are the creator of your contribution, and that you wish to allow dNation Kubernetes Monitoring project to use your work. Acknowledgement of this permission is done using a sign-off process in Git. The sign-off is a simple line at the end of the explanation for the patch. The text of the DCO is available on developercertificate.org . If you are willing to agree to these terms, you just add a line to every git commit message: Signed-off-by: Joe Smith <joe.smith@email.com> If you set your user.name and user.email as part of your git configuration, you can sign your commit automatically with git commit -s . Unfortunately, you have to use your real name (i.e., pseudonyms or anonymous contributions cannot be made). This is because the DCO is a legally binding document, granting the dNation Kubernetes Monitoring project to use your work. License on the top of file /* Copyright 2020 The dNation Kubernetes Monitoring Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ Release policy Release branch vMajor.Minor.x (e.g. v1.1.x ) - all PRs with new functionality should target this branch PRs with hot-fixes target main branch Chart versioning policy Versioning scheme is SemVer . version : should increase when changes in chart are made appVersion : should increase when changes in jsonnet/ folder are made (when appVersion is increased, version has to be too) version and appVersion in release branch are set to same version as new release will be","title":"dNation Kubernetes Monitoring Contributing Guidelines"},{"location":"CONTRIBUTING/#dnation-kubernetes-monitoring-contributing-guidelines","text":"Thank you for considering to contribute to dNation Kubernetes Monitoring project. When contributing to this repository, please first discuss the change you wish to make via issue, email, or by any other method with the owners of this repository before making a change.","title":"dNation Kubernetes Monitoring Contributing Guidelines"},{"location":"CONTRIBUTING/#pull-request-checklist","text":"Before sending your pull requests, make sure you followed this list. Read Contributing Guidelines Read Code of Conduct Read Commit Message Convention Read How To simplify your local development Set up the Developer Certificate of Origin (DCO) Include a License at the top of new files Update the Readme with details of changes to the interface In case the pull request would update the version number, please edit the version number in all appropriate files e.g. Chart.yaml . Read more about our chart versioning policy . Choose appropriate base branch for pull request. Read more about our release policy . You may merge the Pull Request once you have the sign-off of other developer, or if you don't have the permission to do that, you may request the reviewer to merge it for you","title":"Pull Request Checklist"},{"location":"CONTRIBUTING/#developer-certificate-of-origin-dco","text":"The Developer Certificate of Origin (DCO) is a legally binding statement that asserts that you are the creator of your contribution, and that you wish to allow dNation Kubernetes Monitoring project to use your work. Acknowledgement of this permission is done using a sign-off process in Git. The sign-off is a simple line at the end of the explanation for the patch. The text of the DCO is available on developercertificate.org . If you are willing to agree to these terms, you just add a line to every git commit message: Signed-off-by: Joe Smith <joe.smith@email.com> If you set your user.name and user.email as part of your git configuration, you can sign your commit automatically with git commit -s . Unfortunately, you have to use your real name (i.e., pseudonyms or anonymous contributions cannot be made). This is because the DCO is a legally binding document, granting the dNation Kubernetes Monitoring project to use your work.","title":"Developer Certificate of Origin (DCO)"},{"location":"CONTRIBUTING/#license-on-the-top-of-file","text":"/* Copyright 2020 The dNation Kubernetes Monitoring Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */","title":"License on the top of file"},{"location":"CONTRIBUTING/#release-policy","text":"Release branch vMajor.Minor.x (e.g. v1.1.x ) - all PRs with new functionality should target this branch PRs with hot-fixes target main branch","title":"Release policy"},{"location":"CONTRIBUTING/#chart-versioning-policy","text":"Versioning scheme is SemVer . version : should increase when changes in chart are made appVersion : should increase when changes in jsonnet/ folder are made (when appVersion is increased, version has to be too) version and appVersion in release branch are set to same version as new release will be","title":"Chart versioning policy"},{"location":"GETTING_STARTED/","text":"Dashboards and Alerts only Installation Installation on top of an existing monitoring infrastructure (Prometheus Operator, Grafana and Loki are already installed). Installation Prerequisites * Helm3 dNation Kubernetes Monitoring helm chart is hosted in the dNation helm repository . # Add dNation helm repository helm repo add dnationcloud https://dnationcloud.github.io/helm-hub/ helm repo update # Install dNation Kubernetes Monitoring helm install dnation-kubernetes-monitoring dnationcloud/dnation-kubernetes-monitoring Search for Monitoring dashboard. The fun starts here :). If you want to set the Monitoring dashboard as a home dashboard follow here . If you're experiencing issues please read the documentation and FAQ . You should set the external label cluster for your Prometheus instance and this label should be the same, as the one defined in the label field in values.yaml for your cluster monitoring. E.g., if you installed Prometheus via kube-prometheus-stack helm chart: kube-prometheus-stack : prometheus : prometheusSpec : externalLabels : cluster : \"observer-cluster\" Configuration Default values for dNation Kubernetes Monitoring are defined by merging of jsonnet/config.libsonnet and chart/values.yaml files. Full list of possible configuration parameters are listed in the project documentation . All default values can be overridden as in standard helm chart, see examples in helpers directory.","title":"Dashboards and Alerts only Installation"},{"location":"GETTING_STARTED/#dashboards-and-alerts-only-installation","text":"Installation on top of an existing monitoring infrastructure (Prometheus Operator, Grafana and Loki are already installed).","title":"Dashboards and Alerts only Installation"},{"location":"GETTING_STARTED/#installation","text":"Prerequisites * Helm3 dNation Kubernetes Monitoring helm chart is hosted in the dNation helm repository . # Add dNation helm repository helm repo add dnationcloud https://dnationcloud.github.io/helm-hub/ helm repo update # Install dNation Kubernetes Monitoring helm install dnation-kubernetes-monitoring dnationcloud/dnation-kubernetes-monitoring Search for Monitoring dashboard. The fun starts here :). If you want to set the Monitoring dashboard as a home dashboard follow here . If you're experiencing issues please read the documentation and FAQ . You should set the external label cluster for your Prometheus instance and this label should be the same, as the one defined in the label field in values.yaml for your cluster monitoring. E.g., if you installed Prometheus via kube-prometheus-stack helm chart: kube-prometheus-stack : prometheus : prometheusSpec : externalLabels : cluster : \"observer-cluster\"","title":"Installation"},{"location":"GETTING_STARTED/#configuration","text":"Default values for dNation Kubernetes Monitoring are defined by merging of jsonnet/config.libsonnet and chart/values.yaml files. Full list of possible configuration parameters are listed in the project documentation . All default values can be overridden as in standard helm chart, see examples in helpers directory.","title":"Configuration"},{"location":"chart/","text":"dNation Kubernetes Monitoring Please follow instructions described in dNation Kubernetes Monitoring project.","title":"dNation Kubernetes Monitoring"},{"location":"chart/#dnation-kubernetes-monitoring","text":"Please follow instructions described in dNation Kubernetes Monitoring project.","title":"dNation Kubernetes Monitoring"},{"location":"docs/AWS_doc/","text":"Get Started With dNation Cloud Chart Using Amazon EKS And The AWS Marketplace Introduction Amazon Web Services (AWS) provides a number of different cloud and container services, including the Amazon Elastic Container Service for Kubernetes (EKS), which allows users to quickly and easily create Kubernetes clusters in the cloud. But starting up a cluster is just the beginning: the next step is to deploy applications and monitor them. That\u2019s where this tutorial comes in. It will walk you, step by step, through the process of using the AWS Marketplace to deploy dNation Kubernetes monitoring on a running EKS cluster. Overview This guide will walk you through the process of deploying and managing applications in an EKS cluster using the AWS Marketplace and Helm . This guide will show you the steps to deploy the dNation Kubernetes Monitoring Helm chart on your EKS cluster with Helm. Here are the steps you\u2019ll follow in this tutorial: Subscribe to the dNation Kubernetes Monitoring using the AWS Marketplace Deploy the dNation Kubernetes Monitoring Helm chart on EKS through Helm Log in and start using dNation Kubernetes Monitoring The next sections will walk you through these steps in detail. Assumptions And Prerequisites This guide assumes that: You have an active AWS account. If you don\u2019t have this, create a new account . You have a running EKS cluster with a minimal two nodes with allocatable 11 pods per node, e.g. instances like t2.small or t3.small and higher. And also Helm 3.x, kubectl are installed. Step 1: Subscribe To The dNation Kubernetes Monitoring Using The AWS Marketplace At the end of this step, you will have subscribed to the dNation Kubernetes Monitoring solution in the AWS Marketplace. Follow these steps: Log in to the AWS Marketplace . Search for the dNation Kubernetes Monitoring by entering the search term \u201cdNation kubernetes monitoring\u201d in the search bar at the top. Select the dNation Kubernetes Monitoring in the list of search results. On the product detail page, review the details of the solution and click the \u201cContinue to subscribe\u201d button. On the product subscription page, select \u201cContinue to configuration\u201d as accept the terms and pricing. On the product configuration page, select delivery method as \u201cContainer\u201d , select Software version and click the \u201cContinue to Launch\u201c. On the launch page, select \u201cView container image details\u201c and copy the URL to the AWS Marketplace registry. You will need these details in installation. Review your configuration, select \u201cDeployment Guide\u201c and follow step-by-step instructions for installing dNation Kubernetes Monitoring. Step 2: Deploy dNation Kubernetes Monitoring Helm Chart On EKS At the end of this step, you will have deployed dNation Kubernetes Monitoring on your EKS cluster. The next step is to deploy dNation Kubernetes Monitoring on your EKS cluster using AWS repository, which you obtained in a previous step. The easiest way to do this is with a Helm chart. Follow these steps: Add dNation helm repository and update it. # Add dNation helm repository helm repo add dnationcloud https://dnationcloud.github.io/helm-hub/ helm repo update It is a good practice to install new packages in a separate namespace, as it is easier to manage it this way. Create a new namespace, for example call it \"monitoring\" kubectl create namespace monitoring In case your current Kubernetes installation doesn't contain Prometheus Operator, Grafana or Loki, please install dNation Kubernetes Monitoring Stack helm chart (recommended) with dNation Kubernetes Monitoring Chart # Install dNation Kubernetes Monitoring Stack with dNation Kubernetes Monitoring chart helm upgrade --install dnation-kubernetes-monitoring-stack dnationcloud/dnation-kubernetes-monitoring-stack --namespace monitoring --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = <image-tag> --version = <helm-chart-version> Example of installation for helm chart version 1.1.2 : # Install dNation Kubernetes Monitoring Stack with dNation Kubernetes Monitoring chart helm upgrade --install dnation-kubernetes-monitoring-stack dnationcloud/dnation-kubernetes-monitoring-stack --namespace monitoring --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = 0 .2.0 --version = 1 .1.2 If your current Kubernetes installation already contains Prometheus Operator, Grafana and Loki, please follow this # Install dNation Kubernetes Monitoring helm upgrade --install dnation-kubernetes-monitoring dnationcloud/dnation-kubernetes-monitoring --namespace monitoring --set dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = <image-tag> --version = <helm-chart-version> Example of installation for helm chart version 1.1.2 : # Install dNation Kubernetes Monitoring helm upgrade --install dnation-kubernetes-monitoring dnationcloud/dnation-kubernetes-monitoring --namespace monitoring --set dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = 0 .2.0 --version = 1 .1.2 Check its status by running kubectl get pods --namespace monitoring Step 3: Log In And Start Using dNation Kubernetes Monitoring At the end of this step, you will have logged in to the dNation Kubernetes Monitoring. To log in to the dNation Kubernetes Monitoring dashboard, follow these steps: Get your 'admin' user password by running kubectl --namespace monitoring get secret dnation-kubernetes-monitoring-stack-grafana -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo Use Port Forwarding if you want to access the Grafana server from outside your cluster export POD_NAME = $( kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=dnation-kubernetes-monitoring-stack\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace monitoring port-forward $POD_NAME 3000 If you didn't modify the default values the Grafana server is exposed by ClusterIP service and can be accessed via port 80 on the DNS name. Usually at the URL http://DNS/ or if you used Port Forwarding at http://localhost:3000/ Login with the password you obtained at the first bullet point and the username: 'admin', then you should arrive at 'Home' page Click on the \"Search\" icon on left hand side and search for Monitoring dashboard in the dNation directory and you will see \"L0 layer\", where your cluster and hosts will be displayed. If you want to see more information about your cluster, just drill down by left-clicking on the state panel. The fun starts here :) If you want to set the Monitoring dashboard as a home dashboard follow here . If you're experiencing issues please read the documentation and FAQ . Happy Monitoring ! Useful Links To learn more about the topics discussed in this tutorial, use the links below: Amazon EKS Kubernetes Helm dNation Kubernetes Monitoring dNation Cloud","title":"AWS"},{"location":"docs/AWS_doc/#get-started-with-dnation-cloud-chart-using-amazon-eks-and-the-aws-marketplace","text":"","title":"Get Started With dNation Cloud Chart Using Amazon EKS And The AWS Marketplace"},{"location":"docs/AWS_doc/#introduction","text":"Amazon Web Services (AWS) provides a number of different cloud and container services, including the Amazon Elastic Container Service for Kubernetes (EKS), which allows users to quickly and easily create Kubernetes clusters in the cloud. But starting up a cluster is just the beginning: the next step is to deploy applications and monitor them. That\u2019s where this tutorial comes in. It will walk you, step by step, through the process of using the AWS Marketplace to deploy dNation Kubernetes monitoring on a running EKS cluster.","title":"Introduction"},{"location":"docs/AWS_doc/#overview","text":"This guide will walk you through the process of deploying and managing applications in an EKS cluster using the AWS Marketplace and Helm . This guide will show you the steps to deploy the dNation Kubernetes Monitoring Helm chart on your EKS cluster with Helm. Here are the steps you\u2019ll follow in this tutorial: Subscribe to the dNation Kubernetes Monitoring using the AWS Marketplace Deploy the dNation Kubernetes Monitoring Helm chart on EKS through Helm Log in and start using dNation Kubernetes Monitoring The next sections will walk you through these steps in detail.","title":"Overview"},{"location":"docs/AWS_doc/#assumptions-and-prerequisites","text":"This guide assumes that: You have an active AWS account. If you don\u2019t have this, create a new account . You have a running EKS cluster with a minimal two nodes with allocatable 11 pods per node, e.g. instances like t2.small or t3.small and higher. And also Helm 3.x, kubectl are installed.","title":"Assumptions And Prerequisites"},{"location":"docs/AWS_doc/#step-1-subscribe-to-the-dnation-kubernetes-monitoring-using-the-aws-marketplace","text":"At the end of this step, you will have subscribed to the dNation Kubernetes Monitoring solution in the AWS Marketplace. Follow these steps: Log in to the AWS Marketplace . Search for the dNation Kubernetes Monitoring by entering the search term \u201cdNation kubernetes monitoring\u201d in the search bar at the top. Select the dNation Kubernetes Monitoring in the list of search results. On the product detail page, review the details of the solution and click the \u201cContinue to subscribe\u201d button. On the product subscription page, select \u201cContinue to configuration\u201d as accept the terms and pricing. On the product configuration page, select delivery method as \u201cContainer\u201d , select Software version and click the \u201cContinue to Launch\u201c. On the launch page, select \u201cView container image details\u201c and copy the URL to the AWS Marketplace registry. You will need these details in installation. Review your configuration, select \u201cDeployment Guide\u201c and follow step-by-step instructions for installing dNation Kubernetes Monitoring.","title":"Step 1: Subscribe To The dNation Kubernetes Monitoring Using The AWS Marketplace"},{"location":"docs/AWS_doc/#step-2-deploy-dnation-kubernetes-monitoring-helm-chart-on-eks","text":"At the end of this step, you will have deployed dNation Kubernetes Monitoring on your EKS cluster. The next step is to deploy dNation Kubernetes Monitoring on your EKS cluster using AWS repository, which you obtained in a previous step. The easiest way to do this is with a Helm chart. Follow these steps: Add dNation helm repository and update it. # Add dNation helm repository helm repo add dnationcloud https://dnationcloud.github.io/helm-hub/ helm repo update It is a good practice to install new packages in a separate namespace, as it is easier to manage it this way. Create a new namespace, for example call it \"monitoring\" kubectl create namespace monitoring In case your current Kubernetes installation doesn't contain Prometheus Operator, Grafana or Loki, please install dNation Kubernetes Monitoring Stack helm chart (recommended) with dNation Kubernetes Monitoring Chart # Install dNation Kubernetes Monitoring Stack with dNation Kubernetes Monitoring chart helm upgrade --install dnation-kubernetes-monitoring-stack dnationcloud/dnation-kubernetes-monitoring-stack --namespace monitoring --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = <image-tag> --version = <helm-chart-version> Example of installation for helm chart version 1.1.2 : # Install dNation Kubernetes Monitoring Stack with dNation Kubernetes Monitoring chart helm upgrade --install dnation-kubernetes-monitoring-stack dnationcloud/dnation-kubernetes-monitoring-stack --namespace monitoring --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = 0 .2.0 --version = 1 .1.2 If your current Kubernetes installation already contains Prometheus Operator, Grafana and Loki, please follow this # Install dNation Kubernetes Monitoring helm upgrade --install dnation-kubernetes-monitoring dnationcloud/dnation-kubernetes-monitoring --namespace monitoring --set dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = <image-tag> --version = <helm-chart-version> Example of installation for helm chart version 1.1.2 : # Install dNation Kubernetes Monitoring helm upgrade --install dnation-kubernetes-monitoring dnationcloud/dnation-kubernetes-monitoring --namespace monitoring --set dnation-kubernetes-jsonnet-translator.image.repository = 709825985650 .dkr.ecr.us-east-1.amazonaws.com/dnation/kubernetes-jsonnet-translator --set dnation-kubernetes-monitoring.dnation-kubernetes-jsonnet-translator.image.tag = 0 .2.0 --version = 1 .1.2 Check its status by running kubectl get pods --namespace monitoring","title":"Step 2: Deploy dNation Kubernetes Monitoring Helm Chart On EKS"},{"location":"docs/AWS_doc/#step-3-log-in-and-start-using-dnation-kubernetes-monitoring","text":"At the end of this step, you will have logged in to the dNation Kubernetes Monitoring. To log in to the dNation Kubernetes Monitoring dashboard, follow these steps: Get your 'admin' user password by running kubectl --namespace monitoring get secret dnation-kubernetes-monitoring-stack-grafana -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo Use Port Forwarding if you want to access the Grafana server from outside your cluster export POD_NAME = $( kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=dnation-kubernetes-monitoring-stack\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace monitoring port-forward $POD_NAME 3000 If you didn't modify the default values the Grafana server is exposed by ClusterIP service and can be accessed via port 80 on the DNS name. Usually at the URL http://DNS/ or if you used Port Forwarding at http://localhost:3000/ Login with the password you obtained at the first bullet point and the username: 'admin', then you should arrive at 'Home' page Click on the \"Search\" icon on left hand side and search for Monitoring dashboard in the dNation directory and you will see \"L0 layer\", where your cluster and hosts will be displayed. If you want to see more information about your cluster, just drill down by left-clicking on the state panel. The fun starts here :) If you want to set the Monitoring dashboard as a home dashboard follow here . If you're experiencing issues please read the documentation and FAQ . Happy Monitoring !","title":"Step 3: Log In And Start Using dNation Kubernetes Monitoring"},{"location":"docs/AWS_doc/#useful-links","text":"To learn more about the topics discussed in this tutorial, use the links below: Amazon EKS Kubernetes Helm dNation Kubernetes Monitoring dNation Cloud","title":"Useful Links"},{"location":"docs/README_DOCS/","text":"Kubernetes Monitoring Documentation Generate documentation Prerequisites - Python3 pip3 install -r \"docs/requirements.txt\" make docs-generate Afterwards folder docs/site with static website is created. Local development Python script docs/generate_md_docs.py is responsible for creating markdown documents from configuration files. Mkdocs tool is used for generating documentation website from markdown files. Tool comes with built-in dev-server that can be used to preview work on documentation. # whole project is copied inside docs/project folder rsync -Rr ./ docs/project cd docs/project python3 docs/generate_md_docs.py cd .. mkdocs serve Mkdocs doesn't have access to files outside docs_dir (in our case docs/project ) and configuration file ( mkdocs.yaml ) has to be at least one level above docs_dir in filesystem tree. Therefore whole project has to be copied inside docs/project to allow Mkdocs to access files like chart/README.md or helpers/FAQ.md . To see changes at dev-server, files inside docs/project has to be modified. After development is done, changes has to be copied to kubernetes-monitoring folder and command make docs-generate has to be run. Other option that avoids copying changes is using symlink. # symlink to kubernets-monitoring folder is created inside docs folder ln -s .. docs/project python3 docs/generate_md_docs.py cd docs mkdocs serve --no-livereload Disadvantage is unability to use livereload because of 'infinite path' of symlink ( docs/project/docs/project/... ). Server has to be restarted to reload changes. After development is done, symlink has to be deleted and command make docs-generate run. CI/CD Github workflow is used to regenerate documentation and deploy site to branch gh-pages if changes are made inside docs folder, any REAMDE file or in configuration files.","title":"Kubernetes Monitoring Documentation"},{"location":"docs/README_DOCS/#kubernetes-monitoring-documentation","text":"","title":"Kubernetes Monitoring Documentation"},{"location":"docs/README_DOCS/#generate-documentation","text":"Prerequisites - Python3 pip3 install -r \"docs/requirements.txt\" make docs-generate Afterwards folder docs/site with static website is created.","title":"Generate documentation"},{"location":"docs/README_DOCS/#local-development","text":"Python script docs/generate_md_docs.py is responsible for creating markdown documents from configuration files. Mkdocs tool is used for generating documentation website from markdown files. Tool comes with built-in dev-server that can be used to preview work on documentation. # whole project is copied inside docs/project folder rsync -Rr ./ docs/project cd docs/project python3 docs/generate_md_docs.py cd .. mkdocs serve Mkdocs doesn't have access to files outside docs_dir (in our case docs/project ) and configuration file ( mkdocs.yaml ) has to be at least one level above docs_dir in filesystem tree. Therefore whole project has to be copied inside docs/project to allow Mkdocs to access files like chart/README.md or helpers/FAQ.md . To see changes at dev-server, files inside docs/project has to be modified. After development is done, changes has to be copied to kubernetes-monitoring folder and command make docs-generate has to be run. Other option that avoids copying changes is using symlink. # symlink to kubernets-monitoring folder is created inside docs folder ln -s .. docs/project python3 docs/generate_md_docs.py cd docs mkdocs serve --no-livereload Disadvantage is unability to use livereload because of 'infinite path' of symlink ( docs/project/docs/project/... ). Server has to be restarted to reload changes. After development is done, symlink has to be deleted and command make docs-generate run.","title":"Local development"},{"location":"docs/README_DOCS/#cicd","text":"Github workflow is used to regenerate documentation and deploy site to branch gh-pages if changes are made inside docs folder, any REAMDE file or in configuration files.","title":"CI/CD"},{"location":"docs/documentation-intro/","text":"dnation Kubernetes Monitoring Docs This is generated documentation from configuration files of Kubernetes Monitoring . Each configuration parameter can be overriden by providing custom values.yaml during helm installation.","title":"Documentation intro"},{"location":"docs/documentation-intro/#dnation-kubernetes-monitoring-docs","text":"This is generated documentation from configuration files of Kubernetes Monitoring . Each configuration parameter can be overriden by providing custom values.yaml during helm installation.","title":"dnation Kubernetes Monitoring Docs"},{"location":"docs/documentation/","text":"dnation Kubernetes Monitoring Docs This is generated documentation from configuration files of Kubernetes Monitoring . Each configuration parameter can be overriden by providing custom values.yaml during helm installation. Property Value clusterMonitoring clusterMonitoring commonLabels {} dnation-kubernetes-jsonnet-translator dnation-kubernetes-jsonnet-translator fullnameOverride \"\" grafanaDashboards grafanaDashboards hostMonitoring hostMonitoring nameOverride \"\" namespaceOverride \"\" prometheusRules prometheusRules templates templates clusterMonitoring Property Value clusters - apps: [] description: Kubernetes cluster monitoring label: observer-cluster name: K8sCluster enabled true dnation-kubernetes-jsonnet-translator Property Value enabled true image image image Property Value args - --libsonnet - https://github.com/grafana/grafonnet-lib/grafonnet@daad85cf3fad3580e58029414630e29956aefe21 - https://github.com/thelastpickle/grafonnet-polystat-panel@275a48de57afdac0d72219d82863d8ab8bd0e682 grafanaDashboards Property Value color color constants constants dataLinkCommonArgs \"refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to\" dataLinkCommonArgsNoCluster \"refresh=10s&var-datasource=$datasource&from=$__from&to=$__to\" editable true enable true ids ids isLoki true labelGrafana grafana_dashboard: '1' labelJsonnet grafana_dashboard_jsonnet: '1' refresh \"10s\" selectors selectors severityColors severityColors tags tags templateRefresh \"time\" templateSort 5 time_from \"now-5m\" tooltip \"shared_crosshair\" color Property Value black \"#000000\" blue \"#5794f2\" gray \"#858187\" green \"#56a64b\" lightblue \"#8ab8ff\" orange \"#ff780a\" pink \"#fce2de\" purple \"#a352cc\" red \"#e02f44\" white \"#ffffff\" yellow \"#fade2a\" constants Property Value infinity 100000000000000005366162204393472 maxWarnings 10000 ids Property Value alertClusterOverview \"alertclusteroverview\" alertHostOverview \"alerthostoverview\" alertVMOverview \"alertvmoverview\" apache \"apache\" apiServer \"apiserver\" autoscaler \"autoscaler\" cAdvisor \"cadvisor\" containerDetail \"containerdetail\" containerOverview \"containeroverview\" controllerManager \"controllermanager\" cpuNamespaceOverview \"cpunamespaceoverview\" cpuOverview \"cpuoverview\" daemonSetOverview \"daemonsetoverview\" deploymentOverview \"deploymentoverview\" diskOverview \"diskoverview\" etcd \"etcd\" harbor \"harbor\" hostMonitoring \"hostmonitoring\" javaActuator \"javaactuator\" jobOverview \"joboverview\" jvm \"jvm\" k8sMonitoring \"k8smonitoring\" kubelet \"kubelet\" lokiDistributed \"loki-distributed\" memoryNamespaceOverview \"memorynamespaceoverview\" memoryOverview \"memoryoverview\" monitoring \"monitoring\" mysqlExporter \"mysqlexporter\" networkNamespaceOverview \"networknamespaceoverview\" networkOverview \"networkoverview\" nginxIngress \"nginxingress\" nginxNrpe \"nginxnrpe\" nginxVts \"nginxvts\" nginxVtsEnhanced \"nginxvtsenhanced\" nginxVtsEnhancedLegacy \"nginxvtsenhancedlegacy\" nginxVtsLegacy \"nginxvtslegacy\" nodeExporter \"nodeexporter\" nodeOverview \"nodeoverview\" persistentVolumes \"persistentvolumes\" phpFpm \"phpfpm\" podOverview \"podoverview\" postfix \"postfix\" prometheus \"prometheus\" proxy \"proxy\" pvcOverview \"pvcoverview\" pythonFlask \"pythonflask\" rabbitmq \"rabbitmq\" scheduler \"scheduler\" sslExporter \"ssl-exporter\" statefulSet \"statefulset\" statefulSetOverview \"statefulsetoverview\" vmMonitoring \"vmmonitoring\" websocket \"websocket\" selectors Property Value apiServer \"job=\\\"apiserver\\\"\" controllerManager \"job=\\\"kube-controller-manager\\\"\" etcd \"job=\\\"kube-etcd\\\"\" kubelet \"job=\\\"kubelet\\\"\" proxy \"job=\\\"kube-proxy\\\"\" scheduler \"job=\\\"kube-scheduler\\\"\" severityColors Property Value critical \"red\" default \"green\" invalid \"black\" warning \"orange\" tags Property Value k8sApps - k8s - app - L1 k8sAppsMain - k8s - app - L0 k8sContainer - k8s - container - L3 k8sHostsMain - k8s - host - L1 k8sMonitoring - k8s - monitoring - L1 k8sMonitoringMain - k8s - cluster - host - L0 k8sNodeExporter - k8s - nodeexporter - L3 k8sOverview - k8s - overview - L2 k8sPVC - k8s - pvc - L3 k8sStatefulSet - k8s - statefulset - L3 k8sSystem - k8s - system - L2 k8sVMs - k8s - vm - L2 hostMonitoring Property Value enabled false hosts [] prometheusRules Property Value alertGroupCluster \"Cluster\" alertGroupClusterApp \"ClusterApp\" alertGroupClusterVM \"ClusterVM\" alertGroupClusterVMApp \"ClusterVMApp\" alertGroupHost \"Host\" alertGroupHostApp \"HostApp\" alertInterval \"5m\" alertNamePrefix \"KubernetesMonitoring\" enable true labelJsonnet prometheus_rule_jsonnet: '1' labelPrometheus prometheus_rule: '1' templates Property Value L0 L0 L1 L1 L2 L2 RecordRules - expr: node_uname_info{job=~\"node-exporter\"} and on(nodename) label_replace(kube_node_role{role=~\"control-plane\"}, \"nodename\", \"$1\", \"node\", \"(.+)\") record: master_uname_info - expr: node_uname_info{job=~\"node-exporter\"} unless on(nodename) label_replace(kube_node_role{role=~\"control-plane\"}, \"nodename\", \"$1\", \"node\", \"(.+)\") record: worker_uname_info commonThresholds commonThresholds templateBases templateBases L0 Property Value host host k8s k8s host Property Value main main main Property Value panel expr: ((sum(up{job=~\"%(job)s\"}) or on() vector(0)) == bool 0) * (-1) + sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"warning\", job=~\"%(job)s\", alertgroup=~\"%(groupHost)s|%(groupHostApp)s\"} OR on() vector(0)) + sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"critical\", job=~\"%(job)s\", alertgroup=~\"%(groupHost)s|%(groupHostApp)s\"} OR on() vector(0)) * %(maxWarnings)d graphMode: none gridPos: h: 3 w: 4 mappings: - from: -1 text: Down to: -1 type: 2 value: '' - from: 0 text: OK to: 0 type: 2 value: '' - from: 1 text: Warning to: 9999 type: 2 value: '' - from: 10000 text: Critical to: 100000000000000005366162204393472 type: 2 value: '' thresholds: critical: 10000 lowest: 0 operator: '>=' warning: 1 unit: none k8s Property Value main main main Property Value panel expr: ((sum(up{job=~\"node-exporter\", cluster=\"%(cluster)s\"}) or on() vector(0)) == bool 0) * (-1) + sum(ALERTS{alertname!=\"Watchdog\", cluster=\"%(cluster)s\", alertstate=\"firing\", severity=\"warning\", alertgroup=~\"%(groupCluster)s|%(groupApp)s\"} OR on() vector(0)) + sum(ALERTS{alertname!=\"Watchdog\", cluster=\"%(cluster)s\", alertstate=\"firing\", severity=\"critical\", alertgroup=~\"%(groupCluster)s|%(groupApp)s\"} OR on() vector(0)) * %(maxWarnings)d graphMode: none gridPos: h: 3 w: 4 mappings: - from: -1 text: Down to: -1 type: 2 value: '' - from: 0 text: OK to: 0 type: 2 value: '' - from: 1 text: Warning to: 9999 type: 2 value: '' - from: 10000 text: Critical to: 100000000000000005366162204393472 type: 2 value: '' thresholds: critical: 10000 lowest: 0 operator: '>=' warning: 1 unit: none L1 Property Value host host hostApps hostApps k8s k8s k8sApps k8sApps vm vm vmApps vmApps host Property Value overallNetworkErrors overallNetworkErrors overallUtilizationCPU overallUtilizationCPU overallUtilizationDisk overallUtilizationDisk overallUtilizationRAM overallUtilizationRAM targetDown targetDown totalCores totalCores totalDisk totalDisk totalRAM totalRAM usedCores usedCores usedDisk usedDisk usedRAM usedRAM overallNetworkErrors Property Value alert customLables: alertgroup: Host expr: sum(rate(node_network_transmit_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High Overall Network Errors Count {{ $value }}%' name: HostNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(rate(node_network_transmit_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) gridPos: x: 18 y: 6 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps overallUtilizationCPU Property Value alert customLables: alertgroup: Host expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"%s\", mode=\"idle\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High CPU Overall Utilization {{ $value }}%' name: HostCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) gridPos: x: 0 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationDisk Property Value alert customLables: alertgroup: Host expr: round((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High Disk Overall Utilization {{ $value }}%' name: HostDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: x: 12 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationRAM Property Value alert customLables: alertgroup: Host expr: round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High RAM Overall Utilization {{ $value }}%' name: HostRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info))) * 100)) gridPos: x: 6 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization targetDown Property Value alert customLables: alertgroup: Host expr: 100 * (count by(job, namespace, service) (up{job=~\"%s\"} == 0) / count by(job, namespace, service) (up{job=~\"%s\"})) message: '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.' name: HostTargetDown thresholds: critical: 90 operator: '>=' warning: 10 panel null totalCores Property Value panel colorMode: value expr: count(node_cpu_seconds_total{job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 9 thresholds: color: '#858187' value: title: Total Cores unit: none totalDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes totalRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes usedCores Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m])))) * count(node_cpu_seconds_total{job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 9 thresholds: color: '#858187' value: title: Used Cores unit: none usedDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{job=~\"$job\"}) - sum(node_filesystem_free_bytes{job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes usedRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{job=~\"$job\"}) * (((1 - sum(node_memory_MemAvailable_bytes{job=~\"$job\"}) / sum(node_memory_MemTotal_bytes{job=~\"$job\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes hostApps Property Value apache apache autoscaler autoscaler cAdvisor cAdvisor genericApp genericApp harbor harbor javaActuator javaActuator jvm jvm lokiDistributed lokiDistributed mysqlExporter mysqlExporter nginxIngress nginxIngress nginxIngressCertificateExpiry nginxIngressCertificateExpiry nginxNrpe nginxNrpe nginxVts nginxVts nginxVtsEnhanced nginxVtsEnhanced nginxVtsEnhancedLegacy nginxVtsEnhancedLegacy nginxVtsLegacy nginxVtsLegacy phpFpm phpFpm postfix postfix prometheus prometheus pythonFlask pythonFlask rabbitmq rabbitmq sslExporter sslExporter websocket websocket apache Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 autoscaler Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", job=~\".+\"}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", job=~\".+\"}) * 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Autoscaler Health Low {{ $value }}%' name: HostAppAutoscalerHealthLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - autoscaler panel expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", %(job)s}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", %(job)s}) * 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 cAdvisor Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 genericApp Property Value alert {} default false panel description: GenericApp template. Used when application monitoring is requested but appropriate template was not found. expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 gridPos: w: 4 thresholds: critical: 95 operator: < warning: 99 harbor Property Value alert customLables: alertgroup: HostApp expr: harbor_up{cluster=\"$cluster\", job=~\".+\"} linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Harbor component \"{{ $labels.component }}\" is down' name: HostAppHarborComponentDown thresholds: critical: 0 operator: == warning: 0 default false linkTo - harbor panel expr: (sum(harbor_up{cluster=\"$cluster\", %(job)s}) / count(harbor_up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 javaActuator Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Java Actuator Heap High {{ $value }}%' name: HostAppJavaActuatorHeapHigh thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 default false linkTo - javaactuator panel expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 jvm Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 lokiDistributed Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 mysqlExporter Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxIngress Property Value alert customLables: alertgroup: HostApp expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\", status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100)) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx Ingress Success Rate (non-4|5xx responses) Low {{ printf \"%.0f\" $value }}%' name: HostAppNginxIngressSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxingress panel expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s, status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100)) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxIngressCertificateExpiry Property Value alert customLables: alertgroup: HostApp expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", job=~\".+\"} - time()) / 60 / 60 / 24 linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx Ingress Certificate Expiry in {{ printf \"%.2f\" $value }} days' name: HostAppNginxIngressCertificateExpiry thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 8 default false linkTo - nginxingress panel dataLinks: - title: Detail url: /d/nginxingress?var-job=%(job)s&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to decimals: 0 expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", %(job)s} - time()) OR on() vector(-100000000000000005366162204393472) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s nginxNrpe Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxVts Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvts panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhanced Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhanced panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhancedLegacy Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhancedlegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsLegacy Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtslegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 phpFpm Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 postfix Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (postfix_size{cluster=\"$cluster\", job=~\".+\"})) linkGetParams: var-job={{ $labels.job }} mappings: - text: '-' type: 1 value: -1 message: 'HostApp {{ $labels.job }}: Postfix Queue Size High {{ $value }}%' name: HostAppPostfixQueueSizeHigh thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 default false linkTo - postfix panel expr: (sum by (job) (postfix_size{cluster=\"$cluster\", %(job)s})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 unit: mailq prometheus Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 pythonFlask Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\",status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Python Flask Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppPythonFlaskSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - pythonflask panel expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s,status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 rabbitmq Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 sslExporter Property Value alert {} default false linkTo - ssl-exporter panel decimals: 0 expr: bottomk(1,ssl_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_file_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubeconfig_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubernetes_cert_not_after{cluster=\"$cluster\"}-time()) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s websocket Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 k8s Property Value apiServerHealth apiServerHealth controllerManagerHealth controllerManagerHealth daemonSetsHealth daemonSetsHealth deploymentsHealth deploymentsHealth etcdHealth etcdHealth kubeletHealth kubeletHealth mostUtilizedMasterNodeCPU mostUtilizedMasterNodeCPU mostUtilizedMasterNodeDisk mostUtilizedMasterNodeDisk mostUtilizedMasterNodeNetworkErrors mostUtilizedMasterNodeNetworkErrors mostUtilizedMasterNodeRAM mostUtilizedMasterNodeRAM mostUtilizedPVC mostUtilizedPVC mostUtilizedWorkerNodeCPU mostUtilizedWorkerNodeCPU mostUtilizedWorkerNodeDisk mostUtilizedWorkerNodeDisk mostUtilizedWorkerNodeNetworkErrors mostUtilizedWorkerNodeNetworkErrors mostUtilizedWorkerNodeRAM mostUtilizedWorkerNodeRAM nodeHealth nodeHealth overallMasterNodesNetworkErrors overallMasterNodesNetworkErrors overallUtilizationMasterNodesCPU overallUtilizationMasterNodesCPU overallUtilizationMasterNodesDisk overallUtilizationMasterNodesDisk overallUtilizationMasterNodesRAM overallUtilizationMasterNodesRAM overallUtilizationWorkerNodesCPU overallUtilizationWorkerNodesCPU overallUtilizationWorkerNodesDisk overallUtilizationWorkerNodesDisk overallUtilizationWorkerNodesRAM overallUtilizationWorkerNodesRAM overallWorkerNodesNetworkErrors overallWorkerNodesNetworkErrors proxyHealth proxyHealth pvcBound pvcBound runningContainers runningContainers runningPods runningPods runningStatefulSets runningStatefulSets schedulerHealth schedulerHealth succeededJobs succeededJobs targetDown targetDown totalCoresMasterNodes totalCoresMasterNodes totalCoresWorkerNodes totalCoresWorkerNodes totalDiskMasterNodes totalDiskMasterNodes totalDiskWorkerNodes totalDiskWorkerNodes totalRAMMasterNodes totalRAMMasterNodes totalRAMWorkerNodes totalRAMWorkerNodes usedCoresMasterNodes usedCoresMasterNodes usedCoresWorkerNodes usedCoresWorkerNodes usedDiskMasterNodes usedDiskMasterNodes usedDiskWorkerNodes usedDiskWorkerNodes usedRAMMasterNodes usedRAMMasterNodes usedRAMWorkerNodes usedRAMWorkerNodes apiServerHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"apiserver\"}) by (cluster) / count(up{job=\"apiserver\"}) by (cluster)) * 100 message: Cluster Api Server Health Low {{ $value }}% name: ClusterApiServerHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - apiserver panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"apiserver\"}) / count(up{cluster=\"$cluster\", job=\"apiserver\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 0 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: API Server controllerManagerHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-controller-manager\"}) by (cluster) / count(up{job=\"kube-controller-manager\"}) by (cluster)) * 100 message: Cluster Controller Manager Health Low {{ $value }}% name: ClusterControllerManagerHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - controllermanager panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-controller-manager\"}) / count(up{cluster=\"$cluster\", job=\"kube-controller-manager\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 4 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Controller Manager daemonSetsHealth Property Value alert customLables: alertgroup: Cluster expr: round((sum(kube_daemonset_status_updated_number_scheduled OR kube_daemonset_updated_number_scheduled) by (cluster) + sum(kube_daemonset_status_number_available) by (cluster)) / (2 * sum(kube_daemonset_status_desired_number_scheduled) by (cluster)) * 100) message: DaemonSets Health Low {{ $value }}% name: RunningDaemonSetsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - daemonSetOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round((sum(kube_daemonset_status_updated_number_scheduled{cluster=\"$cluster\"} OR kube_daemonset_updated_number_scheduled{cluster=\"$cluster\"}) + sum(kube_daemonset_status_number_available{cluster=\"$cluster\"})) / (2 * sum(kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\"})) * 100) gridPos: x: 6 y: 12 thresholds: critical: 95 operator: < warning: 99 title: DaemonSets Health deploymentsHealth Property Value alert customLables: alertgroup: Cluster expr: round((sum(kube_deployment_status_replicas_updated) by (cluster) + sum(kube_deployment_status_replicas_available) by (cluster)) / (2 * sum(kube_deployment_status_replicas) by (cluster)) * 100) message: Running Deployments Health Low {{ $value }}% name: RunningDeploymentsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - deploymentOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round((sum(kube_deployment_status_replicas_updated{cluster=\"$cluster\"}) + sum(kube_deployment_status_replicas_available{cluster=\"$cluster\"})) / (2 * sum(kube_deployment_status_replicas{cluster=\"$cluster\"})) * 100) gridPos: x: 0 y: 12 thresholds: critical: 95 operator: < warning: 99 title: Deployments Health etcdHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-etcd\"}) by (cluster) / count(up{job=\"kube-etcd\"}) by (cluster)) * 100 message: Cluster Etcd Health Low {{ $value }}% name: ClusterEtcdHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - etcd panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-etcd\"}) / count(up{cluster=\"$cluster\", job=\"kube-etcd\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 8 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Etcd kubeletHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kubelet\", metrics_path=\"/metrics\"}) by (cluster) / count(up{job=\"kubelet\", metrics_path=\"/metrics\"}) by (cluster)) * 100 message: Cluster Kubelet Health Low {{ $value }}% name: ClusterKubeletHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - kubelet panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kubelet\", metrics_path=\"/metrics\"}) / count(up{cluster=\"$cluster\", job=\"kubelet\", metrics_path=\"/metrics\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 12 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Kubelet mostUtilizedMasterNodeCPU Property Value alert customLables: alertgroup: Cluster expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, cluster) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High CPU Utilization {{ $value }}%' name: ClusterMasterNodeCPUUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/cpuoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 3 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedMasterNodeDisk Property Value alert customLables: alertgroup: Cluster expr: round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High Disk Utilization {{ $value }}%' name: ClusterMasterNodeDiskUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/diskoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) * 100)) gridPos: w: 3 x: 15 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedMasterNodeNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High Network Errors Count {{ $value }}%' name: ClusterMasterNodeNetworkErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/networkoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 21 y: 17 thresholds: critical: 15 operator: '>=' warning: 10 title: Most Affected Node unit: pps mostUtilizedMasterNodeRAM Property Value alert customLables: alertgroup: Cluster expr: round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High RAM Utilization {{ $value }}%' name: ClusterMasterNodesRAMUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/memoryoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: max(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100)) gridPos: w: 3 x: 9 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedPVC Property Value alert customLables: alertgroup: Cluster expr: sum(((kubelet_volume_stats_capacity_bytes - kubelet_volume_stats_available_bytes) / kubelet_volume_stats_capacity_bytes) * 100) by (persistentvolumeclaim, cluster) linkGetParams: var-pvc={{ $labels.persistentvolumeclaim }} message: '\"{{ $labels.persistentvolumeclaim }}\": High PVC Utilization {{ $value }}%' name: PVCUtilizationHigh thresholds: critical: 97 lowest: 0 operator: '>=' warning: 85 linkTo - pvcOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(((kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\"} - kubelet_volume_stats_available_bytes{cluster=\"$cluster\"}) / kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\"}) * 100) by (persistentvolumeclaim)) OR on() vector(-1) gridPos: w: 3 x: 21 y: 12 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 97 lowest: 0 operator: '>=' warning: 85 title: Most Utilized PVC mostUtilizedWorkerNodeCPU Property Value alert customLables: alertgroup: Cluster expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, cluster) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High CPU Utilization {{ $value }}%' name: ClusterWorkerNodeCPUUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/cpuoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 3 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedWorkerNodeDisk Property Value alert customLables: alertgroup: Cluster expr: round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High Disk Utilization {{ $value }}%' name: ClusterWorkerNodeDiskUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/diskoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) * 100)) gridPos: w: 3 x: 15 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedWorkerNodeNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High Network Errors Count {{ $value }}%' name: ClusterWorkerNodeNetworkErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/networkoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 21 y: 24 thresholds: critical: 15 operator: '>=' warning: 10 title: Most Affected Node unit: pps mostUtilizedWorkerNodeRAM Property Value alert customLables: alertgroup: Cluster expr: round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High RAM Utilization {{ $value }}%' name: ClusterWorkerNodesRAMUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/memoryoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: max(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100)) gridPos: w: 3 x: 9 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node nodeHealth Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_node_info) by (cluster) / (sum(kube_node_info) by (cluster) + sum(kube_node_spec_unschedulable) by (cluster) + sum(kube_node_status_condition{condition=~\"DiskPressure|MemoryPressure|PIDPressure\", status=~\"true|unknown\"}) by (cluster) + sum(kube_node_status_condition{condition=\"Ready\", status=~\"false|unknown\"}) by (cluster)) * 100) message: Nodes Health Low {{ $value }}% name: NodesHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - nodeOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_node_info{cluster=\"$cluster\"}) / (sum(kube_node_info{cluster=\"$cluster\"}) + sum(kube_node_spec_unschedulable{cluster=\"$cluster\"}) + sum(kube_node_status_condition{cluster=\"$cluster\", condition=~\"DiskPressure|MemoryPressure|PIDPressure\", status=~\"true|unknown\"}) + sum(kube_node_status_condition{cluster=\"$cluster\", condition=\"Ready\", status=~\"false|unknown\"}) ) * 100) gridPos: x: 0 y: 9 thresholds: critical: 95 operator: < warning: 99 title: Nodes Health overallMasterNodesNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster)) by (cluster) message: Cluster Master Nodes High Overall Network Errors Count {{ $value }}% name: ClusterMasterNodesNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - networkPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 18 y: 17 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps overallUtilizationMasterNodesCPU Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, cluster) )) * 100)) by (cluster) message: Cluster Master Nodes High CPU Overall Utilization {{ $value }}% name: ClusterMasterNodesCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - cpuPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 0 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationMasterNodesDisk Property Value alert customLables: alertgroup: Cluster expr: avg(round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) * 100 > 0)) by (cluster) message: Cluster Master Nodes High Disk Overall Utilization {{ $value }}% name: ClusterMasterNodesDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - diskPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: avg(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: w: 3 x: 12 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationMasterNodesRAM Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100)) by (cluster) message: Cluster Master Nodes High RAM Overall Utilization {{ $value }}% name: ClusterMasterNodesRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - memoryPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100)) gridPos: w: 3 x: 6 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationWorkerNodesCPU Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, cluster) )) * 100)) by (cluster) message: Cluster Worker Nodes High CPU Overall Utilization {{ $value }}% name: ClusterWorkerNodesCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - cpuPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 0 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationWorkerNodesDisk Property Value alert customLables: alertgroup: Cluster expr: avg(round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) * 100 > 0)) by (cluster) message: Cluster Worker Nodes High Disk Overall Utilization {{ $value }}% name: ClusterWorkerNodesDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - diskPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: avg(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: w: 3 x: 12 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationWorkerNodesRAM Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100)) by (cluster) message: Cluster Worker Nodes High RAM Overall Utilization {{ $value }}% name: ClusterWorkerNodesRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - memoryPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100)) gridPos: w: 3 x: 6 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallWorkerNodesNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster)) by (cluster) message: Cluster Worker Nodes High Overall Network Errors Count {{ $value }}% name: ClusterWorkerNodesNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - networkPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 18 y: 24 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps proxyHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-proxy\"}) by (cluster) / count(up{job=\"kube-proxy\"}) by (cluster)) * 100 message: Cluster Proxy Health Low {{ $value }}% name: ClusterProxyHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - proxy panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-proxy\"}) / count(up{cluster=\"$cluster\", job=\"kube-proxy\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 16 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Proxy pvcBound Property Value alert customLables: alertgroup: Cluster expr: \"round(sum(kube_persistentvolumeclaim_status_phase{phase=\\\"Bound\\\"}) by (cluster)\\ \\ / (\\nsum(kube_persistentvolumeclaim_status_phase{phase=\\\"Bound\\\"}) by (cluster)\\ \\ + sum(kube_persistentvolumeclaim_status_phase{phase=\\\"Pending\\\"}) by (cluster)\\ \\ +\\nsum(kube_persistentvolumeclaim_status_phase{phase=\\\"Lost\\\"}) by (cluster)\\n\\ ) * 100)\" message: PVC Bound Rate Low {{ $value }}% name: PVCBoundRateLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - pvcOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: \"round(sum(kube_persistentvolumeclaim_status_phase{cluster=\\\"$cluster\\\", phase=\\\"\\ Bound\\\"}) / (\\nsum(kube_persistentvolumeclaim_status_phase{cluster=\\\"$cluster\\\"\\ , phase=\\\"Bound\\\"}) + sum(kube_persistentvolumeclaim_status_phase{cluster=\\\"$cluster\\\"\\ , phase=\\\"Pending\\\"}) +\\nsum(kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", phase=\\\"Lost\\\"})\\n) * 100) OR on() vector(-1)\" gridPos: w: 3 x: 18 y: 12 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: PVC Bound runningContainers Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_pod_container_status_running) by (cluster) / (sum(kube_pod_container_status_running) by (cluster) + (count(kube_pod_container_status_terminated) by (cluster) - count(kube_pod_container_status_terminated unless ignoring(reason) kube_pod_container_status_terminated_reason{reason!=\"Completed\"}) by (cluster)) + sum(kube_pod_container_status_waiting) by (cluster)) * 100) message: Running Containers Health Low {{ $value }}% name: RunningContainersHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - containerOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_pod_container_status_running{cluster=\"$cluster\"}) / (sum(kube_pod_container_status_running{cluster=\"$cluster\"}) + (sum(kube_pod_container_status_terminated_reason{cluster=\"$cluster\", reason!=\"Completed\"}) OR vector(0)) + sum(kube_pod_container_status_waiting{cluster=\"$cluster\"})) * 100) gridPos: x: 12 y: 12 thresholds: critical: 95 operator: < warning: 99 title: Running Containers runningPods Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_pod_status_phase{phase=\"Running\"}) by (cluster) / (sum(kube_pod_status_phase{phase=\"Running\"}) by (cluster) + sum(kube_pod_status_phase{phase=\"Pending\"}) by (cluster) + sum(kube_pod_status_phase{phase=\"Failed\"}) by (cluster) + sum(kube_pod_status_phase{phase=\"Unknown\"}) by (cluster)) * 100) message: Pods Health Low {{ $value }}% name: RunningPodsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - podOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Running\"}) / (sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Running\"}) + sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Pending\"}) + sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Failed\"}) + sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Unknown\"})) * 100) gridPos: x: 12 y: 9 thresholds: critical: 95 operator: < warning: 99 title: Running Pods runningStatefulSets Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_statefulset_status_replicas_ready) by (cluster) / sum(kube_statefulset_status_replicas) by (cluster) * 100) message: StatefulSets Health Low {{ $value }}% name: RunningStatefulSetsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - statefulSetOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_statefulset_status_replicas_ready{cluster=\"$cluster\"}) / sum(kube_statefulset_status_replicas{cluster=\"$cluster\"}) * 100) gridPos: x: 6 y: 9 thresholds: critical: 95 operator: < warning: 99 title: Running StatefulSets schedulerHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-scheduler\"}) by (cluster) / count(up{job=\"kube-scheduler\"}) by (cluster)) * 100 message: Cluster Scheduler Health Low {{ $value }}% name: ClusterSchedulerHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - scheduler panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-scheduler\"}) / count(up{cluster=\"$cluster\", job=\"kube-scheduler\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 20 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Scheduler succeededJobs Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_job_status_succeeded) by (cluster) / (sum(kube_job_status_succeeded) by (cluster) + sum(kube_job_status_failed) by (cluster)) * 100) message: Succeeded Jobs Rate Low {{ $value }}% name: SucceededJobsRateLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - jobOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_job_status_succeeded{cluster=\"$cluster\"}) / (sum(kube_job_status_succeeded{cluster=\"$cluster\"}) + sum(kube_job_status_failed{cluster=\"$cluster\"})) * 100) OR on() vector(-1) gridPos: x: 18 y: 9 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Succeeded Jobs targetDown Property Value alert customLables: alertgroup: Cluster expr: 100 * (count by(job, namespace, service, cluster) (up{pod!~\"virt-launcher.*|\"} == 0) / count by(job, namespace, service, cluster) (up{pod!~\"virt-launcher.*|\"})) message: '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.' name: ClusterTargetDown thresholds: critical: 90 operator: '>=' warning: 10 panel null totalCoresMasterNodes Property Value panel colorMode: value expr: count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 20 thresholds: color: '#858187' value: title: Total Cores unit: none totalCoresWorkerNodes Property Value panel colorMode: value expr: count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 27 thresholds: color: '#858187' value: title: Total Cores unit: none totalDiskMasterNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 20 thresholds: color: '#858187' value: title: Total unit: bytes totalDiskWorkerNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 27 thresholds: color: '#858187' value: title: Total unit: bytes totalRAMMasterNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 20 thresholds: color: '#858187' value: title: Total unit: bytes totalRAMWorkerNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 27 thresholds: color: '#858187' value: title: Total unit: bytes usedCoresMasterNodes Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\", instance=~\"$masterInstance\"}[5m])))) * count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 20 thresholds: color: '#858187' value: title: Used Cores unit: none usedCoresWorkerNodes Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\", instance=~\"$workerInstance\"}[5m])))) * count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 27 thresholds: color: '#858187' value: title: Used Cores unit: none usedDiskMasterNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 20 thresholds: color: '#858187' value: title: Used unit: bytes usedDiskWorkerNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 27 thresholds: color: '#858187' value: title: Used unit: bytes usedRAMMasterNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) * (((1 - sum(node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) / sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 20 thresholds: color: '#858187' value: title: Used unit: bytes usedRAMWorkerNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) * (((1 - sum(node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) / sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 27 thresholds: color: '#858187' value: title: Used unit: bytes k8sApps Property Value apache apache autoscaler autoscaler cAdvisor cAdvisor genericApp genericApp harbor harbor javaActuator javaActuator jvm jvm lokiDistributed lokiDistributed mysqlExporter mysqlExporter nginxIngress nginxIngress nginxIngressCertificateExpiry nginxIngressCertificateExpiry nginxNrpe nginxNrpe nginxVts nginxVts nginxVtsEnhanced nginxVtsEnhanced nginxVtsEnhancedLegacy nginxVtsEnhancedLegacy nginxVtsLegacy nginxVtsLegacy phpFpm phpFpm postfix postfix prometheus prometheus pythonFlask pythonFlask rabbitmq rabbitmq sslExporter sslExporter websocket websocket apache Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 autoscaler Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", job=~\".+\"}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", job=~\".+\"}) * 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Autoscaler Health Low {{ $value }}%' name: ClusterAppAutoscalerHealthLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - autoscaler panel expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", %(job)s}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", %(job)s}) * 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 cAdvisor Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 genericApp Property Value alert {} default false panel description: GenericApp template. Used when application monitoring is requested but appropriate template was not found. expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 gridPos: w: 4 thresholds: critical: 95 operator: < warning: 99 harbor Property Value alert customLables: alertgroup: ClusterApp expr: harbor_up{cluster=\"$cluster\", job=~\".+\"} linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Harbor component \"{{ $labels.component }}\" is down' name: ClusterAppHarborComponentDown thresholds: critical: 0 operator: == warning: 0 default false linkTo - harbor panel expr: (sum(harbor_up{cluster=\"$cluster\", %(job)s}) / count(harbor_up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 javaActuator Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Java Actuator Heap High {{ $value }}%' name: ClusterAppJavaActuatorHeapHigh thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 default false linkTo - javaactuator panel expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 jvm Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 lokiDistributed Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 mysqlExporter Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxIngress Property Value alert customLables: alertgroup: ClusterApp expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\", status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100)) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx Ingress Success Rate (non-4|5xx responses) Low {{ printf \"%.0f\" $value }}%' name: ClusterAppNginxIngressSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxingress panel expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s, status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100)) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxIngressCertificateExpiry Property Value alert customLables: alertgroup: ClusterApp expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", job=~\".+\"} - time()) / 60 / 60 / 24 linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx Ingress Certificate Expiry in {{ printf \"%.2f\" $value }} days' name: ClusterAppNginxIngressCertificateExpiry thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 8 default false linkTo - nginxingress panel dataLinks: - title: Detail url: /d/nginxingress?var-job=%(job)s&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to decimals: 0 expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", %(job)s} - time()) OR on() vector(-100000000000000005366162204393472) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s nginxNrpe Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxVts Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvts panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhanced Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhanced panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhancedLegacy Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhancedlegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsLegacy Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtslegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 phpFpm Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 postfix Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (postfix_size{cluster=\"$cluster\", job=~\".+\"})) linkGetParams: var-job={{ $labels.job }} mappings: - text: '-' type: 1 value: -1 message: 'ClusterApp {{ $labels.job }}: Postfix Queue Size High {{ $value }}%' name: ClusterAppPostfixQueueSizeHigh thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 default false linkTo - postfix panel expr: (sum by (job) (postfix_size{cluster=\"$cluster\", %(job)s})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 unit: mailq prometheus Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 pythonFlask Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\",status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Python Flask Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppPythonFlaskSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - pythonflask panel expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s,status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 rabbitmq Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 sslExporter Property Value alert {} default false linkTo - ssl-exporter panel decimals: 0 expr: bottomk(1,ssl_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_file_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubeconfig_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubernetes_cert_not_after{cluster=\"$cluster\"}-time()) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s websocket Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 vm Property Value main main main Property Value panel expr: sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"warning\", job=~\"%(job)s\", alertgroup=~\"%(groupVM)s|%(groupVMApp)s\"} OR on() vector(0)) + sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"critical\", job=~\"%(job)s\", alertgroup=~\"%(groupVM)s|%(groupVMApp)s\"} OR on() vector(0)) * %(maxWarnings)d graphMode: none gridPos: h: 3 w: 4 mappings: - from: 0 text: OK to: 0 type: 2 value: '' - from: 1 text: Warning to: 9999 type: 2 value: '' - from: 10000 text: Critical to: 100000000000000005366162204393472 type: 2 value: '' thresholds: critical: 10000 operator: '>=' warning: 1 unit: none vmApps Property Value apache apache autoscaler autoscaler cAdvisor cAdvisor genericApp genericApp harbor harbor javaActuator javaActuator jvm jvm lokiDistributed lokiDistributed mysqlExporter mysqlExporter nginxIngress nginxIngress nginxIngressCertificateExpiry nginxIngressCertificateExpiry nginxNrpe nginxNrpe nginxVts nginxVts nginxVtsEnhanced nginxVtsEnhanced nginxVtsEnhancedLegacy nginxVtsEnhancedLegacy nginxVtsLegacy nginxVtsLegacy phpFpm phpFpm postfix postfix prometheus prometheus pythonFlask pythonFlask rabbitmq rabbitmq sslExporter sslExporter websocket websocket apache Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 autoscaler Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", job=~\".+\"}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", job=~\".+\"}) * 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Autoscaler Health Low {{ $value }}%' name: ClusterVMAppAutoscalerHealthLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - autoscaler panel expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", %(job)s}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", %(job)s}) * 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 cAdvisor Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 genericApp Property Value alert {} default false panel description: GenericApp template. Used when application monitoring is requested but appropriate template was not found. expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 gridPos: w: 4 thresholds: critical: 95 operator: < warning: 99 harbor Property Value alert customLables: alertgroup: ClusterVMApp expr: harbor_up{cluster=\"$cluster\", job=~\".+\"} linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Harbor component \"{{ $labels.component }}\" is down' name: ClusterVMAppHarborComponentDown thresholds: critical: 0 operator: == warning: 0 default false linkTo - harbor panel expr: (sum(harbor_up{cluster=\"$cluster\", %(job)s}) / count(harbor_up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 javaActuator Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Java Actuator Heap High {{ $value }}%' name: ClusterVMAppJavaActuatorHeapHigh thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 default false linkTo - javaactuator panel expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 jvm Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 lokiDistributed Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 mysqlExporter Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxIngress Property Value alert customLables: alertgroup: ClusterVMApp expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\", status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100)) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx Ingress Success Rate (non-4|5xx responses) Low {{ printf \"%.0f\" $value }}%' name: ClusterVMAppNginxIngressSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxingress panel expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s, status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100)) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxIngressCertificateExpiry Property Value alert customLables: alertgroup: ClusterVMApp expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", job=~\".+\"} - time()) / 60 / 60 / 24 linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx Ingress Certificate Expiry in {{ printf \"%.2f\" $value }} days' name: ClusterVMAppNginxIngressCertificateExpiry thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 8 default false linkTo - nginxingress panel dataLinks: - title: Detail url: /d/nginxingress?var-job=%(job)s&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to decimals: 0 expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", %(job)s} - time()) OR on() vector(-100000000000000005366162204393472) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s nginxNrpe Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxVts Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvts panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhanced Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhanced panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhancedLegacy Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhancedlegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsLegacy Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtslegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 phpFpm Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 postfix Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (postfix_size{cluster=\"$cluster\", job=~\".+\"})) linkGetParams: var-job={{ $labels.job }} mappings: - text: '-' type: 1 value: -1 message: 'ClusterVMApp {{ $labels.job }}: Postfix Queue Size High {{ $value }}%' name: ClusterVMAppPostfixQueueSizeHigh thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 default false linkTo - postfix panel expr: (sum by (job) (postfix_size{cluster=\"$cluster\", %(job)s})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 unit: mailq prometheus Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 pythonFlask Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\",status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Python Flask Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppPythonFlaskSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - pythonflask panel expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s,status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 rabbitmq Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 sslExporter Property Value alert {} default false linkTo - ssl-exporter panel decimals: 0 expr: bottomk(1,ssl_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_file_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubeconfig_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubernetes_cert_not_after{cluster=\"$cluster\"}-time()) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s websocket Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 L2 Property Value containerOverview containerOverview cpuPerNode cpuPerNode daemonSetOverview daemonSetOverview deploymentOverview deploymentOverview diskPerNode diskPerNode jobOverview jobOverview memoryPerNode memoryPerNode networkPerNode networkPerNode nodeOverview nodeOverview podOverview podOverview pvcOverview pvcOverview statefulSetOverview statefulSetOverview vm vm containerOverview Property Value containerOverviewTable containerOverviewTable containerOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_pod_container_info{cluster=\"$cluster\", namespace=~\"$namespace\", pod=~\"$pod\"}, container) panel expr: - \"sum by (container, namespace, pod) ((kube_pod_container_status_terminated * 0 or\\ \\ kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", pod=~\\\"$pod\\\", container=~\\\"$container\\\", reason=\\\"Completed\\\"}) *\\ \\ 1) + \\nsum by (container, namespace, pod) (kube_pod_container_status_running{cluster=\\\"\\ $cluster\\\"} * 2) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ContainerCreating\\\"}) * 3) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CrashLoopBackOff\\\"}) * 4) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CreateContainerConfigError\\\"}) * 5) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ErrImagePull\\\"}) * 6) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ImagePullBackOff\\\"}) * 7) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CreateContainerError\\\"}) * 8) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ InvalidImageName\\\"}) * 9) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CrashLoopBackOff\\\"}) * 10) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ OOMKilled\\\"}) * 11) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ Error\\\"}) * 12) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ContainerCannotRun\\\"}) * 13) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ DeadlineExceeded\\\"}) * 14) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ Evicted\\\"}) * 15)\" - sum by (container, namespace, pod) (kube_pod_container_status_restarts_total{cluster=\"$cluster\", namespace=~\"$namespace\", pod=~\"$pod\", container=~\"$container\"}) sort: col: 5 desc: true styles: - pattern: Time type: hidden - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #A' thresholds: - 4 - 4 type: string valueMaps: - text: Terminated (Completed) value: 1 - text: Running value: 2 - text: Waiting (ContainerCreating) value: 3 - text: Waiting (CrashLoopBackOff) value: 4 - text: Waiting (CreateContainerConfigError) value: 5 - text: Waiting (ErrImagePull) value: 6 - text: Waiting (ImagePullBackOff) value: 7 - text: Waiting (CreateContainerError) value: 8 - text: Waiting (InvalidImageName) value: 9 - text: Waiting (CrashLoopBackOff) value: 10 - text: Terminated (OOMKilled) value: 11 - text: Terminated (Error) value: 12 - text: Terminated (ContainerCannotRun) value: 13 - text: Terminated (DeadlineExceeded) value: 14 - text: Terminated (Evicted) value: 15 - alias: Restarts colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' pattern: 'Value #B' thresholds: - 5 - 10 type: number - alias: Container link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-container=${__cell_3}&var-namespace=${__cell_1}&var-pod=${__cell_2}&var-view=container&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: container - alias: Namespace pattern: namespace type: string - alias: Pod pattern: pod type: string title: Containers transformations: - id: merge options: {} - id: organize options: excludeByName: Time: false indexByName: Time: 0 'Value #A': 4 'Value #B': 5 container: 3 namespace: 1 pod: 2 renameByName: {} cpuPerNode Property Value cpuPerNodePolystat cpuPerNodePolystat cpuPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: \"avg(round((1 - (avg by (instance, pod) (irate(node_cpu_seconds_total{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\", mode=\\\"idle\\\"}[5m])))) * 100)\\n* on(instance, pod) group_left(nodename)\\ \\ \\n node_uname_info{cluster=\\\"$cluster\\\", nodename=~\\\"$instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 75 - color: '#e02f44' state: 2 value: 90 global_unit_format: percent title: CPU per Node daemonSetOverview Property Value daemonSetOverviewTable daemonSetOverviewTable daemonSetOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\"}, daemonset) panel expr: - sum by (daemonset, namespace) (kube_daemonset_status_number_misscheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_updated_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_number_available{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_number_ready{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) sort: col: 5 desc: true styles: - pattern: Time type: hidden - alias: Scheduled colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #A' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Updated colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #B' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Available colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #C' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Ready colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #D' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: DaemonSet pattern: daemonset type: string - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-pod=All&var-view=pod&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: DaemonSets transformations: - id: merge options: {} - id: organize options: excludeByName: Time: true indexByName: Time: 0 'Value #A': 3 'Value #B': 4 'Value #C': 5 'Value #D': 6 daemonset: 2 namespace: 1 renameByName: {} deploymentOverview Property Value deploymentOverviewTable deploymentOverviewTable deploymentOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_deployment_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\"}, deployment) panel expr: - sum by (deployment, namespace) (kube_deployment_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) - sum by (deployment, namespace) (kube_deployment_status_replicas_updated{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) - sum by (deployment, namespace) (kube_deployment_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) - sum by (deployment, namespace) (kube_deployment_status_replicas_available{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Updated colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #A' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Available colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #B' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Deployment pattern: deployment type: string - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-pod=All&var-view=pod&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: Deployments transformations: - id: merge options: {} - id: organize options: excludeByName: Time: true indexByName: Time: 0 'Value #A': 3 'Value #B': 4 deployment: 2 namespace: 1 renameByName: {} diskPerNode Property Value diskPerNodePolystat diskPerNodePolystat diskPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: \"max(round(\\n(sum(node_filesystem_size_bytes{cluster=\\\"$cluster\\\", job=~\\\"$job\\\"\\ }) by (instance, device, pod) - sum(node_filesystem_free_bytes{cluster=\\\"$cluster\\\"\\ , job=~\\\"$job\\\"}) by (instance, device, pod)) /\\n(sum(node_filesystem_size_bytes{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\"}) by (instance, device, pod) - sum(node_filesystem_free_bytes{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\"}) by (instance, device, pod) +\\nsum(node_filesystem_avail_bytes{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\"}) by (instance, device, pod))\\n * 100\\n) * on(instance,\\ \\ pod) group_left(nodename) \\n node_uname_info{cluster=\\\"$cluster\\\", nodename=~\\\"\\ $instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 75 - color: '#e02f44' state: 2 value: 90 global_unit_format: percent title: Disk per Node jobOverview Property Value jobOverviewTable jobOverviewTable jobOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_job_info{cluster=\"$cluster\", namespace=~\"$namespace\"}, job_name) panel expr: - \"sum by (job_name, namespace) (clamp_max(kube_job_status_succeeded{cluster=\\\"$cluster\\\"\\ , namespace=~\\\"$namespace\\\", job_name=~\\\"$job_name\\\"}, 1) * 1) * on(job_name, namespace)\\ \\ group_left(owner_name) kube_job_owner{cluster=\\\"$cluster\\\", namespace=~\\\"$namespace\\\"\\ , job_name=~\\\"$job_name\\\"} +\\nsum by (job_name, namespace) (clamp_max(kube_job_status_active{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", job_name=~\\\"$job_name\\\"}, 1) * 2) * on(job_name,\\ \\ namespace) group_left(owner_name) kube_job_owner{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", job_name=~\\\"$job_name\\\"} +\\nsum by (job_name, namespace) (clamp_max(kube_job_status_failed{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", job_name=~\\\"$job_name\\\"}, 1) * 3) * on(job_name,\\ \\ namespace) group_left(owner_name) kube_job_owner{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", job_name=~\\\"$job_name\\\"}\\n\" sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: Value thresholds: - 3 - 3 type: string valueMaps: - text: Succeeded value: 1 - text: Active value: 2 - text: Failed value: 3 - alias: Job name pattern: job_name type: string - alias: Owner pattern: owner_name type: string - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-container=All&var-view=container&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: Jobs transformations: - id: organize options: excludeByName: Time: true indexByName: Time: 0 Value: 4 job_name: 2 namespace: 1 owner_name: 3 renameByName: {} memoryPerNode Property Value memoryPerNodePolystat memoryPerNodePolystat memoryPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: \"avg(round((1 - (sum(node_memory_MemAvailable_bytes{cluster=\\\"$cluster\\\", job=~\\\"\\ $job\\\"}) by (instance, pod) / sum(node_memory_MemTotal_bytes{cluster=\\\"$cluster\\\"\\ , job=~\\\"$job\\\"}) by (instance, pod) )) * 100)\\n* on(instance, pod) group_left(nodename)\\ \\ \\n node_uname_info{cluster=\\\"$cluster\\\", nodename=~\\\"$instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 75 - color: '#e02f44' state: 2 value: 90 global_unit_format: percent title: Memory per Node networkPerNode Property Value networkPerNodePolystat networkPerNodePolystat networkPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: \"avg((sum(rate(node_network_transmit_errs_total{cluster=\\\"$cluster\\\", job=~\\\"\\ $job\\\", device!~\\\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\\\"}[5m])) \\ \\ by (instance, pod) \\n + sum(rate(node_network_receive_errs_total{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\", device!~\\\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\\\"\\ }[5m])) by (instance, pod))\\n* on(instance, pod) group_left(nodename) \\n node_uname_info{cluster=\\\"\\ $cluster\\\", nodename=~\\\"$instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 10 - color: '#e02f44' state: 2 value: 30 global_unit_format: pps title: Network Errors per Node nodeOverview Property Value nodeOverviewTable nodeOverviewTable nodeOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo {} panel expr: - sum by (node) (kube_node_spec_unschedulable{cluster=\"$cluster\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"DiskPressure\", status=~\"true|unknown\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"MemoryPressure\", status=~\"true|unknown\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"PIDPressure\", status=~\"true|unknown\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"Ready\", status=~\"false|unknown\"}) sort: col: 6 desc: true styles: - pattern: Time type: hidden - alias: Schedulable colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #A' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Disk Pressure colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #B' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Memory Pressure colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #C' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: PID Pressure colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #D' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Ready colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #E' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Node link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-view=pod&var-instance=$__cell&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: node title: Nodes podOverview Property Value podOverviewTable podOverviewTable podOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_pod_info{cluster=\"$cluster\", namespace=~\"$namespace\"}, pod) panel expr: - \"sum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", phase=\\\"Running\\\"} * 1) +\\nsum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", phase=\\\"Succeeded\\\"} * 2) +\\nsum by (namespace,\\ \\ pod) (kube_pod_status_phase{cluster=\\\"$cluster\\\", namespace=~\\\"$namespace\\\", phase=\\\"\\ Unknown\\\"} * 3) +\\nsum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"$cluster\\\"\\ , namespace=~\\\"$namespace\\\", phase=\\\"Failed\\\"} * 4) +\\nsum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", pod=~\\\"$pod\\\", phase=\\\"Pending\\\"} * 5)\\n\" sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: Value thresholds: - 3 - 3 type: string valueMaps: - text: Running value: 1 - text: Succeeded value: 2 - text: Unknown value: 3 - text: Failed value: 4 - text: Pending value: 5 - alias: Namespace pattern: namespace type: string - alias: Pod link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-container=All&var-view=pod&var-namespace=${__cell_1}&var-pod=${__cell_2}&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: pod title: Pods pvcOverview Property Value pvcOverviewTable pvcOverviewTable pvcOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_persistentvolumeclaim_info{cluster=\"$cluster\", namespace=~\"$namespace\"}, persistentvolumeclaim) panel description: Capacity is available only for remote pvc. expr: - sum by (persistentvolumeclaim, namespace) (((kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\", namespace=~\"$namespace\", persistentvolumeclaim=~\"$pvc\"} - kubelet_volume_stats_available_bytes{cluster=\"$cluster\", namespace=~\"$namespace\", persistentvolumeclaim=~\"$pvc\"}) / kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\", namespace=~\"$namespace\", persistentvolumeclaim=~\"$pvc\"}) * 100) - \"sum by (persistentvolumeclaim, namespace) (kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", persistentvolumeclaim=~\\\"$pvc\\\", phase=\\\"\\ Bound\\\"} * 1) +\\nsum by (persistentvolumeclaim, namespace) (kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", persistentvolumeclaim=~\\\"$pvc\\\", phase=\\\"\\ Lost\\\"} * 2) +\\nsum by (persistentvolumeclaim, namespace) (kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", persistentvolumeclaim=~\\\"$pvc\\\", phase=\\\"\\ Pending\\\"} * 3)\\n\" sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Capacity colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' pattern: 'Value #A' thresholds: - 85 - 97 type: number unit: percent - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #B' thresholds: - 2 - 2 type: string valueMaps: - text: Bound value: 1 - text: Lost value: 2 - text: Pending value: 3 - alias: PVC link: true linkTooltip: Detail linkUrl: /d/persistentvolumes?var-namespace=${__cell_1}&var-pvc=${__cell_2}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: persistentvolumeclaim - alias: Namespace pattern: namespace type: string title: Persistent Volumes statefulSetOverview Property Value statefulSetOverviewTable statefulSetOverviewTable statefulSetOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_statefulset_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\"}, statefulset) panel expr: - sum by (statefulset, namespace) (kube_statefulset_status_replicas_updated{cluster=\"$cluster\", namespace=~\"$namespace\", statefulset=~\"$statefulset\"}) - sum by (statefulset, namespace) (kube_statefulset_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\", statefulset=~\"$statefulset\"}) - sum by (statefulset, namespace) (kube_statefulset_status_replicas_ready{cluster=\"$cluster\", namespace=~\"$namespace\", statefulset=~\"$statefulset\"}) sort: col: 4 desc: true styles: - pattern: Time type: hidden - alias: Updated pattern: 'Value #A' type: number - alias: Ready colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #B' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: StatefulSet link: true linkTooltip: Detail linkUrl: /d/statefulset?var-namespace=${__cell_1}&var-statefulset=${__cell_2}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: statefulset - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-pod=All&var-view=pod&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: StatefulSets vm Property Value mostUtilizedVMCPU mostUtilizedVMCPU mostUtilizedVMDisk mostUtilizedVMDisk mostUtilizedVMNetworkErrors mostUtilizedVMNetworkErrors mostUtilizedVMRAM mostUtilizedVMRAM overallNetworkErrors overallNetworkErrors overallUtilizationCPU overallUtilizationCPU overallUtilizationDisk overallUtilizationDisk overallUtilizationRAM overallUtilizationRAM targetDown targetDown totalCores totalCores totalDisk totalDisk totalRAM totalRAM usedCores usedCores usedDisk usedDisk usedRAM usedRAM mostUtilizedVMCPU Property Value alert customLables: alertgroup: ClusterVM expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"%s\", mode=\"idle\"}[5m]) * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High CPU Utilization {{ $value }}%' name: VMCPUUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m]) * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 3 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized VM mostUtilizedVMDisk Property Value alert customLables: alertgroup: ClusterVM expr: round((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High Disk Utilization {{ $value }}%' name: VMDiskUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: w: 3 x: 15 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized VM mostUtilizedVMNetworkErrors Property Value alert customLables: alertgroup: ClusterVM expr: sum(rate(node_network_transmit_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High Network Errors Count {{ $value }}%' name: VMNetworkErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(rate(node_network_transmit_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 21 y: 6 thresholds: critical: 15 operator: '>=' warning: 10 title: Most Affected VM unit: pps mostUtilizedVMRAM Property Value alert customLables: alertgroup: ClusterVM expr: round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High RAM Utilization {{ $value }}%' name: VMRAMUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: max(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info))) * 100)) gridPos: w: 3 x: 9 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized VM overallNetworkErrors Property Value alert customLables: alertgroup: ClusterVM expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename)) message: VM High Overall Network Errors Count {{ $value }}% name: VMNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 18 y: 6 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps overallUtilizationCPU Property Value alert customLables: alertgroup: ClusterVM expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"%s\", mode=\"idle\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) message: VM High CPU Overall Utilization {{ $value }}% name: VMCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 0 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationDisk Property Value alert customLables: alertgroup: ClusterVM expr: round((sum(node_filesystem_size_bytes{job=~\"%s\"}) - sum(node_filesystem_free_bytes{job=~\"%s\"})) / (sum(node_filesystem_size_bytes{job=~\"%s\"}) - sum(node_filesystem_free_bytes{job=~\"%s\"}) + sum(node_filesystem_avail_bytes{job=~\"%s\"})) * 100 > 0) message: VM High Disk Overall Utilization {{ $value }}% name: VMDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: round((sum(node_filesystem_size_bytes{job=~\"$job\"}) - sum(node_filesystem_free_bytes{job=~\"$job\"})) / (sum(node_filesystem_size_bytes{job=~\"$job\"}) - sum(node_filesystem_free_bytes{job=~\"$job\"}) + sum(node_filesystem_avail_bytes{job=~\"$job\"})) * 100 > 0) gridPos: w: 3 x: 12 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationRAM Property Value alert customLables: alertgroup: ClusterVM expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"%s\"} * on(instance, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"%s\"} * on(instance, cluster) group_left(nodename) (node_uname_info))) * 100)) message: VM High RAM Overall Utilization {{ $value }}% name: VMRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"$job\"} * on(instance, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"$job\"} * on(instance, cluster) group_left(nodename) (node_uname_info))) * 100)) gridPos: w: 3 x: 6 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization targetDown Property Value alert customLables: alertgroup: ClusterVM expr: 100 * (count by(job, namespace, service) (up{job=~\"%s\"} == 0) / count by(job, namespace, service) (up{job=~\"%s\"})) message: '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.' name: VMTargetDown thresholds: critical: 90 operator: '>=' warning: 10 panel null totalCores Property Value panel colorMode: value expr: count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 9 thresholds: color: '#858187' value: title: Total Cores unit: none totalDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes totalRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes usedCores Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m])))) * count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 9 thresholds: color: '#858187' value: title: Used Cores unit: none usedDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"}) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes usedRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"}) * (((1 - sum(node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"}) / sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes commonThresholds Property Value app app controlPlane controlPlane criticalPanel criticalPanel k8s k8s node node warningPanel warningPanel app Property Value critical 95 operator \" < \" warning 99 controlPlane Property Value critical 95 lowest 0 operator \" < \" warning 99 criticalPanel Property Value critical 1 operator \">=\" k8s Property Value critical 95 operator \" < \" warning 99 node Property Value critical 90 operator \">=\" warning 75 warningPanel Property Value operator \">=\" warning 1 templateBases Property Value baseAlert baseAlert basePolystatTemplate basePolystatTemplate baseStatsTemplate baseStatsTemplate baseTableTemplate baseTableTemplate baseAlert Property Value customLables {} expr \"\" linkGetParams \"\" message \"\" name \"error must be overwritten\" thresholds {} basePolystatTemplate Property Value default true enabled true panel panel panel Property Value datasource \"$datasource\" default_click_through \"\" description \"\" expr \"\" fontAutoColor false fontColor \"white\" globalDecimals null global_thresholds {} global_unit_format \"\" gridPos h: 6 w: 24 x: 0 y: 0 hexagon_sort_by_direction 2 hexagon_sort_by_field \"value\" polygon_border_size 0 title \"error must be overwritten\" tooltip_timestamp_enabled false baseStatsTemplate Property Value alert {} default true enabled true panel panel panel Property Value colorMode \"background\" dataLinks [] datasource \"$datasource\" decimals null description \"\" expr \"\" graphMode \"area\" gridPos h: 3 w: 6 x: error must be overwritten y: error must be overwritten mappings [] thresholds {} title \"error must be overwritten\" unit \"percent\" baseTableTemplate Property Value default true enabled true panel panel panel Property Value datasource \"$datasource\" description \"\" expr [] gridPos h: 19 w: 24 x: 0 y: 1 sort {} styles [] title \"error must be overwritten\" transformations []","title":"Docs"},{"location":"docs/documentation/#dnation-kubernetes-monitoring-docs","text":"This is generated documentation from configuration files of Kubernetes Monitoring . Each configuration parameter can be overriden by providing custom values.yaml during helm installation. Property Value clusterMonitoring clusterMonitoring commonLabels {} dnation-kubernetes-jsonnet-translator dnation-kubernetes-jsonnet-translator fullnameOverride \"\" grafanaDashboards grafanaDashboards hostMonitoring hostMonitoring nameOverride \"\" namespaceOverride \"\" prometheusRules prometheusRules templates templates","title":"dnation Kubernetes Monitoring Docs"},{"location":"docs/documentation/#clustermonitoring","text":"Property Value clusters - apps: [] description: Kubernetes cluster monitoring label: observer-cluster name: K8sCluster enabled true","title":"clusterMonitoring"},{"location":"docs/documentation/#dnation-kubernetes-jsonnet-translator","text":"Property Value enabled true image image","title":"dnation-kubernetes-jsonnet-translator"},{"location":"docs/documentation/#image","text":"Property Value args - --libsonnet - https://github.com/grafana/grafonnet-lib/grafonnet@daad85cf3fad3580e58029414630e29956aefe21 - https://github.com/thelastpickle/grafonnet-polystat-panel@275a48de57afdac0d72219d82863d8ab8bd0e682","title":"image"},{"location":"docs/documentation/#grafanadashboards","text":"Property Value color color constants constants dataLinkCommonArgs \"refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to\" dataLinkCommonArgsNoCluster \"refresh=10s&var-datasource=$datasource&from=$__from&to=$__to\" editable true enable true ids ids isLoki true labelGrafana grafana_dashboard: '1' labelJsonnet grafana_dashboard_jsonnet: '1' refresh \"10s\" selectors selectors severityColors severityColors tags tags templateRefresh \"time\" templateSort 5 time_from \"now-5m\" tooltip \"shared_crosshair\"","title":"grafanaDashboards"},{"location":"docs/documentation/#color","text":"Property Value black \"#000000\" blue \"#5794f2\" gray \"#858187\" green \"#56a64b\" lightblue \"#8ab8ff\" orange \"#ff780a\" pink \"#fce2de\" purple \"#a352cc\" red \"#e02f44\" white \"#ffffff\" yellow \"#fade2a\"","title":"color"},{"location":"docs/documentation/#constants","text":"Property Value infinity 100000000000000005366162204393472 maxWarnings 10000","title":"constants"},{"location":"docs/documentation/#ids","text":"Property Value alertClusterOverview \"alertclusteroverview\" alertHostOverview \"alerthostoverview\" alertVMOverview \"alertvmoverview\" apache \"apache\" apiServer \"apiserver\" autoscaler \"autoscaler\" cAdvisor \"cadvisor\" containerDetail \"containerdetail\" containerOverview \"containeroverview\" controllerManager \"controllermanager\" cpuNamespaceOverview \"cpunamespaceoverview\" cpuOverview \"cpuoverview\" daemonSetOverview \"daemonsetoverview\" deploymentOverview \"deploymentoverview\" diskOverview \"diskoverview\" etcd \"etcd\" harbor \"harbor\" hostMonitoring \"hostmonitoring\" javaActuator \"javaactuator\" jobOverview \"joboverview\" jvm \"jvm\" k8sMonitoring \"k8smonitoring\" kubelet \"kubelet\" lokiDistributed \"loki-distributed\" memoryNamespaceOverview \"memorynamespaceoverview\" memoryOverview \"memoryoverview\" monitoring \"monitoring\" mysqlExporter \"mysqlexporter\" networkNamespaceOverview \"networknamespaceoverview\" networkOverview \"networkoverview\" nginxIngress \"nginxingress\" nginxNrpe \"nginxnrpe\" nginxVts \"nginxvts\" nginxVtsEnhanced \"nginxvtsenhanced\" nginxVtsEnhancedLegacy \"nginxvtsenhancedlegacy\" nginxVtsLegacy \"nginxvtslegacy\" nodeExporter \"nodeexporter\" nodeOverview \"nodeoverview\" persistentVolumes \"persistentvolumes\" phpFpm \"phpfpm\" podOverview \"podoverview\" postfix \"postfix\" prometheus \"prometheus\" proxy \"proxy\" pvcOverview \"pvcoverview\" pythonFlask \"pythonflask\" rabbitmq \"rabbitmq\" scheduler \"scheduler\" sslExporter \"ssl-exporter\" statefulSet \"statefulset\" statefulSetOverview \"statefulsetoverview\" vmMonitoring \"vmmonitoring\" websocket \"websocket\"","title":"ids"},{"location":"docs/documentation/#selectors","text":"Property Value apiServer \"job=\\\"apiserver\\\"\" controllerManager \"job=\\\"kube-controller-manager\\\"\" etcd \"job=\\\"kube-etcd\\\"\" kubelet \"job=\\\"kubelet\\\"\" proxy \"job=\\\"kube-proxy\\\"\" scheduler \"job=\\\"kube-scheduler\\\"\"","title":"selectors"},{"location":"docs/documentation/#severitycolors","text":"Property Value critical \"red\" default \"green\" invalid \"black\" warning \"orange\"","title":"severityColors"},{"location":"docs/documentation/#tags","text":"Property Value k8sApps - k8s - app - L1 k8sAppsMain - k8s - app - L0 k8sContainer - k8s - container - L3 k8sHostsMain - k8s - host - L1 k8sMonitoring - k8s - monitoring - L1 k8sMonitoringMain - k8s - cluster - host - L0 k8sNodeExporter - k8s - nodeexporter - L3 k8sOverview - k8s - overview - L2 k8sPVC - k8s - pvc - L3 k8sStatefulSet - k8s - statefulset - L3 k8sSystem - k8s - system - L2 k8sVMs - k8s - vm - L2","title":"tags"},{"location":"docs/documentation/#hostmonitoring","text":"Property Value enabled false hosts []","title":"hostMonitoring"},{"location":"docs/documentation/#prometheusrules","text":"Property Value alertGroupCluster \"Cluster\" alertGroupClusterApp \"ClusterApp\" alertGroupClusterVM \"ClusterVM\" alertGroupClusterVMApp \"ClusterVMApp\" alertGroupHost \"Host\" alertGroupHostApp \"HostApp\" alertInterval \"5m\" alertNamePrefix \"KubernetesMonitoring\" enable true labelJsonnet prometheus_rule_jsonnet: '1' labelPrometheus prometheus_rule: '1'","title":"prometheusRules"},{"location":"docs/documentation/#templates","text":"Property Value L0 L0 L1 L1 L2 L2 RecordRules - expr: node_uname_info{job=~\"node-exporter\"} and on(nodename) label_replace(kube_node_role{role=~\"control-plane\"}, \"nodename\", \"$1\", \"node\", \"(.+)\") record: master_uname_info - expr: node_uname_info{job=~\"node-exporter\"} unless on(nodename) label_replace(kube_node_role{role=~\"control-plane\"}, \"nodename\", \"$1\", \"node\", \"(.+)\") record: worker_uname_info commonThresholds commonThresholds templateBases templateBases","title":"templates"},{"location":"docs/documentation/#l0","text":"Property Value host host k8s k8s","title":"L0"},{"location":"docs/documentation/#host","text":"Property Value main main main Property Value panel expr: ((sum(up{job=~\"%(job)s\"}) or on() vector(0)) == bool 0) * (-1) + sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"warning\", job=~\"%(job)s\", alertgroup=~\"%(groupHost)s|%(groupHostApp)s\"} OR on() vector(0)) + sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"critical\", job=~\"%(job)s\", alertgroup=~\"%(groupHost)s|%(groupHostApp)s\"} OR on() vector(0)) * %(maxWarnings)d graphMode: none gridPos: h: 3 w: 4 mappings: - from: -1 text: Down to: -1 type: 2 value: '' - from: 0 text: OK to: 0 type: 2 value: '' - from: 1 text: Warning to: 9999 type: 2 value: '' - from: 10000 text: Critical to: 100000000000000005366162204393472 type: 2 value: '' thresholds: critical: 10000 lowest: 0 operator: '>=' warning: 1 unit: none","title":"host"},{"location":"docs/documentation/#k8s","text":"Property Value main main main Property Value panel expr: ((sum(up{job=~\"node-exporter\", cluster=\"%(cluster)s\"}) or on() vector(0)) == bool 0) * (-1) + sum(ALERTS{alertname!=\"Watchdog\", cluster=\"%(cluster)s\", alertstate=\"firing\", severity=\"warning\", alertgroup=~\"%(groupCluster)s|%(groupApp)s\"} OR on() vector(0)) + sum(ALERTS{alertname!=\"Watchdog\", cluster=\"%(cluster)s\", alertstate=\"firing\", severity=\"critical\", alertgroup=~\"%(groupCluster)s|%(groupApp)s\"} OR on() vector(0)) * %(maxWarnings)d graphMode: none gridPos: h: 3 w: 4 mappings: - from: -1 text: Down to: -1 type: 2 value: '' - from: 0 text: OK to: 0 type: 2 value: '' - from: 1 text: Warning to: 9999 type: 2 value: '' - from: 10000 text: Critical to: 100000000000000005366162204393472 type: 2 value: '' thresholds: critical: 10000 lowest: 0 operator: '>=' warning: 1 unit: none","title":"k8s"},{"location":"docs/documentation/#l1","text":"Property Value host host hostApps hostApps k8s k8s k8sApps k8sApps vm vm vmApps vmApps","title":"L1"},{"location":"docs/documentation/#host_1","text":"Property Value overallNetworkErrors overallNetworkErrors overallUtilizationCPU overallUtilizationCPU overallUtilizationDisk overallUtilizationDisk overallUtilizationRAM overallUtilizationRAM targetDown targetDown totalCores totalCores totalDisk totalDisk totalRAM totalRAM usedCores usedCores usedDisk usedDisk usedRAM usedRAM overallNetworkErrors Property Value alert customLables: alertgroup: Host expr: sum(rate(node_network_transmit_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High Overall Network Errors Count {{ $value }}%' name: HostNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(rate(node_network_transmit_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) gridPos: x: 18 y: 6 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps overallUtilizationCPU Property Value alert customLables: alertgroup: Host expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"%s\", mode=\"idle\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High CPU Overall Utilization {{ $value }}%' name: HostCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) gridPos: x: 0 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationDisk Property Value alert customLables: alertgroup: Host expr: round((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High Disk Overall Utilization {{ $value }}%' name: HostDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: x: 12 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationRAM Property Value alert customLables: alertgroup: Host expr: round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"%s\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Host {{ $labels.nodename }}: High RAM Overall Utilization {{ $value }}%' name: HostRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"$job\"} * on(instance, job, cluster) group_left(nodename) (node_uname_info))) * 100)) gridPos: x: 6 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization targetDown Property Value alert customLables: alertgroup: Host expr: 100 * (count by(job, namespace, service) (up{job=~\"%s\"} == 0) / count by(job, namespace, service) (up{job=~\"%s\"})) message: '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.' name: HostTargetDown thresholds: critical: 90 operator: '>=' warning: 10 panel null totalCores Property Value panel colorMode: value expr: count(node_cpu_seconds_total{job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 9 thresholds: color: '#858187' value: title: Total Cores unit: none totalDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes totalRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes usedCores Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m])))) * count(node_cpu_seconds_total{job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 9 thresholds: color: '#858187' value: title: Used Cores unit: none usedDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{job=~\"$job\"}) - sum(node_filesystem_free_bytes{job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes usedRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{job=~\"$job\"}) * (((1 - sum(node_memory_MemAvailable_bytes{job=~\"$job\"}) / sum(node_memory_MemTotal_bytes{job=~\"$job\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes","title":"host"},{"location":"docs/documentation/#hostapps","text":"Property Value apache apache autoscaler autoscaler cAdvisor cAdvisor genericApp genericApp harbor harbor javaActuator javaActuator jvm jvm lokiDistributed lokiDistributed mysqlExporter mysqlExporter nginxIngress nginxIngress nginxIngressCertificateExpiry nginxIngressCertificateExpiry nginxNrpe nginxNrpe nginxVts nginxVts nginxVtsEnhanced nginxVtsEnhanced nginxVtsEnhancedLegacy nginxVtsEnhancedLegacy nginxVtsLegacy nginxVtsLegacy phpFpm phpFpm postfix postfix prometheus prometheus pythonFlask pythonFlask rabbitmq rabbitmq sslExporter sslExporter websocket websocket apache Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 autoscaler Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", job=~\".+\"}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", job=~\".+\"}) * 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Autoscaler Health Low {{ $value }}%' name: HostAppAutoscalerHealthLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - autoscaler panel expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", %(job)s}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", %(job)s}) * 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 cAdvisor Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 genericApp Property Value alert {} default false panel description: GenericApp template. Used when application monitoring is requested but appropriate template was not found. expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 gridPos: w: 4 thresholds: critical: 95 operator: < warning: 99 harbor Property Value alert customLables: alertgroup: HostApp expr: harbor_up{cluster=\"$cluster\", job=~\".+\"} linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Harbor component \"{{ $labels.component }}\" is down' name: HostAppHarborComponentDown thresholds: critical: 0 operator: == warning: 0 default false linkTo - harbor panel expr: (sum(harbor_up{cluster=\"$cluster\", %(job)s}) / count(harbor_up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 javaActuator Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Java Actuator Heap High {{ $value }}%' name: HostAppJavaActuatorHeapHigh thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 default false linkTo - javaactuator panel expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 jvm Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 lokiDistributed Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 mysqlExporter Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxIngress Property Value alert customLables: alertgroup: HostApp expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\", status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100)) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx Ingress Success Rate (non-4|5xx responses) Low {{ printf \"%.0f\" $value }}%' name: HostAppNginxIngressSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxingress panel expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s, status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100)) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxIngressCertificateExpiry Property Value alert customLables: alertgroup: HostApp expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", job=~\".+\"} - time()) / 60 / 60 / 24 linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx Ingress Certificate Expiry in {{ printf \"%.2f\" $value }} days' name: HostAppNginxIngressCertificateExpiry thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 8 default false linkTo - nginxingress panel dataLinks: - title: Detail url: /d/nginxingress?var-job=%(job)s&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to decimals: 0 expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", %(job)s} - time()) OR on() vector(-100000000000000005366162204393472) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s nginxNrpe Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxVts Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvts panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhanced Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhanced panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhancedLegacy Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhancedlegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsLegacy Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtslegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 phpFpm Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 postfix Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (postfix_size{cluster=\"$cluster\", job=~\".+\"})) linkGetParams: var-job={{ $labels.job }} mappings: - text: '-' type: 1 value: -1 message: 'HostApp {{ $labels.job }}: Postfix Queue Size High {{ $value }}%' name: HostAppPostfixQueueSizeHigh thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 default false linkTo - postfix panel expr: (sum by (job) (postfix_size{cluster=\"$cluster\", %(job)s})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 unit: mailq prometheus Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 pythonFlask Property Value alert customLables: alertgroup: HostApp expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\",status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'HostApp {{ $labels.job }}: Python Flask Success Rate (non-4|5xx responses) Low {{ $value }}%' name: HostAppPythonFlaskSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - pythonflask panel expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s,status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 rabbitmq Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 sslExporter Property Value alert {} default false linkTo - ssl-exporter panel decimals: 0 expr: bottomk(1,ssl_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_file_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubeconfig_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubernetes_cert_not_after{cluster=\"$cluster\"}-time()) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s websocket Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99","title":"hostApps"},{"location":"docs/documentation/#k8s_1","text":"Property Value apiServerHealth apiServerHealth controllerManagerHealth controllerManagerHealth daemonSetsHealth daemonSetsHealth deploymentsHealth deploymentsHealth etcdHealth etcdHealth kubeletHealth kubeletHealth mostUtilizedMasterNodeCPU mostUtilizedMasterNodeCPU mostUtilizedMasterNodeDisk mostUtilizedMasterNodeDisk mostUtilizedMasterNodeNetworkErrors mostUtilizedMasterNodeNetworkErrors mostUtilizedMasterNodeRAM mostUtilizedMasterNodeRAM mostUtilizedPVC mostUtilizedPVC mostUtilizedWorkerNodeCPU mostUtilizedWorkerNodeCPU mostUtilizedWorkerNodeDisk mostUtilizedWorkerNodeDisk mostUtilizedWorkerNodeNetworkErrors mostUtilizedWorkerNodeNetworkErrors mostUtilizedWorkerNodeRAM mostUtilizedWorkerNodeRAM nodeHealth nodeHealth overallMasterNodesNetworkErrors overallMasterNodesNetworkErrors overallUtilizationMasterNodesCPU overallUtilizationMasterNodesCPU overallUtilizationMasterNodesDisk overallUtilizationMasterNodesDisk overallUtilizationMasterNodesRAM overallUtilizationMasterNodesRAM overallUtilizationWorkerNodesCPU overallUtilizationWorkerNodesCPU overallUtilizationWorkerNodesDisk overallUtilizationWorkerNodesDisk overallUtilizationWorkerNodesRAM overallUtilizationWorkerNodesRAM overallWorkerNodesNetworkErrors overallWorkerNodesNetworkErrors proxyHealth proxyHealth pvcBound pvcBound runningContainers runningContainers runningPods runningPods runningStatefulSets runningStatefulSets schedulerHealth schedulerHealth succeededJobs succeededJobs targetDown targetDown totalCoresMasterNodes totalCoresMasterNodes totalCoresWorkerNodes totalCoresWorkerNodes totalDiskMasterNodes totalDiskMasterNodes totalDiskWorkerNodes totalDiskWorkerNodes totalRAMMasterNodes totalRAMMasterNodes totalRAMWorkerNodes totalRAMWorkerNodes usedCoresMasterNodes usedCoresMasterNodes usedCoresWorkerNodes usedCoresWorkerNodes usedDiskMasterNodes usedDiskMasterNodes usedDiskWorkerNodes usedDiskWorkerNodes usedRAMMasterNodes usedRAMMasterNodes usedRAMWorkerNodes usedRAMWorkerNodes apiServerHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"apiserver\"}) by (cluster) / count(up{job=\"apiserver\"}) by (cluster)) * 100 message: Cluster Api Server Health Low {{ $value }}% name: ClusterApiServerHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - apiserver panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"apiserver\"}) / count(up{cluster=\"$cluster\", job=\"apiserver\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 0 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: API Server controllerManagerHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-controller-manager\"}) by (cluster) / count(up{job=\"kube-controller-manager\"}) by (cluster)) * 100 message: Cluster Controller Manager Health Low {{ $value }}% name: ClusterControllerManagerHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - controllermanager panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-controller-manager\"}) / count(up{cluster=\"$cluster\", job=\"kube-controller-manager\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 4 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Controller Manager daemonSetsHealth Property Value alert customLables: alertgroup: Cluster expr: round((sum(kube_daemonset_status_updated_number_scheduled OR kube_daemonset_updated_number_scheduled) by (cluster) + sum(kube_daemonset_status_number_available) by (cluster)) / (2 * sum(kube_daemonset_status_desired_number_scheduled) by (cluster)) * 100) message: DaemonSets Health Low {{ $value }}% name: RunningDaemonSetsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - daemonSetOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round((sum(kube_daemonset_status_updated_number_scheduled{cluster=\"$cluster\"} OR kube_daemonset_updated_number_scheduled{cluster=\"$cluster\"}) + sum(kube_daemonset_status_number_available{cluster=\"$cluster\"})) / (2 * sum(kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\"})) * 100) gridPos: x: 6 y: 12 thresholds: critical: 95 operator: < warning: 99 title: DaemonSets Health deploymentsHealth Property Value alert customLables: alertgroup: Cluster expr: round((sum(kube_deployment_status_replicas_updated) by (cluster) + sum(kube_deployment_status_replicas_available) by (cluster)) / (2 * sum(kube_deployment_status_replicas) by (cluster)) * 100) message: Running Deployments Health Low {{ $value }}% name: RunningDeploymentsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - deploymentOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round((sum(kube_deployment_status_replicas_updated{cluster=\"$cluster\"}) + sum(kube_deployment_status_replicas_available{cluster=\"$cluster\"})) / (2 * sum(kube_deployment_status_replicas{cluster=\"$cluster\"})) * 100) gridPos: x: 0 y: 12 thresholds: critical: 95 operator: < warning: 99 title: Deployments Health etcdHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-etcd\"}) by (cluster) / count(up{job=\"kube-etcd\"}) by (cluster)) * 100 message: Cluster Etcd Health Low {{ $value }}% name: ClusterEtcdHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - etcd panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-etcd\"}) / count(up{cluster=\"$cluster\", job=\"kube-etcd\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 8 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Etcd kubeletHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kubelet\", metrics_path=\"/metrics\"}) by (cluster) / count(up{job=\"kubelet\", metrics_path=\"/metrics\"}) by (cluster)) * 100 message: Cluster Kubelet Health Low {{ $value }}% name: ClusterKubeletHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - kubelet panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kubelet\", metrics_path=\"/metrics\"}) / count(up{cluster=\"$cluster\", job=\"kubelet\", metrics_path=\"/metrics\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 12 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Kubelet mostUtilizedMasterNodeCPU Property Value alert customLables: alertgroup: Cluster expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, cluster) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High CPU Utilization {{ $value }}%' name: ClusterMasterNodeCPUUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/cpuoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 3 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedMasterNodeDisk Property Value alert customLables: alertgroup: Cluster expr: round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High Disk Utilization {{ $value }}%' name: ClusterMasterNodeDiskUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/diskoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) * 100)) gridPos: w: 3 x: 15 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedMasterNodeNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High Network Errors Count {{ $value }}%' name: ClusterMasterNodeNetworkErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/networkoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 21 y: 17 thresholds: critical: 15 operator: '>=' warning: 10 title: Most Affected Node unit: pps mostUtilizedMasterNodeRAM Property Value alert customLables: alertgroup: Cluster expr: round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Master Node {{ $labels.nodename }}: High RAM Utilization {{ $value }}%' name: ClusterMasterNodesRAMUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/memoryoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: max(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100)) gridPos: w: 3 x: 9 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedPVC Property Value alert customLables: alertgroup: Cluster expr: sum(((kubelet_volume_stats_capacity_bytes - kubelet_volume_stats_available_bytes) / kubelet_volume_stats_capacity_bytes) * 100) by (persistentvolumeclaim, cluster) linkGetParams: var-pvc={{ $labels.persistentvolumeclaim }} message: '\"{{ $labels.persistentvolumeclaim }}\": High PVC Utilization {{ $value }}%' name: PVCUtilizationHigh thresholds: critical: 97 lowest: 0 operator: '>=' warning: 85 linkTo - pvcOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(((kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\"} - kubelet_volume_stats_available_bytes{cluster=\"$cluster\"}) / kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\"}) * 100) by (persistentvolumeclaim)) OR on() vector(-1) gridPos: w: 3 x: 21 y: 12 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 97 lowest: 0 operator: '>=' warning: 85 title: Most Utilized PVC mostUtilizedWorkerNodeCPU Property Value alert customLables: alertgroup: Cluster expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, cluster) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High CPU Utilization {{ $value }}%' name: ClusterWorkerNodeCPUUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/cpuoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 3 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedWorkerNodeDisk Property Value alert customLables: alertgroup: Cluster expr: round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High Disk Utilization {{ $value }}%' name: ClusterWorkerNodeDiskUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/diskoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) * 100)) gridPos: w: 3 x: 15 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node mostUtilizedWorkerNodeNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High Network Errors Count {{ $value }}%' name: ClusterWorkerNodeNetworkErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/networkoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 21 y: 24 thresholds: critical: 15 operator: '>=' warning: 10 title: Most Affected Node unit: pps mostUtilizedWorkerNodeRAM Property Value alert customLables: alertgroup: Cluster expr: round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'Cluster Worker Node {{ $labels.nodename }}: High RAM Utilization {{ $value }}%' name: ClusterWorkerNodesRAMUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/memoryoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: max(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100)) gridPos: w: 3 x: 9 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized Node nodeHealth Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_node_info) by (cluster) / (sum(kube_node_info) by (cluster) + sum(kube_node_spec_unschedulable) by (cluster) + sum(kube_node_status_condition{condition=~\"DiskPressure|MemoryPressure|PIDPressure\", status=~\"true|unknown\"}) by (cluster) + sum(kube_node_status_condition{condition=\"Ready\", status=~\"false|unknown\"}) by (cluster)) * 100) message: Nodes Health Low {{ $value }}% name: NodesHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - nodeOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_node_info{cluster=\"$cluster\"}) / (sum(kube_node_info{cluster=\"$cluster\"}) + sum(kube_node_spec_unschedulable{cluster=\"$cluster\"}) + sum(kube_node_status_condition{cluster=\"$cluster\", condition=~\"DiskPressure|MemoryPressure|PIDPressure\", status=~\"true|unknown\"}) + sum(kube_node_status_condition{cluster=\"$cluster\", condition=\"Ready\", status=~\"false|unknown\"}) ) * 100) gridPos: x: 0 y: 9 thresholds: critical: 95 operator: < warning: 99 title: Nodes Health overallMasterNodesNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename, cluster)) by (cluster) message: Cluster Master Nodes High Overall Network Errors Count {{ $value }}% name: ClusterMasterNodesNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - networkPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 18 y: 17 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps overallUtilizationMasterNodesCPU Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, cluster) )) * 100)) by (cluster) message: Cluster Master Nodes High CPU Overall Utilization {{ $value }}% name: ClusterMasterNodesCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - cpuPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 0 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationMasterNodesDisk Property Value alert customLables: alertgroup: Cluster expr: avg(round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device, cluster)) * 100 > 0)) by (cluster) message: Cluster Master Nodes High Disk Overall Utilization {{ $value }}% name: ClusterMasterNodesDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - diskPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: avg(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: w: 3 x: 12 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationMasterNodesRAM Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100)) by (cluster) message: Cluster Master Nodes High RAM Overall Utilization {{ $value }}% name: ClusterMasterNodesRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - memoryPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (master_uname_info))) * 100)) gridPos: w: 3 x: 6 y: 17 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationWorkerNodesCPU Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"node-exporter\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, cluster) )) * 100)) by (cluster) message: Cluster Worker Nodes High CPU Overall Utilization {{ $value }}% name: ClusterWorkerNodesCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - cpuPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/cpunamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 0 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationWorkerNodesDisk Property Value alert customLables: alertgroup: Cluster expr: avg(round((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) / ((sum(node_filesystem_size_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster) - sum(node_filesystem_free_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) + sum(node_filesystem_avail_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device, cluster)) * 100 > 0)) by (cluster) message: Cluster Worker Nodes High Disk Overall Utilization {{ $value }}% name: ClusterWorkerNodesDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - diskPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: avg(round((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: w: 3 x: 12 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationWorkerNodesRAM Property Value alert customLables: alertgroup: Cluster expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"node-exporter\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100)) by (cluster) message: Cluster Worker Nodes High RAM Overall Utilization {{ $value }}% name: ClusterWorkerNodesRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - memoryPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/memorynamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"} * on(instance, pod) group_left(nodename) (worker_uname_info))) * 100)) gridPos: w: 3 x: 6 y: 24 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallWorkerNodesNetworkErrors Property Value alert customLables: alertgroup: Cluster expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster) + sum(rate(node_network_receive_errs_total{job=~\"node-exporter\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename, cluster)) by (cluster) message: Cluster Worker Nodes High Overall Network Errors Count {{ $value }}% name: ClusterWorkerNodesNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - networkPerNodePolystat panel dataLinks: - title: System Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to&var-instance=All - title: K8s Overview url: /d/networknamespaceoverview?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(sum(rate(node_network_transmit_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{cluster=\"$cluster\", job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, pod) group_left(nodename) (worker_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 18 y: 24 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps proxyHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-proxy\"}) by (cluster) / count(up{job=\"kube-proxy\"}) by (cluster)) * 100 message: Cluster Proxy Health Low {{ $value }}% name: ClusterProxyHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - proxy panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-proxy\"}) / count(up{cluster=\"$cluster\", job=\"kube-proxy\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 16 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Proxy pvcBound Property Value alert customLables: alertgroup: Cluster expr: \"round(sum(kube_persistentvolumeclaim_status_phase{phase=\\\"Bound\\\"}) by (cluster)\\ \\ / (\\nsum(kube_persistentvolumeclaim_status_phase{phase=\\\"Bound\\\"}) by (cluster)\\ \\ + sum(kube_persistentvolumeclaim_status_phase{phase=\\\"Pending\\\"}) by (cluster)\\ \\ +\\nsum(kube_persistentvolumeclaim_status_phase{phase=\\\"Lost\\\"}) by (cluster)\\n\\ ) * 100)\" message: PVC Bound Rate Low {{ $value }}% name: PVCBoundRateLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - pvcOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: \"round(sum(kube_persistentvolumeclaim_status_phase{cluster=\\\"$cluster\\\", phase=\\\"\\ Bound\\\"}) / (\\nsum(kube_persistentvolumeclaim_status_phase{cluster=\\\"$cluster\\\"\\ , phase=\\\"Bound\\\"}) + sum(kube_persistentvolumeclaim_status_phase{cluster=\\\"$cluster\\\"\\ , phase=\\\"Pending\\\"}) +\\nsum(kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", phase=\\\"Lost\\\"})\\n) * 100) OR on() vector(-1)\" gridPos: w: 3 x: 18 y: 12 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: PVC Bound runningContainers Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_pod_container_status_running) by (cluster) / (sum(kube_pod_container_status_running) by (cluster) + (count(kube_pod_container_status_terminated) by (cluster) - count(kube_pod_container_status_terminated unless ignoring(reason) kube_pod_container_status_terminated_reason{reason!=\"Completed\"}) by (cluster)) + sum(kube_pod_container_status_waiting) by (cluster)) * 100) message: Running Containers Health Low {{ $value }}% name: RunningContainersHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - containerOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_pod_container_status_running{cluster=\"$cluster\"}) / (sum(kube_pod_container_status_running{cluster=\"$cluster\"}) + (sum(kube_pod_container_status_terminated_reason{cluster=\"$cluster\", reason!=\"Completed\"}) OR vector(0)) + sum(kube_pod_container_status_waiting{cluster=\"$cluster\"})) * 100) gridPos: x: 12 y: 12 thresholds: critical: 95 operator: < warning: 99 title: Running Containers runningPods Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_pod_status_phase{phase=\"Running\"}) by (cluster) / (sum(kube_pod_status_phase{phase=\"Running\"}) by (cluster) + sum(kube_pod_status_phase{phase=\"Pending\"}) by (cluster) + sum(kube_pod_status_phase{phase=\"Failed\"}) by (cluster) + sum(kube_pod_status_phase{phase=\"Unknown\"}) by (cluster)) * 100) message: Pods Health Low {{ $value }}% name: RunningPodsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - podOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Running\"}) / (sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Running\"}) + sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Pending\"}) + sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Failed\"}) + sum(kube_pod_status_phase{cluster=\"$cluster\", phase=\"Unknown\"})) * 100) gridPos: x: 12 y: 9 thresholds: critical: 95 operator: < warning: 99 title: Running Pods runningStatefulSets Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_statefulset_status_replicas_ready) by (cluster) / sum(kube_statefulset_status_replicas) by (cluster) * 100) message: StatefulSets Health Low {{ $value }}% name: RunningStatefulSetsHealthLow thresholds: critical: 95 operator: < warning: 99 linkTo - statefulSetOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_statefulset_status_replicas_ready{cluster=\"$cluster\"}) / sum(kube_statefulset_status_replicas{cluster=\"$cluster\"}) * 100) gridPos: x: 6 y: 9 thresholds: critical: 95 operator: < warning: 99 title: Running StatefulSets schedulerHealth Property Value alert customLables: alertgroup: Cluster expr: (sum(up{job=\"kube-scheduler\"}) by (cluster) / count(up{job=\"kube-scheduler\"}) by (cluster)) * 100 message: Cluster Scheduler Health Low {{ $value }}% name: ClusterSchedulerHealthLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - scheduler panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: (sum(up{cluster=\"$cluster\", job=\"kube-scheduler\"}) / count(up{cluster=\"$cluster\", job=\"kube-scheduler\"})) * 100 OR on() vector(-1) gridPos: w: 4 x: 20 y: 5 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Scheduler succeededJobs Property Value alert customLables: alertgroup: Cluster expr: round(sum(kube_job_status_succeeded) by (cluster) / (sum(kube_job_status_succeeded) by (cluster) + sum(kube_job_status_failed) by (cluster)) * 100) message: Succeeded Jobs Rate Low {{ $value }}% name: SucceededJobsRateLow thresholds: critical: 95 lowest: 0 operator: < warning: 99 linkTo - jobOverviewTable panel dataLinks: - title: K8s Overview url: /d/{}?refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: round(sum(kube_job_status_succeeded{cluster=\"$cluster\"}) / (sum(kube_job_status_succeeded{cluster=\"$cluster\"}) + sum(kube_job_status_failed{cluster=\"$cluster\"})) * 100) OR on() vector(-1) gridPos: x: 18 y: 9 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 title: Succeeded Jobs targetDown Property Value alert customLables: alertgroup: Cluster expr: 100 * (count by(job, namespace, service, cluster) (up{pod!~\"virt-launcher.*|\"} == 0) / count by(job, namespace, service, cluster) (up{pod!~\"virt-launcher.*|\"})) message: '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.' name: ClusterTargetDown thresholds: critical: 90 operator: '>=' warning: 10 panel null totalCoresMasterNodes Property Value panel colorMode: value expr: count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 20 thresholds: color: '#858187' value: title: Total Cores unit: none totalCoresWorkerNodes Property Value panel colorMode: value expr: count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 27 thresholds: color: '#858187' value: title: Total Cores unit: none totalDiskMasterNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 20 thresholds: color: '#858187' value: title: Total unit: bytes totalDiskWorkerNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 27 thresholds: color: '#858187' value: title: Total unit: bytes totalRAMMasterNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 20 thresholds: color: '#858187' value: title: Total unit: bytes totalRAMWorkerNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 27 thresholds: color: '#858187' value: title: Total unit: bytes usedCoresMasterNodes Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\", instance=~\"$masterInstance\"}[5m])))) * count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 20 thresholds: color: '#858187' value: title: Used Cores unit: none usedCoresWorkerNodes Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\", instance=~\"$workerInstance\"}[5m])))) * count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 27 thresholds: color: '#858187' value: title: Used Cores unit: none usedDiskMasterNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 20 thresholds: color: '#858187' value: title: Used unit: bytes usedDiskWorkerNodes Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 27 thresholds: color: '#858187' value: title: Used unit: bytes usedRAMMasterNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) * (((1 - sum(node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"}) / sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$masterInstance\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 20 thresholds: color: '#858187' value: title: Used unit: bytes usedRAMWorkerNodes Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) * (((1 - sum(node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"}) / sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\", instance=~\"$workerInstance\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 27 thresholds: color: '#858187' value: title: Used unit: bytes","title":"k8s"},{"location":"docs/documentation/#k8sapps","text":"Property Value apache apache autoscaler autoscaler cAdvisor cAdvisor genericApp genericApp harbor harbor javaActuator javaActuator jvm jvm lokiDistributed lokiDistributed mysqlExporter mysqlExporter nginxIngress nginxIngress nginxIngressCertificateExpiry nginxIngressCertificateExpiry nginxNrpe nginxNrpe nginxVts nginxVts nginxVtsEnhanced nginxVtsEnhanced nginxVtsEnhancedLegacy nginxVtsEnhancedLegacy nginxVtsLegacy nginxVtsLegacy phpFpm phpFpm postfix postfix prometheus prometheus pythonFlask pythonFlask rabbitmq rabbitmq sslExporter sslExporter websocket websocket apache Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 autoscaler Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", job=~\".+\"}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", job=~\".+\"}) * 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Autoscaler Health Low {{ $value }}%' name: ClusterAppAutoscalerHealthLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - autoscaler panel expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", %(job)s}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", %(job)s}) * 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 cAdvisor Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 genericApp Property Value alert {} default false panel description: GenericApp template. Used when application monitoring is requested but appropriate template was not found. expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 gridPos: w: 4 thresholds: critical: 95 operator: < warning: 99 harbor Property Value alert customLables: alertgroup: ClusterApp expr: harbor_up{cluster=\"$cluster\", job=~\".+\"} linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Harbor component \"{{ $labels.component }}\" is down' name: ClusterAppHarborComponentDown thresholds: critical: 0 operator: == warning: 0 default false linkTo - harbor panel expr: (sum(harbor_up{cluster=\"$cluster\", %(job)s}) / count(harbor_up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 javaActuator Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Java Actuator Heap High {{ $value }}%' name: ClusterAppJavaActuatorHeapHigh thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 default false linkTo - javaactuator panel expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 jvm Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 lokiDistributed Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 mysqlExporter Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxIngress Property Value alert customLables: alertgroup: ClusterApp expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\", status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100)) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx Ingress Success Rate (non-4|5xx responses) Low {{ printf \"%.0f\" $value }}%' name: ClusterAppNginxIngressSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxingress panel expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s, status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100)) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxIngressCertificateExpiry Property Value alert customLables: alertgroup: ClusterApp expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", job=~\".+\"} - time()) / 60 / 60 / 24 linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx Ingress Certificate Expiry in {{ printf \"%.2f\" $value }} days' name: ClusterAppNginxIngressCertificateExpiry thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 8 default false linkTo - nginxingress panel dataLinks: - title: Detail url: /d/nginxingress?var-job=%(job)s&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to decimals: 0 expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", %(job)s} - time()) OR on() vector(-100000000000000005366162204393472) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s nginxNrpe Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxVts Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvts panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhanced Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhanced panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhancedLegacy Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhancedlegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsLegacy Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtslegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 phpFpm Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 postfix Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (postfix_size{cluster=\"$cluster\", job=~\".+\"})) linkGetParams: var-job={{ $labels.job }} mappings: - text: '-' type: 1 value: -1 message: 'ClusterApp {{ $labels.job }}: Postfix Queue Size High {{ $value }}%' name: ClusterAppPostfixQueueSizeHigh thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 default false linkTo - postfix panel expr: (sum by (job) (postfix_size{cluster=\"$cluster\", %(job)s})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 unit: mailq prometheus Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 pythonFlask Property Value alert customLables: alertgroup: ClusterApp expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\",status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterApp {{ $labels.job }}: Python Flask Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterAppPythonFlaskSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - pythonflask panel expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s,status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 rabbitmq Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 sslExporter Property Value alert {} default false linkTo - ssl-exporter panel decimals: 0 expr: bottomk(1,ssl_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_file_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubeconfig_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubernetes_cert_not_after{cluster=\"$cluster\"}-time()) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s websocket Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99","title":"k8sApps"},{"location":"docs/documentation/#vm","text":"Property Value main main main Property Value panel expr: sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"warning\", job=~\"%(job)s\", alertgroup=~\"%(groupVM)s|%(groupVMApp)s\"} OR on() vector(0)) + sum(ALERTS{alertname!=\"Watchdog\", alertstate=\"firing\", severity=\"critical\", job=~\"%(job)s\", alertgroup=~\"%(groupVM)s|%(groupVMApp)s\"} OR on() vector(0)) * %(maxWarnings)d graphMode: none gridPos: h: 3 w: 4 mappings: - from: 0 text: OK to: 0 type: 2 value: '' - from: 1 text: Warning to: 9999 type: 2 value: '' - from: 10000 text: Critical to: 100000000000000005366162204393472 type: 2 value: '' thresholds: critical: 10000 operator: '>=' warning: 1 unit: none","title":"vm"},{"location":"docs/documentation/#vmapps","text":"Property Value apache apache autoscaler autoscaler cAdvisor cAdvisor genericApp genericApp harbor harbor javaActuator javaActuator jvm jvm lokiDistributed lokiDistributed mysqlExporter mysqlExporter nginxIngress nginxIngress nginxIngressCertificateExpiry nginxIngressCertificateExpiry nginxNrpe nginxNrpe nginxVts nginxVts nginxVtsEnhanced nginxVtsEnhanced nginxVtsEnhancedLegacy nginxVtsEnhancedLegacy nginxVtsLegacy nginxVtsLegacy phpFpm phpFpm postfix postfix prometheus prometheus pythonFlask pythonFlask rabbitmq rabbitmq sslExporter sslExporter websocket websocket apache Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 autoscaler Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", job=~\".+\"}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", job=~\".+\"}) * 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Autoscaler Health Low {{ $value }}%' name: ClusterVMAppAutoscalerHealthLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - autoscaler panel expr: (sum by (job) (autoscaler_healthy{cluster=\"$cluster\", %(job)s}) / sum by (job) (autoscaler_instances{cluster=\"$cluster\", %(job)s}) * 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 cAdvisor Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 genericApp Property Value alert {} default false panel description: GenericApp template. Used when application monitoring is requested but appropriate template was not found. expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 gridPos: w: 4 thresholds: critical: 95 operator: < warning: 99 harbor Property Value alert customLables: alertgroup: ClusterVMApp expr: harbor_up{cluster=\"$cluster\", job=~\".+\"} linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Harbor component \"{{ $labels.component }}\" is down' name: ClusterVMAppHarborComponentDown thresholds: critical: 0 operator: == warning: 0 default false linkTo - harbor panel expr: (sum(harbor_up{cluster=\"$cluster\", %(job)s}) / count(harbor_up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 javaActuator Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", job=~\".+\", area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", job=~\".+\", area=\"heap\"})) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Java Actuator Heap High {{ $value }}%' name: ClusterVMAppJavaActuatorHeapHigh thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 default false linkTo - javaactuator panel expr: (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"}) > sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"}) or (sum by (job) (jvm_memory_used_bytes{cluster=\"$cluster\", %(job)s, area=\"nonheap\"})*100)/sum by (job) (jvm_memory_max_bytes{cluster=\"$cluster\", %(job)s, area=\"heap\"})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 90 lowest: 0 operator: '>=' warning: 75 jvm Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 lokiDistributed Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 mysqlExporter Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxIngress Property Value alert customLables: alertgroup: ClusterVMApp expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\", status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100)) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx Ingress Success Rate (non-4|5xx responses) Low {{ printf \"%.0f\" $value }}%' name: ClusterVMAppNginxIngressSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxingress panel expr: ((sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s, status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_ingress_controller_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100)) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxIngressCertificateExpiry Property Value alert customLables: alertgroup: ClusterVMApp expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", job=~\".+\"} - time()) / 60 / 60 / 24 linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx Ingress Certificate Expiry in {{ printf \"%.2f\" $value }} days' name: ClusterVMAppNginxIngressCertificateExpiry thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 8 default false linkTo - nginxingress panel dataLinks: - title: Detail url: /d/nginxingress?var-job=%(job)s&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to decimals: 0 expr: bottomk(1, nginx_ingress_controller_ssl_expire_time_seconds{cluster=\"$cluster\", %(job)s} - time()) OR on() vector(-100000000000000005366162204393472) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s nginxNrpe Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 nginxVts Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvts panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhanced Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhanced panel expr: (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_vts_server_requests_total{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsEnhancedLegacy Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtsenhancedlegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 nginxVtsLegacy Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\", code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Nginx VTS Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppNginxVTSSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - nginxvtslegacy panel expr: (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!~\"[4-5].*\", code!=\"total\"}[5m])) / sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s, code!=\"total\"}[5m])) * 100) > 0 OR (sum by (job) (rate(nginx_server_requests{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 phpFpm Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 postfix Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (postfix_size{cluster=\"$cluster\", job=~\".+\"})) linkGetParams: var-job={{ $labels.job }} mappings: - text: '-' type: 1 value: -1 message: 'ClusterVMApp {{ $labels.job }}: Postfix Queue Size High {{ $value }}%' name: ClusterVMAppPostfixQueueSizeHigh thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 default false linkTo - postfix panel expr: (sum by (job) (postfix_size{cluster=\"$cluster\", %(job)s})) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 10 lowest: 0 operator: '>=' warning: 5 unit: mailq prometheus Property Value alert {} default false panel expr: (sum(up{%(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 pythonFlask Property Value alert customLables: alertgroup: ClusterVMApp expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\",status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", job=~\".+\"}[5m])) + 100) linkGetParams: var-job={{ $labels.job }} message: 'ClusterVMApp {{ $labels.job }}: Python Flask Success Rate (non-4|5xx responses) Low {{ $value }}%' name: ClusterVMAppPythonFlaskSuccessRateLow thresholds: critical: 85 lowest: 0 operator: < warning: 95 default false linkTo - pythonflask panel expr: (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s,status!~\"[4-5].*\"}[5m])) / sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) * 100) > 0 OR (sum by (job) (rate(flask_http_request_duration_seconds_count{cluster=\"$cluster\", %(job)s}[5m])) + 100) OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 85 lowest: 0 operator: < warning: 95 rabbitmq Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99 sslExporter Property Value alert {} default false linkTo - ssl-exporter panel decimals: 0 expr: bottomk(1,ssl_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_file_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubeconfig_cert_not_after{cluster=\"$cluster\"}-time() OR ssl_kubernetes_cert_not_after{cluster=\"$cluster\"}-time()) gridPos: w: 4 mappings: - text: '-' type: 1 value: -100000000000000005366162204393472 thresholds: critical: 0 lowest: -100000000000000005366162204393472 operator: < warning: 691200 unit: s websocket Property Value alert {} default false panel expr: (sum(up{cluster=\"$cluster\", %(job)s}) / count(up{cluster=\"$cluster\", %(job)s}))*100 OR on() vector(-1) gridPos: w: 4 mappings: - text: '-' type: 1 value: -1 thresholds: critical: 95 lowest: 0 operator: < warning: 99","title":"vmApps"},{"location":"docs/documentation/#l2","text":"Property Value containerOverview containerOverview cpuPerNode cpuPerNode daemonSetOverview daemonSetOverview deploymentOverview deploymentOverview diskPerNode diskPerNode jobOverview jobOverview memoryPerNode memoryPerNode networkPerNode networkPerNode nodeOverview nodeOverview podOverview podOverview pvcOverview pvcOverview statefulSetOverview statefulSetOverview vm vm","title":"L2"},{"location":"docs/documentation/#containeroverview","text":"Property Value containerOverviewTable containerOverviewTable containerOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_pod_container_info{cluster=\"$cluster\", namespace=~\"$namespace\", pod=~\"$pod\"}, container) panel expr: - \"sum by (container, namespace, pod) ((kube_pod_container_status_terminated * 0 or\\ \\ kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", pod=~\\\"$pod\\\", container=~\\\"$container\\\", reason=\\\"Completed\\\"}) *\\ \\ 1) + \\nsum by (container, namespace, pod) (kube_pod_container_status_running{cluster=\\\"\\ $cluster\\\"} * 2) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ContainerCreating\\\"}) * 3) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CrashLoopBackOff\\\"}) * 4) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CreateContainerConfigError\\\"}) * 5) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ErrImagePull\\\"}) * 6) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ImagePullBackOff\\\"}) * 7) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CreateContainerError\\\"}) * 8) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ InvalidImageName\\\"}) * 9) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_waiting\\ \\ * 0 or kube_pod_container_status_waiting_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ CrashLoopBackOff\\\"}) * 10) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ OOMKilled\\\"}) * 11) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ Error\\\"}) * 12) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ ContainerCannotRun\\\"}) * 13) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ DeadlineExceeded\\\"}) * 14) + \\nsum by (container, namespace, pod) ((kube_pod_container_status_terminated\\ \\ * 0 or kube_pod_container_status_terminated_reason{cluster=\\\"$cluster\\\", reason=\\\"\\ Evicted\\\"}) * 15)\" - sum by (container, namespace, pod) (kube_pod_container_status_restarts_total{cluster=\"$cluster\", namespace=~\"$namespace\", pod=~\"$pod\", container=~\"$container\"}) sort: col: 5 desc: true styles: - pattern: Time type: hidden - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #A' thresholds: - 4 - 4 type: string valueMaps: - text: Terminated (Completed) value: 1 - text: Running value: 2 - text: Waiting (ContainerCreating) value: 3 - text: Waiting (CrashLoopBackOff) value: 4 - text: Waiting (CreateContainerConfigError) value: 5 - text: Waiting (ErrImagePull) value: 6 - text: Waiting (ImagePullBackOff) value: 7 - text: Waiting (CreateContainerError) value: 8 - text: Waiting (InvalidImageName) value: 9 - text: Waiting (CrashLoopBackOff) value: 10 - text: Terminated (OOMKilled) value: 11 - text: Terminated (Error) value: 12 - text: Terminated (ContainerCannotRun) value: 13 - text: Terminated (DeadlineExceeded) value: 14 - text: Terminated (Evicted) value: 15 - alias: Restarts colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' pattern: 'Value #B' thresholds: - 5 - 10 type: number - alias: Container link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-container=${__cell_3}&var-namespace=${__cell_1}&var-pod=${__cell_2}&var-view=container&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: container - alias: Namespace pattern: namespace type: string - alias: Pod pattern: pod type: string title: Containers transformations: - id: merge options: {} - id: organize options: excludeByName: Time: false indexByName: Time: 0 'Value #A': 4 'Value #B': 5 container: 3 namespace: 1 pod: 2 renameByName: {}","title":"containerOverview"},{"location":"docs/documentation/#cpupernode","text":"Property Value cpuPerNodePolystat cpuPerNodePolystat cpuPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: \"avg(round((1 - (avg by (instance, pod) (irate(node_cpu_seconds_total{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\", mode=\\\"idle\\\"}[5m])))) * 100)\\n* on(instance, pod) group_left(nodename)\\ \\ \\n node_uname_info{cluster=\\\"$cluster\\\", nodename=~\\\"$instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 75 - color: '#e02f44' state: 2 value: 90 global_unit_format: percent title: CPU per Node","title":"cpuPerNode"},{"location":"docs/documentation/#daemonsetoverview","text":"Property Value daemonSetOverviewTable daemonSetOverviewTable daemonSetOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\"}, daemonset) panel expr: - sum by (daemonset, namespace) (kube_daemonset_status_number_misscheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_updated_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_number_available{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_desired_number_scheduled{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) - sum by (daemonset, namespace) (kube_daemonset_status_number_ready{cluster=\"$cluster\", namespace=~\"$namespace\", daemonset=~\"$daemonset\"}) sort: col: 5 desc: true styles: - pattern: Time type: hidden - alias: Scheduled colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #A' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Updated colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #B' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Available colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #C' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Ready colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #D' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: DaemonSet pattern: daemonset type: string - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-pod=All&var-view=pod&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: DaemonSets transformations: - id: merge options: {} - id: organize options: excludeByName: Time: true indexByName: Time: 0 'Value #A': 3 'Value #B': 4 'Value #C': 5 'Value #D': 6 daemonset: 2 namespace: 1 renameByName: {}","title":"daemonSetOverview"},{"location":"docs/documentation/#deploymentoverview","text":"Property Value deploymentOverviewTable deploymentOverviewTable deploymentOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_deployment_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\"}, deployment) panel expr: - sum by (deployment, namespace) (kube_deployment_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) - sum by (deployment, namespace) (kube_deployment_status_replicas_updated{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) - sum by (deployment, namespace) (kube_deployment_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) - sum by (deployment, namespace) (kube_deployment_status_replicas_available{cluster=\"$cluster\", namespace=~\"$namespace\", deployment=~\"$deployment\"}) sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Updated colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #A' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Available colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #B' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: Deployment pattern: deployment type: string - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-pod=All&var-view=pod&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: Deployments transformations: - id: merge options: {} - id: organize options: excludeByName: Time: true indexByName: Time: 0 'Value #A': 3 'Value #B': 4 deployment: 2 namespace: 1 renameByName: {}","title":"deploymentOverview"},{"location":"docs/documentation/#diskpernode","text":"Property Value diskPerNodePolystat diskPerNodePolystat diskPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: \"max(round(\\n(sum(node_filesystem_size_bytes{cluster=\\\"$cluster\\\", job=~\\\"$job\\\"\\ }) by (instance, device, pod) - sum(node_filesystem_free_bytes{cluster=\\\"$cluster\\\"\\ , job=~\\\"$job\\\"}) by (instance, device, pod)) /\\n(sum(node_filesystem_size_bytes{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\"}) by (instance, device, pod) - sum(node_filesystem_free_bytes{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\"}) by (instance, device, pod) +\\nsum(node_filesystem_avail_bytes{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\"}) by (instance, device, pod))\\n * 100\\n) * on(instance,\\ \\ pod) group_left(nodename) \\n node_uname_info{cluster=\\\"$cluster\\\", nodename=~\\\"\\ $instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 75 - color: '#e02f44' state: 2 value: 90 global_unit_format: percent title: Disk per Node","title":"diskPerNode"},{"location":"docs/documentation/#joboverview","text":"Property Value jobOverviewTable jobOverviewTable jobOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_job_info{cluster=\"$cluster\", namespace=~\"$namespace\"}, job_name) panel expr: - \"sum by (job_name, namespace) (clamp_max(kube_job_status_succeeded{cluster=\\\"$cluster\\\"\\ , namespace=~\\\"$namespace\\\", job_name=~\\\"$job_name\\\"}, 1) * 1) * on(job_name, namespace)\\ \\ group_left(owner_name) kube_job_owner{cluster=\\\"$cluster\\\", namespace=~\\\"$namespace\\\"\\ , job_name=~\\\"$job_name\\\"} +\\nsum by (job_name, namespace) (clamp_max(kube_job_status_active{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", job_name=~\\\"$job_name\\\"}, 1) * 2) * on(job_name,\\ \\ namespace) group_left(owner_name) kube_job_owner{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", job_name=~\\\"$job_name\\\"} +\\nsum by (job_name, namespace) (clamp_max(kube_job_status_failed{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", job_name=~\\\"$job_name\\\"}, 1) * 3) * on(job_name,\\ \\ namespace) group_left(owner_name) kube_job_owner{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", job_name=~\\\"$job_name\\\"}\\n\" sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: Value thresholds: - 3 - 3 type: string valueMaps: - text: Succeeded value: 1 - text: Active value: 2 - text: Failed value: 3 - alias: Job name pattern: job_name type: string - alias: Owner pattern: owner_name type: string - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-container=All&var-view=container&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: Jobs transformations: - id: organize options: excludeByName: Time: true indexByName: Time: 0 Value: 4 job_name: 2 namespace: 1 owner_name: 3 renameByName: {}","title":"jobOverview"},{"location":"docs/documentation/#memorypernode","text":"Property Value memoryPerNodePolystat memoryPerNodePolystat memoryPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: \"avg(round((1 - (sum(node_memory_MemAvailable_bytes{cluster=\\\"$cluster\\\", job=~\\\"\\ $job\\\"}) by (instance, pod) / sum(node_memory_MemTotal_bytes{cluster=\\\"$cluster\\\"\\ , job=~\\\"$job\\\"}) by (instance, pod) )) * 100)\\n* on(instance, pod) group_left(nodename)\\ \\ \\n node_uname_info{cluster=\\\"$cluster\\\", nodename=~\\\"$instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 75 - color: '#e02f44' state: 2 value: 90 global_unit_format: percent title: Memory per Node","title":"memoryPerNode"},{"location":"docs/documentation/#networkpernode","text":"Property Value networkPerNodePolystat networkPerNodePolystat networkPerNodePolystat Property Value base \"basePolystatTemplate\" dashboardInfo grafanaTemplateQuery: label_values(node_uname_info{cluster=\"$cluster\", job=~\"$job\"}, nodename) panel default_click_through: /d/nodeexporter?var-job=$job&var-instance=${__cell_name}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: \"avg((sum(rate(node_network_transmit_errs_total{cluster=\\\"$cluster\\\", job=~\\\"\\ $job\\\", device!~\\\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\\\"}[5m])) \\ \\ by (instance, pod) \\n + sum(rate(node_network_receive_errs_total{cluster=\\\"\\ $cluster\\\", job=~\\\"$job\\\", device!~\\\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\\\"\\ }[5m])) by (instance, pod))\\n* on(instance, pod) group_left(nodename) \\n node_uname_info{cluster=\\\"\\ $cluster\\\", nodename=~\\\"$instance\\\"}) by (nodename)\" fontColor: '#ffffff' global_thresholds: - color: '#56a64b' state: 0 value: 0 - color: '#ff780a' state: 1 value: 10 - color: '#e02f44' state: 2 value: 30 global_unit_format: pps title: Network Errors per Node","title":"networkPerNode"},{"location":"docs/documentation/#nodeoverview","text":"Property Value nodeOverviewTable nodeOverviewTable nodeOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo {} panel expr: - sum by (node) (kube_node_spec_unschedulable{cluster=\"$cluster\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"DiskPressure\", status=~\"true|unknown\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"MemoryPressure\", status=~\"true|unknown\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"PIDPressure\", status=~\"true|unknown\"}) - sum by (node) (kube_node_status_condition{cluster=\"$cluster\", condition=\"Ready\", status=~\"false|unknown\"}) sort: col: 6 desc: true styles: - pattern: Time type: hidden - alias: Schedulable colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #A' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Disk Pressure colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #B' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Memory Pressure colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #C' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: PID Pressure colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #D' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Ready colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #E' thresholds: - 1 - 1 type: string valueMaps: - text: Failed value: 1 - text: OK value: 0 - alias: Node link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-view=pod&var-instance=$__cell&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: node title: Nodes","title":"nodeOverview"},{"location":"docs/documentation/#podoverview","text":"Property Value podOverviewTable podOverviewTable podOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_pod_info{cluster=\"$cluster\", namespace=~\"$namespace\"}, pod) panel expr: - \"sum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"$cluster\\\", namespace=~\\\"\\ $namespace\\\", phase=\\\"Running\\\"} * 1) +\\nsum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", phase=\\\"Succeeded\\\"} * 2) +\\nsum by (namespace,\\ \\ pod) (kube_pod_status_phase{cluster=\\\"$cluster\\\", namespace=~\\\"$namespace\\\", phase=\\\"\\ Unknown\\\"} * 3) +\\nsum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"$cluster\\\"\\ , namespace=~\\\"$namespace\\\", phase=\\\"Failed\\\"} * 4) +\\nsum by (namespace, pod) (kube_pod_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", pod=~\\\"$pod\\\", phase=\\\"Pending\\\"} * 5)\\n\" sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: Value thresholds: - 3 - 3 type: string valueMaps: - text: Running value: 1 - text: Succeeded value: 2 - text: Unknown value: 3 - text: Failed value: 4 - text: Pending value: 5 - alias: Namespace pattern: namespace type: string - alias: Pod link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-container=All&var-view=pod&var-namespace=${__cell_1}&var-pod=${__cell_2}&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: pod title: Pods","title":"podOverview"},{"location":"docs/documentation/#pvcoverview","text":"Property Value pvcOverviewTable pvcOverviewTable pvcOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_persistentvolumeclaim_info{cluster=\"$cluster\", namespace=~\"$namespace\"}, persistentvolumeclaim) panel description: Capacity is available only for remote pvc. expr: - sum by (persistentvolumeclaim, namespace) (((kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\", namespace=~\"$namespace\", persistentvolumeclaim=~\"$pvc\"} - kubelet_volume_stats_available_bytes{cluster=\"$cluster\", namespace=~\"$namespace\", persistentvolumeclaim=~\"$pvc\"}) / kubelet_volume_stats_capacity_bytes{cluster=\"$cluster\", namespace=~\"$namespace\", persistentvolumeclaim=~\"$pvc\"}) * 100) - \"sum by (persistentvolumeclaim, namespace) (kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", persistentvolumeclaim=~\\\"$pvc\\\", phase=\\\"\\ Bound\\\"} * 1) +\\nsum by (persistentvolumeclaim, namespace) (kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", persistentvolumeclaim=~\\\"$pvc\\\", phase=\\\"\\ Lost\\\"} * 2) +\\nsum by (persistentvolumeclaim, namespace) (kube_persistentvolumeclaim_status_phase{cluster=\\\"\\ $cluster\\\", namespace=~\\\"$namespace\\\", persistentvolumeclaim=~\\\"$pvc\\\", phase=\\\"\\ Pending\\\"} * 3)\\n\" sort: col: 3 desc: true styles: - pattern: Time type: hidden - alias: Capacity colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' pattern: 'Value #A' thresholds: - 85 - 97 type: number unit: percent - alias: Status colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 1 pattern: 'Value #B' thresholds: - 2 - 2 type: string valueMaps: - text: Bound value: 1 - text: Lost value: 2 - text: Pending value: 3 - alias: PVC link: true linkTooltip: Detail linkUrl: /d/persistentvolumes?var-namespace=${__cell_1}&var-pvc=${__cell_2}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: persistentvolumeclaim - alias: Namespace pattern: namespace type: string title: Persistent Volumes","title":"pvcOverview"},{"location":"docs/documentation/#statefulsetoverview","text":"Property Value statefulSetOverviewTable statefulSetOverviewTable statefulSetOverviewTable Property Value base \"baseTableTemplate\" dashboardInfo grafanaTemplateQuery: label_values(kube_statefulset_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\"}, statefulset) panel expr: - sum by (statefulset, namespace) (kube_statefulset_status_replicas_updated{cluster=\"$cluster\", namespace=~\"$namespace\", statefulset=~\"$statefulset\"}) - sum by (statefulset, namespace) (kube_statefulset_status_replicas{cluster=\"$cluster\", namespace=~\"$namespace\", statefulset=~\"$statefulset\"}) - sum by (statefulset, namespace) (kube_statefulset_status_replicas_ready{cluster=\"$cluster\", namespace=~\"$namespace\", statefulset=~\"$statefulset\"}) sort: col: 4 desc: true styles: - pattern: Time type: hidden - alias: Updated pattern: 'Value #A' type: number - alias: Ready colorMode: cell colors: - '#56a64b' - '#ff780a' - '#e02f44' mappingType: 2 pattern: 'Value #B' rangeMaps: - from: 0 text: OK to: 0 - from: 1 text: Failed to: 100000000000000005366162204393472 thresholds: - 1 - 1 type: string - alias: StatefulSet link: true linkTooltip: Detail linkUrl: /d/statefulset?var-namespace=${__cell_1}&var-statefulset=${__cell_2}&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: statefulset - alias: Namespace link: true linkTooltip: Detail linkUrl: /d/containerdetail?var-namespace=$__cell&var-pod=All&var-view=pod&var-search=&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to pattern: namespace title: StatefulSets","title":"statefulSetOverview"},{"location":"docs/documentation/#vm_1","text":"Property Value mostUtilizedVMCPU mostUtilizedVMCPU mostUtilizedVMDisk mostUtilizedVMDisk mostUtilizedVMNetworkErrors mostUtilizedVMNetworkErrors mostUtilizedVMRAM mostUtilizedVMRAM overallNetworkErrors overallNetworkErrors overallUtilizationCPU overallUtilizationCPU overallUtilizationDisk overallUtilizationDisk overallUtilizationRAM overallUtilizationRAM targetDown targetDown totalCores totalCores totalDisk totalDisk totalRAM totalRAM usedCores usedCores usedDisk usedDisk usedRAM usedRAM mostUtilizedVMCPU Property Value alert customLables: alertgroup: ClusterVM expr: round((1 - (avg(irate(node_cpu_seconds_total{job=~\"%s\", mode=\"idle\"}[5m]) * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High CPU Utilization {{ $value }}%' name: VMCPUUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m]) * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 3 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized VM mostUtilizedVMDisk Property Value alert customLables: alertgroup: ClusterVM expr: round((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High Disk Utilization {{ $value }}%' name: VMDiskUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: max(round((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) / ((sum(node_filesystem_size_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device) - sum(node_filesystem_free_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) + sum(node_filesystem_avail_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) by (job, nodename, device)) * 100 > 0)) gridPos: w: 3 x: 15 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized VM mostUtilizedVMNetworkErrors Property Value alert customLables: alertgroup: ClusterVM expr: sum(rate(node_network_transmit_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High Network Errors Count {{ $value }}%' name: VMNetworkErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: max(sum(rate(node_network_transmit_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance) group_left(nodename) (node_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 21 y: 6 thresholds: critical: 15 operator: '>=' warning: 10 title: Most Affected VM unit: pps mostUtilizedVMRAM Property Value alert customLables: alertgroup: ClusterVM expr: round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{job=~\"%s\"} * on(instance) group_left(nodename) (node_uname_info))) * 100) linkGetParams: var-instance={{ $labels.nodename }} message: 'VM {{ $labels.nodename }}: High RAM Utilization {{ $value }}%' name: VMRAMUtilizationHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: max(round((1 - sum by (job, nodename) (node_memory_MemAvailable_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info)) / sum by (job, nodename) (node_memory_MemTotal_bytes{job=~\"$job\"} * on(instance) group_left(nodename) (node_uname_info))) * 100)) gridPos: w: 3 x: 9 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Most Utilized VM overallNetworkErrors Property Value alert customLables: alertgroup: ClusterVM expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"%s\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename)) message: VM High Overall Network Errors Count {{ $value }}% name: VMNetworkOverallErrorsHigh thresholds: critical: 15 operator: '>=' warning: 10 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: sum(sum(rate(node_network_transmit_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"} [5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename) + sum(rate(node_network_receive_errs_total{job=~\"$job\", device!~\"lo|veth.+|docker.+|flannel.+|cali.+|cbr.|cni.+|br.+\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info) ) by (job, nodename)) gridPos: w: 3 x: 18 y: 6 thresholds: critical: 15 operator: '>=' warning: 10 title: Overall Errors unit: pps overallUtilizationCPU Property Value alert customLables: alertgroup: ClusterVM expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"%s\", mode=\"idle\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) message: VM High CPU Overall Utilization {{ $value }}% name: VMCPUOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to expr: avg(round((1 - (avg(irate(node_cpu_seconds_total{job=~\"$job\", mode=\"idle\"}[5m]) * on(instance, cluster) group_left(nodename) (node_uname_info)) by (job, nodename) )) * 100)) gridPos: w: 3 x: 0 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationDisk Property Value alert customLables: alertgroup: ClusterVM expr: round((sum(node_filesystem_size_bytes{job=~\"%s\"}) - sum(node_filesystem_free_bytes{job=~\"%s\"})) / (sum(node_filesystem_size_bytes{job=~\"%s\"}) - sum(node_filesystem_free_bytes{job=~\"%s\"}) + sum(node_filesystem_avail_bytes{job=~\"%s\"})) * 100 > 0) message: VM High Disk Overall Utilization {{ $value }}% name: VMDiskOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the disk utilization is calculated using the fraction:\\n\\ ```\\n /( + )\\n```\\nThe value of \\ \\ is reduced by 5% of the available disk capacity, because \\nthe file system\\ \\ marks 5% of the available disk capacity as reserved. \\nIf less than 5% is free,\\ \\ using the remaining reserved space requires root privileges.\\nAny non-privileged\\ \\ users and processes are unable to write new data to the partition. See the list\\ \\ of explicitly ignored mount points and file systems [here](https://github.com/dNationCloud/kubernetes-monitoring-stack/blob/main/chart/values.yaml)\" expr: round((sum(node_filesystem_size_bytes{job=~\"$job\"}) - sum(node_filesystem_free_bytes{job=~\"$job\"})) / (sum(node_filesystem_size_bytes{job=~\"$job\"}) - sum(node_filesystem_free_bytes{job=~\"$job\"}) + sum(node_filesystem_avail_bytes{job=~\"$job\"})) * 100 > 0) gridPos: w: 3 x: 12 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization overallUtilizationRAM Property Value alert customLables: alertgroup: ClusterVM expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"%s\"} * on(instance, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"%s\"} * on(instance, cluster) group_left(nodename) (node_uname_info))) * 100)) message: VM High RAM Overall Utilization {{ $value }}% name: VMRAMOverallHigh thresholds: critical: 90 operator: '>=' warning: 75 linkTo - nodeexporter panel dataLinks: - title: System Overview url: /d/nodeexporter?var-job=$job&refresh=10s&var-datasource=$datasource&var-cluster=$cluster&from=$__from&to=$__to description: \"The percentage of the memory utilization is calculated by:\\n```\\n1 -\\ \\ ( / )\\n```\" expr: avg(round((1 - sum by (job, nodename, cluster) (node_memory_MemAvailable_bytes{job=~\"$job\"} * on(instance, cluster) group_left(nodename) (node_uname_info)) / sum by (job, nodename, cluster) (node_memory_MemTotal_bytes{job=~\"$job\"} * on(instance, cluster) group_left(nodename) (node_uname_info))) * 100)) gridPos: w: 3 x: 6 y: 6 thresholds: critical: 90 operator: '>=' warning: 75 title: Overall Utilization targetDown Property Value alert customLables: alertgroup: ClusterVM expr: 100 * (count by(job, namespace, service) (up{job=~\"%s\"} == 0) / count by(job, namespace, service) (up{job=~\"%s\"})) message: '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.' name: VMTargetDown thresholds: critical: 90 operator: '>=' warning: 10 panel null totalCores Property Value panel colorMode: value expr: count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 3 y: 9 thresholds: color: '#858187' value: title: Total Cores unit: none totalDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 15 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes totalRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 9 y: 9 thresholds: color: '#858187' value: title: Total unit: bytes usedCores Property Value panel colorMode: value expr: (1 - (avg(irate(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"idle\"}[5m])))) * count(node_cpu_seconds_total{cluster=\"$cluster\", job=~\"$job\", mode=\"system\"}) graphMode: none gridPos: h: 2 w: 3 x: 0 y: 9 thresholds: color: '#858187' value: title: Used Cores unit: none usedDisk Property Value panel colorMode: value expr: sum(node_filesystem_size_bytes{cluster=\"$cluster\", job=~\"$job\"}) - sum(node_filesystem_free_bytes{cluster=\"$cluster\", job=~\"$job\"}) graphMode: none gridPos: h: 2 w: 3 x: 12 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes usedRAM Property Value panel colorMode: value expr: sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"}) * (((1 - sum(node_memory_MemAvailable_bytes{cluster=\"$cluster\", job=~\"$job\"}) / sum(node_memory_MemTotal_bytes{cluster=\"$cluster\", job=~\"$job\"})))) graphMode: none gridPos: h: 2 w: 3 x: 6 y: 9 thresholds: color: '#858187' value: title: Used unit: bytes","title":"vm"},{"location":"docs/documentation/#commonthresholds","text":"Property Value app app controlPlane controlPlane criticalPanel criticalPanel k8s k8s node node warningPanel warningPanel","title":"commonThresholds"},{"location":"docs/documentation/#app","text":"Property Value critical 95 operator \" < \" warning 99","title":"app"},{"location":"docs/documentation/#controlplane","text":"Property Value critical 95 lowest 0 operator \" < \" warning 99","title":"controlPlane"},{"location":"docs/documentation/#criticalpanel","text":"Property Value critical 1 operator \">=\"","title":"criticalPanel"},{"location":"docs/documentation/#k8s_2","text":"Property Value critical 95 operator \" < \" warning 99","title":"k8s"},{"location":"docs/documentation/#node","text":"Property Value critical 90 operator \">=\" warning 75","title":"node"},{"location":"docs/documentation/#warningpanel","text":"Property Value operator \">=\" warning 1","title":"warningPanel"},{"location":"docs/documentation/#templatebases","text":"Property Value baseAlert baseAlert basePolystatTemplate basePolystatTemplate baseStatsTemplate baseStatsTemplate baseTableTemplate baseTableTemplate","title":"templateBases"},{"location":"docs/documentation/#basealert","text":"Property Value customLables {} expr \"\" linkGetParams \"\" message \"\" name \"error must be overwritten\" thresholds {}","title":"baseAlert"},{"location":"docs/documentation/#basepolystattemplate","text":"Property Value default true enabled true panel panel panel Property Value datasource \"$datasource\" default_click_through \"\" description \"\" expr \"\" fontAutoColor false fontColor \"white\" globalDecimals null global_thresholds {} global_unit_format \"\" gridPos h: 6 w: 24 x: 0 y: 0 hexagon_sort_by_direction 2 hexagon_sort_by_field \"value\" polygon_border_size 0 title \"error must be overwritten\" tooltip_timestamp_enabled false","title":"basePolystatTemplate"},{"location":"docs/documentation/#basestatstemplate","text":"Property Value alert {} default true enabled true panel panel panel Property Value colorMode \"background\" dataLinks [] datasource \"$datasource\" decimals null description \"\" expr \"\" graphMode \"area\" gridPos h: 3 w: 6 x: error must be overwritten y: error must be overwritten mappings [] thresholds {} title \"error must be overwritten\" unit \"percent\"","title":"baseStatsTemplate"},{"location":"docs/documentation/#basetabletemplate","text":"Property Value default true enabled true panel panel panel Property Value datasource \"$datasource\" description \"\" expr [] gridPos h: 19 w: 24 x: 0 y: 1 sort {} styles [] title \"error must be overwritten\" transformations []","title":"baseTableTemplate"},{"location":"helpers/","text":"Helpers A set of scripts and configuration files which helps to simplify local development. Local development using KinD (Kubernetes in Docker) Prerequisites Kind Docker Helm3 Kubectl Grafana dashboards and Prometheus alerts are stored in the jsonnet templates. Jsonnet templates are shipped in compressed form by the k8s configmap. Then the k8s configmap with compressed jsonnet is consumed by kubernetes-jsonnet-translator . kubernetes-jsonnet-translator translates jsonnet templates to the plain json and generates prometheus rule or grafana configmap k8s objects. If you want to test your local changes in local KinD k8s cluster use following steps: Create KinD cluster kind create cluster --config helpers/kind_cluster_config.yaml --image kindest/node:v1.25.11 Install kubernetes-monitoring-stack (without dNation Kubernetes Monitoring dependency) K8s-m8g-stack is an umbrella helm chart which deploys Grafana, Loki and Prometheus Operator projects. # Add dNation helm repository helm repo add dnationcloud https://dnationcloud.github.io/helm-hub/ helm repo update # Install dNation Kubernetes Monitoring Stack without dNation Kubernetes Monitoring chart helm install dnation-kubernetes-monitoring-stack dnationcloud/dnation-kubernetes-monitoring-stack -f https://raw.githubusercontent.com/dNationCloud/kubernetes-monitoring-stack/main/helpers/values-kind.yaml --set dnation-kubernetes-monitoring.enabled = false Follow installation notes and use Port Forwarding if you want to access the Grafana server from outside your KinD cluster export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=dnation-kubernetes-monitoring-stack\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace default port-forward $POD_NAME 3000 Package jsonnet templates make jsonnet-package Deploy dNation Kubernetes Monitoring with your changes # Update K8s monitoring chart dependencies helm dependency update chart # K8s monitoring only (default) helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack # Cluster monitoring example with custom dashboard templates helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack -f helpers/values-cluster-elk.yaml # Host monitoring example helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack -f helpers/values-host.yaml # Multi-cluster monitoring example helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack -f helpers/values-multicluster.yaml If you want to run jsonnet formatter or linter use following: # Format jsonnet files make jsonnet-fmt # Lint jsonnet files make jsonnet-lint If you want to generate plain json grafana dashboards or prometheus rules use following: # Build json grafana dashboards make json-dashboards # Build json prometheus rules make json-rules If you want to run helm linter use following: # Lint helm chart make helm-lint","title":"Helpers"},{"location":"helpers/#helpers","text":"A set of scripts and configuration files which helps to simplify local development.","title":"Helpers"},{"location":"helpers/#local-development-using-kind-kubernetes-in-docker","text":"Prerequisites Kind Docker Helm3 Kubectl Grafana dashboards and Prometheus alerts are stored in the jsonnet templates. Jsonnet templates are shipped in compressed form by the k8s configmap. Then the k8s configmap with compressed jsonnet is consumed by kubernetes-jsonnet-translator . kubernetes-jsonnet-translator translates jsonnet templates to the plain json and generates prometheus rule or grafana configmap k8s objects. If you want to test your local changes in local KinD k8s cluster use following steps: Create KinD cluster kind create cluster --config helpers/kind_cluster_config.yaml --image kindest/node:v1.25.11 Install kubernetes-monitoring-stack (without dNation Kubernetes Monitoring dependency) K8s-m8g-stack is an umbrella helm chart which deploys Grafana, Loki and Prometheus Operator projects. # Add dNation helm repository helm repo add dnationcloud https://dnationcloud.github.io/helm-hub/ helm repo update # Install dNation Kubernetes Monitoring Stack without dNation Kubernetes Monitoring chart helm install dnation-kubernetes-monitoring-stack dnationcloud/dnation-kubernetes-monitoring-stack -f https://raw.githubusercontent.com/dNationCloud/kubernetes-monitoring-stack/main/helpers/values-kind.yaml --set dnation-kubernetes-monitoring.enabled = false Follow installation notes and use Port Forwarding if you want to access the Grafana server from outside your KinD cluster export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=dnation-kubernetes-monitoring-stack\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl --namespace default port-forward $POD_NAME 3000 Package jsonnet templates make jsonnet-package Deploy dNation Kubernetes Monitoring with your changes # Update K8s monitoring chart dependencies helm dependency update chart # K8s monitoring only (default) helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack # Cluster monitoring example with custom dashboard templates helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack -f helpers/values-cluster-elk.yaml # Host monitoring example helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack -f helpers/values-host.yaml # Multi-cluster monitoring example helm install dnation-kubernetes-monitoring chart --set releaseOverride = dnation-kubernetes-monitoring-stack -f helpers/values-multicluster.yaml If you want to run jsonnet formatter or linter use following: # Format jsonnet files make jsonnet-fmt # Lint jsonnet files make jsonnet-lint If you want to generate plain json grafana dashboards or prometheus rules use following: # Build json grafana dashboards make json-dashboards # Build json prometheus rules make json-rules If you want to run helm linter use following: # Lint helm chart make helm-lint","title":"Local development using KinD (Kubernetes in Docker)"},{"location":"helpers/FAQ/","text":"Frequently Asked Questions (FAQ) How to interpret displayed monitoring values across different monitoring layers? Monitoring Layer 0 Kubernetes monitoring Kubernetes cluster health is simply represented by the single state panel. State panel aggregates alerts from the underlying layer of kubernetes monitoring. The healthy status is represented by the green state panel with 'OK' label. It is displayed if the k8s cluster meets all the defined thresholds for healthy status. When some threshold is crossed and this state persists for 5 minutes (default), relevant alert is triggered and highlighted. The orange state panel with Warning label is displayed in case of warning alert. Red state panel with Critical label is displayed in case of critical alert. We have implemented intuitive green, orange and red color indicators that are signalizing if your action is needed or if everything is OK. If you want to see more information about your k8s cluster, just drill down by left-clicking on the relevant state panel. As of now, multi-cluster monitoring support is available. If you are interested in the k8s cluster monitoring see How to set up k8s cluster monitoring? section. If you are interested in the k8s multi-cluster monitoring see How to set up k8s multi-cluster monitoring? section. Host monitoring Host monitoring integration allows you to monitor your hosts infrastructure within our kubernetes based, stable and highly available monitoring. Hosts state panel functions on the same principles as k8s cluster state panel. Multi host monitoring is supported. - If you are interested in the host monitoring see How to set up host monitoring? section. K8s monitoring Layer 1 The first layer of k8s monitoring consists of state panels that visualize the current state of your k8s cluster. The state panels are separated into several sections: - Alerts - Overview - Control Plane Components Health - Node Metrics (including Master) - Applications (optional) When a failed condition of monitored k8s element occurs the state panel shows lowered percentage value on health indicator. If the state of health is too low and the percentage value reached warning or even critical threshold, corresponding state panel changes its color. Intuitive green, orange and red color principle is used. When failed state lasts longer than 5 minutes (default) then the relevant alert is triggered and highlighted in alert panel. Also the panel representing the overall health of the k8s cluster in the monitoring layer 0 changes accordingly. The first layer is the source of all aggregated k8s cluster alerts triggered by dNation monitoring. Alerts The upper section shows the amount of triggered critical and warning alerts. If you want to see detailed list of triggered alerts simply apply the drill down principle. Overview The overview section allows you to monitor the health status of k8s nodes, workloads (deployments, stateful sets, daemon sets, pods, containers and more) and persistent volumes. Each state panel contains important information of monitored k8s element, e.g. single node health state panel gives you an insight on whether the k8s nodes are able to schedule resources or if they are under disk, memory or PID pressure. Control Plane Components Monitoring of k8s cluster control plane components (api server, controller manager, etcd database, kubelet, proxy and scheduler) is located in separate section. If you want to check the work queue rate of controller manager or scheduler latency, the Control Plane Components section is the section you are looking for. Node Metrics (including Master) Measuring k8s nodes system metrics is important in ensuring k8s cluster availability. Node Metrics section gives you a clear overview of cluster's CPU, memory, disk and network utilization. Each system metric is visualized in several state panels such as overall (average) utilization panel, the most utilized cluster node panel and information panels which show used and total state of system metrics. It is always useful to know overall cluster utilization, but it is also important to know when one node is utilized more than other nodes which may indicates that your k8s cluster doesn't work properly. If you want to drill down for further investigation of your k8s node you can select between System Overview or K8s Overview buttons. System Overview shows CPU, memory, disk and network utilization per k8s cluster node in greater detail. K8s Overview shows systems resource usage in k8s oriented manner, meaning that you can filter particular k8s workload per k8s cluster node or see information about CPU and memory requests and limits. Applications In order to thoroughly understand application health, we created custom dashboards which help us to properly understand and diagnose application workloads in k8s cluster. As of today we have designed several dashboards for well known and widely used frameworks such as java actuator, python flask, nginx ingress controller and more. Layer 1 state panels aggregate important information of monitored application. Green, orange and red color principle informs us if there is action needed and drill down principle can be used to access verbose and detailed application dashboard. If you want to customize your L1 layer see How to customize my k8s monitoring? section. If you are interested in the k8s application monitoring see How to set up k8s application monitoring? section. K8s monitoring lower layers If you want to know details on why is particular stat panel in upper layer green, orange or red, just drill down. Look at some examples from layer 2 and layer 3 of k8s monitoring: K8s monitoring Layer 2 example Containers Node Disks Application K8s monitoring Layer 3 example Containers Nodes Host monitoring Layer 1 The first layer of host monitoring consists of state panels that visualize the current state of your host. The state panels are separated into several sections: - Alerts - Host - Applications (optional) When a failed condition of monitored host element occurs the state panel shows lowered percentage value on health indicator. If the state of health is too low and the percentage value reached warning or even critical threshold, corresponding state panel changes its color. Intuitive green, orange and red color principle is used. When failed state lasts longer than 5 minutes (default) then the relevant alert is triggered and highlighted in alert panel. Also the panel representing the overall health of the host in the monitoring layer 0 changes accordingly. The first layer is the source of all aggregated hosts alerts triggered by dNation monitoring. Alerts The upper section shows the amount of triggered critical and warning alerts. If you want to see detailed list of triggered alerts simply apply the drill down principle. Host Host section gives you clear overview of host's CPU, memory, disk and network utilization. Each system metric is visualized in several state panels. The main host overall utilization panel and information panels show used and total state of system metric. Applications In order to thoroughly understand application health, we created custom dashboards which help us to properly understand and diagnose host applications. As of today, we have designed several dashboards such as cadvisor. Layer 1 state panels aggregate important information of monitored application. Green, orange and red color principle informs us if there is some action needed and drill down principle can be used to access verbose and detailed application dashboard. If you want to customize your L1 layer by custom host monitoring template definition, see How to customize my host monitoring? section. If you are interested in the host application monitoring, see How to set up host application monitoring? section. Host monitoring lower layers If you want to know details why is some stats panel in upper layer green, orange or red, just drill down and click it. See some examples from layer 2 of host monitoring: Host Detail Application How to set up k8s cluster monitoring? Prerequisites Install dNation k8s monitoring on k8s cluster. See installation steps here . Set up K8s cluster monitoring is enabled by default. See the docs : clusterMonitoring : enabled : true clusters : - name : K8sCluster label : observer-cluster # The label should be the same as the external_label `cluster` from prometheus description : 'Kubernetes cluster monitoring' apps : [] It means that your k8s cluster is being monitored right after installation of dNation k8s monitoring. If you want to customize k8s cluster monitoring see How to customize my k8s monitoring? section. How to customize my k8s monitoring? We understand that each k8s cluster may contain various workloads or implement various scaling strategies. We tried to do our best and set up suitable defaults based on our production environment experiences, but we know that isn't possible cover the variety of k8s cluster configurations. To tackle this problem, we implemented template logic that allows you to fully customize the default state panels or alerts. You can also create your own to fulfill your k8s cluster monitoring requirements. Currently only first layer is customizable, however, the templating of other layers is under development. For full list of k8s templates see the docs . If you want to customize default layer 1 template in k8s monitoring just create a simple yaml file: templates : k8s : # Override stats panel title and warning thresholds for the etcd health panel etcdHealth : panel : title : \"Custom Title\" thresholds : warning : 96 alert : thresholds : warning : 96 Each state panel that can be modified contains a template name in its description for easy navigation: Finally, update your k8s monitoring deployment and override the default monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f override.yaml If you want to create your own custom template see full examples in the helpers directory. How to set up k8s application monitoring? Prerequisites Enable or install metrics exporter in your k8s application. Set up K8s application monitoring is disabled by default. See the docs . To enable it just create application monitoring definition as follows: clusterMonitoring : enabled : true clusters : - name : K8sCluster label : observer-cluster # The label should be the same as the external_label `cluster` from prometheus description : 'Kubernetes cluster with application monitoring' apps : - name : app-example description : Example of App Monitoring jobName : flask-app # The job name should be the same as the name which will be retrieved from the `jobLabel`, see the `serviceMonitor` section templates : javaActuator : # Application Exporter template enabled : true serviceMonitor : jobLabel : app # The label to use to retrieve the job name from. namespaceSelector : # Namespaces to transfer from the kubernetes service to the target matchNames : - default selector : # Label selector for services to which this ServiceMonitor applies matchLabels : app : flask-app # Endpoints of the selected service to be monitored # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#endpoint endpoints : - targetPort : metrics # Name or number of the endpoint's target port path : /metrics # HTTP path to scrape for metrics Application monitoring definition contains two main sections: template and serviceMonitor or podMonitor respectively. The template section defines which template and pre-defined application dashboard will be used. For full list of pre-defined k8s application templates see the docs . Application monitoring metrics endpoint is auto-discovered by service monitor or by pod monitor CRDs. See the documentation of CRDs to ensure that your application metrics endpoint will be discovered as you want or find some examples in helpers directory. Finally, update your k8s monitoring deployment and apply the application monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f apps.yaml - If you want to customize the default application template see How to customize my k8s monitoring? section. How to set up host monitoring? Prerequisites Enable or install node exporter in your host. You can use helpers/node_exporter.sh installation script. Set up Host monitoring is disabled by default. See the docs . To enable it just create host monitoring definition as follows: hostMonitoring : enabled : true hosts : - name : host-01 description : Host 01 jobName : host-01 host : address : 1.1.1.1 # Host IP address serviceMonitor : endpoints : - port : 9100 # Node exporter port path : /metrics # Node exporter metrics path Host monitoring definition contains two main sections: host and serviceMonitor . The host section simply defines the host IP address. The serviceMonitor defines service monitor CRD endpoint i.e. the host's node exporter port and path. Lot of service monitor parameters are pre-configured to simplify host monitoring configuration. If you are interested how or you want to override some of these parameters see the chart/templates/hosts directory. Finally, update your k8s monitoring deployment and apply the host monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f host.yaml How to customize my host monitoring? To cover variety of host configurations we implemented the template logic in the host monitoring as well. Templates allow you to fully customize the default state panels or alerts. You can also create your own to fulfill your host monitoring requirements. Currently, layer 1 customization is implemented. For full list of host templates see the docs . If you want to customize default layer 1 template in host monitoring just create a simple yaml file: templates : host : # Override warning thresholds for the CPU utilization panel overallUtilizationCPU : panel : thresholds : warning : 50 alert : thresholds : warning : 50 Each state panel that can be modified contains a template name in its description for easy navigation: Finally, update your k8s monitoring deployment and override the default monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f override.yaml If you want to create your own custom template see full examples in the helpers directory. How to set up host application monitoring? Prerequisites Enable or install metrics exporter in your host application. Set up Hosts application monitoring is disabled by default. See the docs . To enable it just create application monitoring definition as follows: hostMonitoring : enabled : true hosts : - name : host-01 description : 'Host 01 with application monitoring' host : address : 1.2.3.4 apps : - name : host-01-docker description : Host 01 Docker Containers jobName : host-01-docker templates : cAdvisor : # Application Exporter template enable : true serviceMonitor : endpoints : - port : 9101 # Application exporter port path : /metrics # Application exporter metrics path Application monitoring definition contains two main sections: template and serviceMonitor . The template section defines which template and pre-defined application dashboard will be used. For full list of pre-defined host application templates see the docs . The serviceMonitor defines service monitor CRD endpoint, i.e. the application's exporter port and path. Lot of service monitor parameters are pre-configured to simplify application monitoring configuration. If you are curious how or you want to override some of these parameters see the chart/templates/hosts directory or find some examples in helpers directory. Finally, update your k8s monitoring deployment and apply the application monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f apps.yaml - If you want to customize the default application template see the How to customize my host monitoring? section. Why L2 table item has different background color as corresponding L1 stat panel? If you customized your k8s monitoring , you can see that some table items on L2 layer dashboard may have different background color as corresponding L1 stat panel. Example: - Define custom L1 PVC utilization panel with custom thresholds for ELK PVCs - If you drill down, you can see that corresponding PVC capacity is in Critical state (orange background color) Currently only first layer is customizable, which caused this unexpected behaviour. Templating of other layers is under development and is planned in v1.1.x release . Kubernetes Monitoring shows - OR 0% state for some control plane components. Are control plane components working correctly? Control plane components work probably well, but their metrics server might be disabled, misconfigured or may not be present at all. For example, if is used OVNKubernetes CNI kube-proxy doesn't exist here. You should want to check address bindings of control plane components' metrics as follows: The metrics of etcd and kube-proxy control plane components are by default bound to the localhost that prometheus instances cannot access. Also make sure metrics of scheduler and controller-manager control plane components don't have the same address binding if you want to collect them. Edit and use kubeadm_init.yaml file to configure kubeadm init in case of fresh K8s deployment. kubeadm init --config = helpers/kubeadm_init.yaml Manual setup in case of already running K8s deployment. Setup etcd metrics bind address # On k8s master node cd /etc/kubernetes/manifests/ sudo vim etcd.yaml # Add listen-metrics-urls as etcd command option ... - --listen-metrics-urls = http://0.0.0.0:2381 ... Setup kube-proxy metrics bind address Edit kube-proxy daemon set kubectl edit ds kube-proxy -n kube-system ...containers: - command: - /usr/local/bin/kube-proxy - --config = /var/lib/kube-proxy/config.conf - --hostname-override = $( NODE_NAME ) - --metrics-bind-address = 0 .0.0.0 # Add metrics-bind-address line Edit kube-proxy config map kubectl -n kube-system edit cm kube-proxy ... kind: KubeProxyConfiguration metricsBindAddress: \"0.0.0.0:10249\" # Add metrics-bind-address host:port mode: \"\" Delete the kube-proxy pods and reapply the new configuration kubectl -n kube-system delete po -l k8s-app = kube-proxy Setup scheduler metrics bind address # On k8s master node cd /etc/kubernetes/manifests/ sudo vim kube-scheduler.yaml # Edit bind-address and port command options ... - --bind-address = 0 .0.0.0 - --secure-port = 10259 ... Setup controller-manager metrics bind address # On k8s master node cd /etc/kubernetes/manifests/ sudo vim kube-controller-manager.yaml # Edit bind-address and port command options ... - --bind-address = 0 .0.0.0 - --secure-port = 10257 ... You should also check: * TLS configuration of prometheus components responsible for scraping controller-manager and scheduler . If you don't want to set up TLS, you can skip validation as shown here . Authorization to access controller-manager and scheduler . You can skip authorization for some endpoints by setting --authorization-always-allow-paths: \"/healthz,/readyz,/livez,/metrics\" in kubeadm_init.yaml ( see example for kind cluster ) or manually in already running K8s deployment by following same steps as above when setting metrics bind address. How to set up k8s multi-cluster monitoring? Prerequisites Install dNation k8s monitoring on k8s cluster. See installation steps here . Set up Multi-cluster monitoring is supported by default. All you need to do is just to install dNation Kubernetes Monitoring with custom values as shown in example here . clusterMonitoring : enabled : true clusters : - name : Observer label : observer-cluster description : 'Kubernetes cluster with application monitoring' apps : [] - name : Workload label : workload-cluster description : 'Kubernetes cluster with application monitoring' apps : [] Do not forget to set the correct labels for your clusters!","title":"FAQ"},{"location":"helpers/FAQ/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"helpers/FAQ/#how-to-interpret-displayed-monitoring-values-across-different-monitoring-layers","text":"","title":"How to interpret displayed monitoring values across different monitoring layers?"},{"location":"helpers/FAQ/#monitoring-layer-0","text":"","title":"Monitoring Layer 0"},{"location":"helpers/FAQ/#kubernetes-monitoring","text":"Kubernetes cluster health is simply represented by the single state panel. State panel aggregates alerts from the underlying layer of kubernetes monitoring. The healthy status is represented by the green state panel with 'OK' label. It is displayed if the k8s cluster meets all the defined thresholds for healthy status. When some threshold is crossed and this state persists for 5 minutes (default), relevant alert is triggered and highlighted. The orange state panel with Warning label is displayed in case of warning alert. Red state panel with Critical label is displayed in case of critical alert. We have implemented intuitive green, orange and red color indicators that are signalizing if your action is needed or if everything is OK. If you want to see more information about your k8s cluster, just drill down by left-clicking on the relevant state panel. As of now, multi-cluster monitoring support is available. If you are interested in the k8s cluster monitoring see How to set up k8s cluster monitoring? section. If you are interested in the k8s multi-cluster monitoring see How to set up k8s multi-cluster monitoring? section.","title":"Kubernetes monitoring"},{"location":"helpers/FAQ/#host-monitoring","text":"Host monitoring integration allows you to monitor your hosts infrastructure within our kubernetes based, stable and highly available monitoring. Hosts state panel functions on the same principles as k8s cluster state panel. Multi host monitoring is supported. - If you are interested in the host monitoring see How to set up host monitoring? section.","title":"Host monitoring"},{"location":"helpers/FAQ/#k8s-monitoring-layer-1","text":"The first layer of k8s monitoring consists of state panels that visualize the current state of your k8s cluster. The state panels are separated into several sections: - Alerts - Overview - Control Plane Components Health - Node Metrics (including Master) - Applications (optional) When a failed condition of monitored k8s element occurs the state panel shows lowered percentage value on health indicator. If the state of health is too low and the percentage value reached warning or even critical threshold, corresponding state panel changes its color. Intuitive green, orange and red color principle is used. When failed state lasts longer than 5 minutes (default) then the relevant alert is triggered and highlighted in alert panel. Also the panel representing the overall health of the k8s cluster in the monitoring layer 0 changes accordingly. The first layer is the source of all aggregated k8s cluster alerts triggered by dNation monitoring.","title":"K8s monitoring Layer 1"},{"location":"helpers/FAQ/#alerts","text":"The upper section shows the amount of triggered critical and warning alerts. If you want to see detailed list of triggered alerts simply apply the drill down principle.","title":"Alerts"},{"location":"helpers/FAQ/#overview","text":"The overview section allows you to monitor the health status of k8s nodes, workloads (deployments, stateful sets, daemon sets, pods, containers and more) and persistent volumes. Each state panel contains important information of monitored k8s element, e.g. single node health state panel gives you an insight on whether the k8s nodes are able to schedule resources or if they are under disk, memory or PID pressure.","title":"Overview"},{"location":"helpers/FAQ/#control-plane-components","text":"Monitoring of k8s cluster control plane components (api server, controller manager, etcd database, kubelet, proxy and scheduler) is located in separate section. If you want to check the work queue rate of controller manager or scheduler latency, the Control Plane Components section is the section you are looking for.","title":"Control Plane Components"},{"location":"helpers/FAQ/#node-metrics-including-master","text":"Measuring k8s nodes system metrics is important in ensuring k8s cluster availability. Node Metrics section gives you a clear overview of cluster's CPU, memory, disk and network utilization. Each system metric is visualized in several state panels such as overall (average) utilization panel, the most utilized cluster node panel and information panels which show used and total state of system metrics. It is always useful to know overall cluster utilization, but it is also important to know when one node is utilized more than other nodes which may indicates that your k8s cluster doesn't work properly. If you want to drill down for further investigation of your k8s node you can select between System Overview or K8s Overview buttons. System Overview shows CPU, memory, disk and network utilization per k8s cluster node in greater detail. K8s Overview shows systems resource usage in k8s oriented manner, meaning that you can filter particular k8s workload per k8s cluster node or see information about CPU and memory requests and limits.","title":"Node Metrics (including Master)"},{"location":"helpers/FAQ/#applications","text":"In order to thoroughly understand application health, we created custom dashboards which help us to properly understand and diagnose application workloads in k8s cluster. As of today we have designed several dashboards for well known and widely used frameworks such as java actuator, python flask, nginx ingress controller and more. Layer 1 state panels aggregate important information of monitored application. Green, orange and red color principle informs us if there is action needed and drill down principle can be used to access verbose and detailed application dashboard. If you want to customize your L1 layer see How to customize my k8s monitoring? section. If you are interested in the k8s application monitoring see How to set up k8s application monitoring? section.","title":"Applications"},{"location":"helpers/FAQ/#k8s-monitoring-lower-layers","text":"If you want to know details on why is particular stat panel in upper layer green, orange or red, just drill down. Look at some examples from layer 2 and layer 3 of k8s monitoring: K8s monitoring Layer 2 example Containers Node Disks Application K8s monitoring Layer 3 example Containers Nodes","title":"K8s monitoring lower layers"},{"location":"helpers/FAQ/#host-monitoring-layer-1","text":"The first layer of host monitoring consists of state panels that visualize the current state of your host. The state panels are separated into several sections: - Alerts - Host - Applications (optional) When a failed condition of monitored host element occurs the state panel shows lowered percentage value on health indicator. If the state of health is too low and the percentage value reached warning or even critical threshold, corresponding state panel changes its color. Intuitive green, orange and red color principle is used. When failed state lasts longer than 5 minutes (default) then the relevant alert is triggered and highlighted in alert panel. Also the panel representing the overall health of the host in the monitoring layer 0 changes accordingly. The first layer is the source of all aggregated hosts alerts triggered by dNation monitoring.","title":"Host monitoring Layer 1"},{"location":"helpers/FAQ/#alerts_1","text":"The upper section shows the amount of triggered critical and warning alerts. If you want to see detailed list of triggered alerts simply apply the drill down principle.","title":"Alerts"},{"location":"helpers/FAQ/#host","text":"Host section gives you clear overview of host's CPU, memory, disk and network utilization. Each system metric is visualized in several state panels. The main host overall utilization panel and information panels show used and total state of system metric.","title":"Host"},{"location":"helpers/FAQ/#applications_1","text":"In order to thoroughly understand application health, we created custom dashboards which help us to properly understand and diagnose host applications. As of today, we have designed several dashboards such as cadvisor. Layer 1 state panels aggregate important information of monitored application. Green, orange and red color principle informs us if there is some action needed and drill down principle can be used to access verbose and detailed application dashboard. If you want to customize your L1 layer by custom host monitoring template definition, see How to customize my host monitoring? section. If you are interested in the host application monitoring, see How to set up host application monitoring? section.","title":"Applications"},{"location":"helpers/FAQ/#host-monitoring-lower-layers","text":"If you want to know details why is some stats panel in upper layer green, orange or red, just drill down and click it. See some examples from layer 2 of host monitoring: Host Detail Application","title":"Host monitoring lower layers"},{"location":"helpers/FAQ/#how-to-set-up-k8s-cluster-monitoring","text":"","title":"How to set up k8s cluster monitoring?"},{"location":"helpers/FAQ/#prerequisites","text":"Install dNation k8s monitoring on k8s cluster. See installation steps here .","title":"Prerequisites"},{"location":"helpers/FAQ/#set-up","text":"K8s cluster monitoring is enabled by default. See the docs : clusterMonitoring : enabled : true clusters : - name : K8sCluster label : observer-cluster # The label should be the same as the external_label `cluster` from prometheus description : 'Kubernetes cluster monitoring' apps : [] It means that your k8s cluster is being monitored right after installation of dNation k8s monitoring. If you want to customize k8s cluster monitoring see How to customize my k8s monitoring? section.","title":"Set up"},{"location":"helpers/FAQ/#how-to-customize-my-k8s-monitoring","text":"We understand that each k8s cluster may contain various workloads or implement various scaling strategies. We tried to do our best and set up suitable defaults based on our production environment experiences, but we know that isn't possible cover the variety of k8s cluster configurations. To tackle this problem, we implemented template logic that allows you to fully customize the default state panels or alerts. You can also create your own to fulfill your k8s cluster monitoring requirements. Currently only first layer is customizable, however, the templating of other layers is under development. For full list of k8s templates see the docs . If you want to customize default layer 1 template in k8s monitoring just create a simple yaml file: templates : k8s : # Override stats panel title and warning thresholds for the etcd health panel etcdHealth : panel : title : \"Custom Title\" thresholds : warning : 96 alert : thresholds : warning : 96 Each state panel that can be modified contains a template name in its description for easy navigation: Finally, update your k8s monitoring deployment and override the default monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f override.yaml If you want to create your own custom template see full examples in the helpers directory.","title":"How to customize my k8s monitoring?"},{"location":"helpers/FAQ/#how-to-set-up-k8s-application-monitoring","text":"","title":"How to set up k8s application monitoring?"},{"location":"helpers/FAQ/#prerequisites_1","text":"Enable or install metrics exporter in your k8s application.","title":"Prerequisites"},{"location":"helpers/FAQ/#set-up_1","text":"K8s application monitoring is disabled by default. See the docs . To enable it just create application monitoring definition as follows: clusterMonitoring : enabled : true clusters : - name : K8sCluster label : observer-cluster # The label should be the same as the external_label `cluster` from prometheus description : 'Kubernetes cluster with application monitoring' apps : - name : app-example description : Example of App Monitoring jobName : flask-app # The job name should be the same as the name which will be retrieved from the `jobLabel`, see the `serviceMonitor` section templates : javaActuator : # Application Exporter template enabled : true serviceMonitor : jobLabel : app # The label to use to retrieve the job name from. namespaceSelector : # Namespaces to transfer from the kubernetes service to the target matchNames : - default selector : # Label selector for services to which this ServiceMonitor applies matchLabels : app : flask-app # Endpoints of the selected service to be monitored # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#endpoint endpoints : - targetPort : metrics # Name or number of the endpoint's target port path : /metrics # HTTP path to scrape for metrics Application monitoring definition contains two main sections: template and serviceMonitor or podMonitor respectively. The template section defines which template and pre-defined application dashboard will be used. For full list of pre-defined k8s application templates see the docs . Application monitoring metrics endpoint is auto-discovered by service monitor or by pod monitor CRDs. See the documentation of CRDs to ensure that your application metrics endpoint will be discovered as you want or find some examples in helpers directory. Finally, update your k8s monitoring deployment and apply the application monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f apps.yaml - If you want to customize the default application template see How to customize my k8s monitoring? section.","title":"Set up"},{"location":"helpers/FAQ/#how-to-set-up-host-monitoring","text":"","title":"How to set up host monitoring?"},{"location":"helpers/FAQ/#prerequisites_2","text":"Enable or install node exporter in your host. You can use helpers/node_exporter.sh installation script.","title":"Prerequisites"},{"location":"helpers/FAQ/#set-up_2","text":"Host monitoring is disabled by default. See the docs . To enable it just create host monitoring definition as follows: hostMonitoring : enabled : true hosts : - name : host-01 description : Host 01 jobName : host-01 host : address : 1.1.1.1 # Host IP address serviceMonitor : endpoints : - port : 9100 # Node exporter port path : /metrics # Node exporter metrics path Host monitoring definition contains two main sections: host and serviceMonitor . The host section simply defines the host IP address. The serviceMonitor defines service monitor CRD endpoint i.e. the host's node exporter port and path. Lot of service monitor parameters are pre-configured to simplify host monitoring configuration. If you are interested how or you want to override some of these parameters see the chart/templates/hosts directory. Finally, update your k8s monitoring deployment and apply the host monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f host.yaml","title":"Set up"},{"location":"helpers/FAQ/#how-to-customize-my-host-monitoring","text":"To cover variety of host configurations we implemented the template logic in the host monitoring as well. Templates allow you to fully customize the default state panels or alerts. You can also create your own to fulfill your host monitoring requirements. Currently, layer 1 customization is implemented. For full list of host templates see the docs . If you want to customize default layer 1 template in host monitoring just create a simple yaml file: templates : host : # Override warning thresholds for the CPU utilization panel overallUtilizationCPU : panel : thresholds : warning : 50 alert : thresholds : warning : 50 Each state panel that can be modified contains a template name in its description for easy navigation: Finally, update your k8s monitoring deployment and override the default monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f override.yaml If you want to create your own custom template see full examples in the helpers directory.","title":"How to customize my host monitoring?"},{"location":"helpers/FAQ/#how-to-set-up-host-application-monitoring","text":"","title":"How to set up host application monitoring?"},{"location":"helpers/FAQ/#prerequisites_3","text":"Enable or install metrics exporter in your host application.","title":"Prerequisites"},{"location":"helpers/FAQ/#set-up_3","text":"Hosts application monitoring is disabled by default. See the docs . To enable it just create application monitoring definition as follows: hostMonitoring : enabled : true hosts : - name : host-01 description : 'Host 01 with application monitoring' host : address : 1.2.3.4 apps : - name : host-01-docker description : Host 01 Docker Containers jobName : host-01-docker templates : cAdvisor : # Application Exporter template enable : true serviceMonitor : endpoints : - port : 9101 # Application exporter port path : /metrics # Application exporter metrics path Application monitoring definition contains two main sections: template and serviceMonitor . The template section defines which template and pre-defined application dashboard will be used. For full list of pre-defined host application templates see the docs . The serviceMonitor defines service monitor CRD endpoint, i.e. the application's exporter port and path. Lot of service monitor parameters are pre-configured to simplify application monitoring configuration. If you are curious how or you want to override some of these parameters see the chart/templates/hosts directory or find some examples in helpers directory. Finally, update your k8s monitoring deployment and apply the application monitoring configuration: helm upgrade [ RELEASE ] [ CHART ] -f apps.yaml - If you want to customize the default application template see the How to customize my host monitoring? section.","title":"Set up"},{"location":"helpers/FAQ/#why-l2-table-item-has-different-background-color-as-corresponding-l1-stat-panel","text":"If you customized your k8s monitoring , you can see that some table items on L2 layer dashboard may have different background color as corresponding L1 stat panel. Example: - Define custom L1 PVC utilization panel with custom thresholds for ELK PVCs - If you drill down, you can see that corresponding PVC capacity is in Critical state (orange background color) Currently only first layer is customizable, which caused this unexpected behaviour. Templating of other layers is under development and is planned in v1.1.x release .","title":"Why L2 table item has different background color as corresponding L1 stat panel?"},{"location":"helpers/FAQ/#kubernetes-monitoring-shows-or-0-state-for-some-control-plane-components-are-control-plane-components-working-correctly","text":"Control plane components work probably well, but their metrics server might be disabled, misconfigured or may not be present at all. For example, if is used OVNKubernetes CNI kube-proxy doesn't exist here. You should want to check address bindings of control plane components' metrics as follows: The metrics of etcd and kube-proxy control plane components are by default bound to the localhost that prometheus instances cannot access. Also make sure metrics of scheduler and controller-manager control plane components don't have the same address binding if you want to collect them. Edit and use kubeadm_init.yaml file to configure kubeadm init in case of fresh K8s deployment. kubeadm init --config = helpers/kubeadm_init.yaml Manual setup in case of already running K8s deployment. Setup etcd metrics bind address # On k8s master node cd /etc/kubernetes/manifests/ sudo vim etcd.yaml # Add listen-metrics-urls as etcd command option ... - --listen-metrics-urls = http://0.0.0.0:2381 ... Setup kube-proxy metrics bind address Edit kube-proxy daemon set kubectl edit ds kube-proxy -n kube-system ...containers: - command: - /usr/local/bin/kube-proxy - --config = /var/lib/kube-proxy/config.conf - --hostname-override = $( NODE_NAME ) - --metrics-bind-address = 0 .0.0.0 # Add metrics-bind-address line Edit kube-proxy config map kubectl -n kube-system edit cm kube-proxy ... kind: KubeProxyConfiguration metricsBindAddress: \"0.0.0.0:10249\" # Add metrics-bind-address host:port mode: \"\" Delete the kube-proxy pods and reapply the new configuration kubectl -n kube-system delete po -l k8s-app = kube-proxy Setup scheduler metrics bind address # On k8s master node cd /etc/kubernetes/manifests/ sudo vim kube-scheduler.yaml # Edit bind-address and port command options ... - --bind-address = 0 .0.0.0 - --secure-port = 10259 ... Setup controller-manager metrics bind address # On k8s master node cd /etc/kubernetes/manifests/ sudo vim kube-controller-manager.yaml # Edit bind-address and port command options ... - --bind-address = 0 .0.0.0 - --secure-port = 10257 ... You should also check: * TLS configuration of prometheus components responsible for scraping controller-manager and scheduler . If you don't want to set up TLS, you can skip validation as shown here . Authorization to access controller-manager and scheduler . You can skip authorization for some endpoints by setting --authorization-always-allow-paths: \"/healthz,/readyz,/livez,/metrics\" in kubeadm_init.yaml ( see example for kind cluster ) or manually in already running K8s deployment by following same steps as above when setting metrics bind address.","title":"Kubernetes Monitoring shows - OR 0% state for some control plane components. Are control plane components working correctly?"},{"location":"helpers/FAQ/#how-to-set-up-k8s-multi-cluster-monitoring","text":"","title":"How to set up k8s multi-cluster monitoring?"},{"location":"helpers/FAQ/#prerequisites_4","text":"Install dNation k8s monitoring on k8s cluster. See installation steps here .","title":"Prerequisites"},{"location":"helpers/FAQ/#set-up_4","text":"Multi-cluster monitoring is supported by default. All you need to do is just to install dNation Kubernetes Monitoring with custom values as shown in example here . clusterMonitoring : enabled : true clusters : - name : Observer label : observer-cluster description : 'Kubernetes cluster with application monitoring' apps : [] - name : Workload label : workload-cluster description : 'Kubernetes cluster with application monitoring' apps : [] Do not forget to set the correct labels for your clusters!","title":"Set up"},{"location":"helpers/ci_cd_image/","text":"Jsonnet A simple Docker image includes jsonnet, jsonnetfmt, jsonnet-lint, yq and jsonnet lib like grafonnet-lib and grafonnet-polystat-panel. Docker image was built for the CI/CD purposes or to avoid having to install jsonnet tools on your computer (keep it in docker). Usage # Jsonnet docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> jsonnet -h # Jsonnet Formater docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> jsonnetfmt -h # Jsonnet Linter docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> jsonnet-lint -h # YQ docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> yq -h Inspect versions docker inspect --format '{{ index .Config.Labels }}' dnationcloud/jsonnet:<tagname>","title":"Jsonnet"},{"location":"helpers/ci_cd_image/#jsonnet","text":"A simple Docker image includes jsonnet, jsonnetfmt, jsonnet-lint, yq and jsonnet lib like grafonnet-lib and grafonnet-polystat-panel. Docker image was built for the CI/CD purposes or to avoid having to install jsonnet tools on your computer (keep it in docker).","title":"Jsonnet"},{"location":"helpers/ci_cd_image/#usage","text":"# Jsonnet docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> jsonnet -h # Jsonnet Formater docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> jsonnetfmt -h # Jsonnet Linter docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> jsonnet-lint -h # YQ docker run --rm -it -v `pwd`:/src dnationcloud/jsonnet:<tagname> yq -h Inspect versions docker inspect --format '{{ index .Config.Labels }}' dnationcloud/jsonnet:<tagname>","title":"Usage"}]}